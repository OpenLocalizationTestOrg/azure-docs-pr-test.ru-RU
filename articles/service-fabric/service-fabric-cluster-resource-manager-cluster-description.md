---
title: "Описание кластера диспетчера ресурсов aaaCluster | Документы Microsoft"
description: "Описание кластера Service Fabric, указав доменов сбоя, домены обновления, свойства узла и узла емкости hello диспетчер ресурсов кластера."
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: 55f8ab37-9399-4c9a-9e6c-d2d859de6766
ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: f2822075976bd54402af5ad56991b5b360dfb1d8
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/06/2017
---
# <a name="describing-a-service-fabric-cluster"></a>Описание кластера Service Fabric
Диспетчер ресурсов кластера Service Fabric Hello предоставляет несколько механизмов для описания кластера. Во время выполнения hello диспетчер ресурсов кластера использует этот сведения tooensure высокий уровень доступности hello служб, работающих в кластере hello. Во время применения этих важных правил, также выполняется попытка toooptimize потребления ресурсов hello в кластере hello.

## <a name="key-concepts"></a>Основные понятия
Диспетчер ресурсов кластера Hello поддерживает несколько функций, которые описывают кластера:

* Домены сбоя
* Домены обновления
* Свойства узла
* Емкость узла

## <a name="fault-domains"></a>Домены сбоя
Домен сбоя — это любая область координированного сбоя. Один компьютер входит в домен сбоя (поскольку она может завершиться ошибкой в его собственный по различным причинам из power питания сбоев toodrive сбоев toobad Сетевых микропрограммного обеспечения). Компьютеры, подключенных toohello же коммутатор Ethernet находятся в одном домене, hello как — это компьютеры, совместного использования одного источника питания, или в одном месте. Так как это естественное для toooverlap сбои оборудования, домены сбоя по своей природе иерархических и представляются в виде URI в Service Fabric.

Очень важно, что домены сбоя правильно настроены так, как это toosafely месте служб использует Service Fabric. Service Fabric не хочет tooplace служб таким образом, что потери hello домен сбоя (по причине hello сбоя одного из компонентов) вызывает toogo службы вниз. В hello среды Azure Service Fabric использует hello домен сбоя сведения, предоставляемые toocorrectly среды hello настройте hello узлов в кластере hello от вашего имени. Настроена для автономной службы структуры, домены сбоя определяются во время hello hello кластера 

> [!WARNING]
> Очень важно, что hello сведения домен сбоя, если tooService структуры является точным. Например, предположим, что узлы кластера Service Fabric выполняются на 10 виртуальных машинах, работающих на пяти физических узлах. В этом случае, даже если используется 10 виртуальных машин, имеется только 5 разных (верхнего уровня) доменов сбоя. Совместное использование hello вызывает на одном физическом узле виртуальных машин tooshare hello же корневой домен сбоя, поскольку hello взаимодействие с виртуальными машинами координируемой сбой в случае их физического узла.  
>
> Поскольку Service Fabric ожидает hello домен сбоя узла не toochange. Другие механизмы обеспечения высокого уровня доступности hello виртуальных машин, такие как [виртуальных машин высокой НАДЕЖНОСТИ](https://technet.microsoft.com/en-us/library/cc967323.aspx), используйте прозрачный миграция виртуальных машин с одного узла tooanother. Эти механизмы не перенастроить или уведомлять hello выполнение кода внутри hello виртуальной Машины. Следовательно, использующие их среды **не подходят** для выполнения кластеров Service Fabric. Service Fabric должен быть принят на работу технологии только высокого уровня доступности hello. Необходимости в таких механизмах, как динамическая миграция виртуальных машин, сети SAN или прочие, нет. При использовании совместно с Service Fabric эти механизмы _снижают_ доступность и надежность приложений, так как повышают сложность, добавляют централизованные источники сбоев и используют стратегии обеспечения надежности и доступности, которые конфликтуют с механизмами Service Fabric. 
>
>

В следующем рисунке hello мы цвета все сущности hello, задействованных tooFault доменов и список всех hello разным доменам сбоя, привести. В этом примере у нас есть центры обработки данных (DC), стойки (R) и колонки (B). Можно представить при каждой колонки содержит более одной виртуальной машины, возможно, существуют еще один уровень в иерархии домен сбоя hello.

<center>
![Узлы, организованные через домены сбоя][Image1]
</center>

Во время выполнения диспетчер ресурсов кластера Service Fabric hello считает, что домены сбоя hello в кластере hello и планов макеты. Здравствуйте реплики с отслеживанием состояния или без сохранения состояния экземпляры для данной службы будут распределены, поэтому они находятся в отдельных доменах сбоя. Распространение hello службы в доменах сбоя гарантирует, что hello доступность службы hello не подвергаются опасности при сбое домен сбоя на любом уровне иерархии hello.

Диспетчер ресурсов кластера Service Fabric не проверяет, сколько уровней в иерархии домен сбоя hello. Тем не менее он пробует tooensure потери hello любой одной части иерархии hello не оказать влияние на службы, работающие в нем. 

Рекомендуется при наличии hello одинаковое число узлов на каждом уровне глубины в hello иерархии домен сбоя. Если hello домены сбоя «дерево» является несбалансированной в кластере, это затрудняет для hello диспетчер ресурсов кластера toofigure out hello наиболее размещения служб. Несбалансированных макеты домены сбоя означает, что hello потери некоторых доменов hello влияние доступность служб больше, чем другие домены. В результате hello диспетчер ресурсов кластера защита от разрыва между две цели: ему toouse hello машин в домене «большой», поместив в них служб, и ему tooplace службами из других доменов, чтобы потери hello домена не вызывает проблем. 

Как выглядят несбалансированные домены? В следующей схеме hello показано два различными макетами кластера. В первом примере hello узлы hello распределяются равномерно между hello домены сбоя. В втором примере hello один домен сбоя имеет много больше узлов, чем hello другие домены сбоя. 

<center>
![Две различные структуры кластеров][Image2]
</center>

В Azure Выбор hello, из которых домен сбоя содержит узел осуществляется автоматически. Тем не менее в зависимости от числа hello узлы, которые можно подготовить можно по-прежнему завершить с доменами отказоустойчивости с помощью нескольких узлов в них, чем другие. Например предположим, имеется пять домены сбоя в кластере hello, но подготовить семь узлов для данного типа. В этом случае hello сначала в двух доменах отказоустойчивости оказаться больше узлов. Если продолжить toodeploy дополнительные NodeTypes только несколько экземпляров усугубляется проблема hello. По этой причине рекомендуется, hello количество узлов в каждом типе узла делится на количество доменов сбоя hello.

## <a name="upgrade-domains"></a>Домены обновления
Домены обновления являются другой функцией, которая помогает hello диспетчер ресурсов кластера Service Fabric анализа структуры hello hello кластера. Домены обновления определяют наборы узлов, которые будут обновлены в hello то же время. Обновление доменов справки hello диспетчер ресурсов кластера понять и координировать операции управления, такие как обновления.

Домены обновления очень похожи на домены сбоя, но имеют ряд ключевых отличий. Домены сбоя определяются областями согласованных сбоев оборудования. Домены обновления, на hello другой стороны, определяются политикой. Вы получаете toodecide, сколько требуется, вместо его диктовку средой hello. Можно создать любое количество доменов обновления, как и узлов. Еще одно различие между доменами сбоя и доменами обновления заключается в том, что домены обновления не являются иерархическими. Вместо этого они больше напоминают простой тег. 

Hello следующей схеме показаны три домены обновления распределяются в трех доменах сбоя. На ней также представлено одно возможное размещение трех разных реплик службы с отслеживанием состояния, где каждая из них находится в разных доменах сбоя и обновления. Такое размещение позволяет hello потери домен сбоя в середине hello обновление службы и еще одну копию кода hello и данных.  

<center>
![Размещение с доменами сбоя и обновления][Image3]
</center>

Есть свои преимущества и недостатки toohaving большое число доменов обновления. Дополнительные домены обновления означает более детального каждый шаг обновления hello и таким образом влияет на меньшее число узлов или служб. В результате меньше службы имеют toomove во время внесения меньше обработки в системе hello. Это обычно tooimprove надежности, поскольку меньше службы hello затронуты любые проблемы, появившиеся во время обновления hello. Дополнительные домены обновления также означает, что требуется меньше буферную на другие узлы toohandle hello влияние hello обновить. Например если имеется пять домены обновления hello узлов в каждом обработки примерно 20% от трафика. Если требуется tootake работу этого домена обновления для обновления этой нагрузки обычно требуется toogo в любом месте. Так как у вас есть четыре остальные домены обновления, они должны иметь место для 5% hello всего трафика. Дополнительные домены обновления означает, что требуется меньше буфера на узлах кластера hello hello. Например, предположим, что у вас имеется не 5, а 10 доменов обновления. В этом случае каждый UD только происходит обработка около 10% от общего трафика hello. При этапы обновления через hello кластер каждый домен будет достаточно места toohave около 1.1% hello всего трафика. Дополнительные домены обновления обычно позволяют toorun к узлам на более высокую степень использования, поскольку требуется меньше зарезервированный объем ресурсов. Hello то же верно для доменов сбоя.  

Недостатком Hello иметь множество доменов обновления — обновления, как правило tootake больше времени. Service Fabric ожидает на короткое время после завершения обновления домена и выполняет проверку до начала tooupgrade hello следующее. Эти задержки включить обнаружение проблем, представленных hello обновления для продолжения обновления hello. компромисс Hello является допустимым, поскольку он предотвращает неправильный изменения влияют на слишком большую часть hello службы одновременно.

Слишком малое количество доменов обновления имеет множество отрицательных побочных эффектов: пока каждый отдельный домен обновления недоступен и обновляется, значительная часть общей емкости ресурсов недоступна. Например, при наличии только трех доменов обновления вы одновременно отключаете примерно треть общих ресурсов службы или кластера. Наличие слишком много ресурсов службы работу сразу не нежелательно, так как у вас есть toohave достаточную мощность в hello остальная часть рабочей нагрузки кластера toohandle hello. Наличие этого буфера означает, что во время обычной работы данные узлы будут менее загружены, чем они были бы в противном случае. Это увеличивает hello стоимость выполнения службы.

Нет не реальных ограничение toohello общее число ошибок или домены обновления в среде, а также ограничения на том, как они перекрываются. С другой стороны, существует несколько общих шаблонов:

- соотношение 1:1 (каждый домен сбоя сопоставляется с доменом обновления);
- один домен обновления на узел (экземпляр физической или виртуальной ОС);
- Модель «чередующиеся» или «Матрица», где hello домены сбоя и обновления формируют матрицу с машинами обычно работающих вниз диагонали hello

<center>
![Структуры доменов сбоя и обновления][Image4]
</center>

Что не лучший ответ какие toochoose макета, каждый имеет некоторые преимущества и недостатки. Например модель 1FD:1UD hello — простой tooset вверх. Hello 1 домен обновления каждого узла модели является наиболее какие сотрудники, используемый для. Во время обновления все узлы обновляются независимо друг от друга. Это аналогично toohow небольших наборов машин обновлены вручную в прошлом hello. 

наиболее распространенные модель Hello — hello FD/UD матрицы, где hello сбоя и доменам обновления форму таблицы и узлы располагаются начиная вдоль hello по диагонали. Это модель hello, используется по умолчанию в Service Fabric кластеров в Azure. Для кластеров с узлами много все завершает выглядит шаблон плотную матрицу hello выше.

## <a name="fault-and-upgrade-domain-constraints-and-resulting-behavior"></a>Ограничения доменов сбоя и обновления и соответствующее поведение
Диспетчер ресурсов кластера Hello обрабатывает hello желания tookeep службы равномерно распределяется по доменам сбоя и обновления как ограничение. Дополнительные сведения об ограничениях см. в [этой статье](service-fabric-cluster-resource-manager-management-integration.md). Здравствуйте ограничения состояния сбоя и доменов обновления:» для данного служебного раздела никогда не следует различие *больше единицы* hello число объектов службы (службы без отслеживания состояния экземпляры или реплики службы с отслеживанием состояния) между двумя доменами.» Это предотвращает определенные перемещения или упорядочения, которые нарушают данное ограничение.

Давайте рассмотрим один пример. Предположим, что у нас есть кластер с шестью узлами (У), на котором настроено пять доменов сбоя (ДС) и пять доменов обновления (ДО).

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | | |У3 | | |
| **ДО3** | | | |У4 | |
| **ДО4** | | | | |У5 |

Теперь предположим, что мы создаем службу, для которой TargetReplicaSetSize (или InstanceCount для службы без отслеживания состояния) имеет значение 5. Hello реплик поступить в N1 N5. Узел У6 фактически никогда не используется, вне зависимости от количества создаваемых служб. Но почему? Давайте рассмотрим hello разница между текущим макетом hello и что произойдет, если вы выбрали N6.

Вот макета hello мы получили и hello допустимое количество реплик на сбоя и доменов обновления.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** | |Р2 | | | |1 |
| **ДО2** | | |Р3 | | |1 |
| **ДО3** | | | |Р4 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

Эта структура сбалансирована в плане распределения узлов на домен сбоя и домен обновления, Он также сбалансирована с точки зрения hello число реплик на сбоя и доменов обновления. Каждый домен имеет hello одинаковом количестве узлов и hello одинаковое количество реплик.

Теперь давайте посмотрим, что произошло бы, если бы вместо У2 мы использовали У6. Как бы реплик hello распространяться затем?

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** |Р5 | | | | |1 |
| **ДО2** | | |Р2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |2 |0 |1 |1 |1 |- |

Этот макет приводит к нарушению нашей определение hello ограничение домен сбоя. FD0 имеет две реплики, а FD1 имеет нулевое, позволяя hello различие между FD0 и FD1 всего два. Такой подход не позволяет Hello диспетчер ресурсов кластера. Аналогично, если бы мы выбрали узлы У2 и У6 (вместо У1 и У2), то получили бы следующее.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** | | | | | |0 |
| **ДО1** |Р5 |Р1 | | | |2 |
| **ДО2** | | |Р2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

Эта структура сбалансирована с точки зрения доменов сбоя. Однако теперь он нарушил hello ограничений доменов обновления. Причина этого в том, что у ДО0 ноль реплик, а у ДО1 их две. Таким образом этот макет также является недопустимым и не быть получены по hello диспетчер ресурсов кластера. 

## <a name="configuring-fault-and-upgrade-domains"></a>Настройка доменов сбоя и обновления
Определение доменов сбоя и обновления выполняется автоматически в размещенных в Azure развертываниях Service Fabric. Service Fabric и использование сведений о среде hello из Azure.

Если требуется создаете свой кластер (или toorun определенной топологии в разработке), можно предоставить сведения о домене сбоя и доменов обновления hello самостоятельно. В этом примере мы определяем кластер локальной разработки из девяти узлов, охватывающий три центра обработки данных (каждый с тремя стойками). Этот кластер также имеет три домена обновления, чередующиеся с этими тремя центрами обработки данных. Пример конфигурации hello используется следующим образом: 

ClusterManifest.xml

```xml
  <Infrastructure>
    <!-- IsScaleMin indicates that this cluster runs on one-box /one single server -->
    <WindowsServer IsScaleMin="true">
      <NodeList>
        <Node NodeName="Node01" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType01" FaultDomain="fd:/DC01/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node02" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType02" FaultDomain="fd:/DC01/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node03" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType03" FaultDomain="fd:/DC01/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node04" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType04" FaultDomain="fd:/DC02/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node05" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType05" FaultDomain="fd:/DC02/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node06" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType06" FaultDomain="fd:/DC02/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node07" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType07" FaultDomain="fd:/DC03/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node08" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType08" FaultDomain="fd:/DC03/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node09" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType09" FaultDomain="fd:/DC03/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
      </NodeList>
    </WindowsServer>
  </Infrastructure>
```

Использование ClusterConfig.json для автономных развертываний

```json
"nodes": [
  {
    "nodeName": "vm1",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm2",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm3",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm4",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm5",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm6",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm7",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm8",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm9",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD3"
  }
],
```

> [!NOTE]
> При определении кластеров с помощью Azure Resource Manager домены сбоя и домены обновления назначаются платформой Azure. Таким образом hello определения типов узлов и наборы масштабирования виртуальной машины в шаблон диспетчера ресурсов Azure не включает домен сбоя и доменов обновления.
>

## <a name="node-properties-and-placement-constraints"></a>Свойства узлов и ограничения размещения
Иногда (на самом деле, большую часть времени hello) ты tooensure toowant выполнения некоторых рабочих нагрузок только для определенных типов узлов в кластере hello. Например, для одних рабочих нагрузок могут требоваться графические процессоры или накопители SSD, а для других — нет. Хорошим примером того, предназначенных для рабочих нагрузок tooparticular оборудования является практически в каждой n уровневая архитектура здесь. Некоторые машин служат hello переднего плана или API, обслуживающий части приложения hello и клиенты без поддержки toohello или hello Интернета. Различных компьютерах, часто с различных аппаратных ресурсов, обработки работы hello hello вычислений и хранения слоев. Это обычно _не_ tooclients вызывающим объектам прямой доступ или hello Интернета. Service Fabric ожидает, что существуют случаи, где одни рабочие нагрузки должны toorun на конкретной аппаратной конфигурации. Например:

* существующее n-уровневое приложение быстро перемещено в среду Service Fabric;
* Рабочая нагрузка хочет toorun на оборудования для повышения производительности, масштаба или изоляции соображениям безопасности
* рабочая нагрузка должна быть изолирована от других рабочих нагрузок по соображениям политики или потребления ресурсов.

Службы эти элементы конфигурации, toosupport структуры имеет первого класса понятие тегов, которые может быть применен toonodes. Эти теги называются **свойствами узла**. **Ограничения размещения** являются инструкций hello присоединенного tooindividual служб, выделенных для одного или нескольких свойств узла. Ограничения размещения определяют, где должны запускаться службы. расширяемый набор ограничений Hello - каждая пара ключ значение может работать. 

<center>
![Разные рабочие нагрузки структуры кластера][Image5]
</center>

### <a name="built-in-node-properties"></a>Встроенные свойства узла
Service Fabric определяет некоторые свойства узла по умолчанию, которые могут использоваться автоматически без необходимости toodefine пользователя hello их. свойства по умолчанию Hello, определенные в каждом узле представляют hello **NodeType** и hello **NodeName**. Например, ограничение на размещение можно записать в таком виде: `"(NodeType == NodeType03)"`. Обычно мы нашли NodeType toobe одним из наиболее часто используемых hello свойств. Это удобно, так как оно точно соответствует типу компьютера. Каждый тип машины соответствует tooa тип рабочей нагрузки в традиционных n уровневого приложения.

<center>
![Ограничения на размещение и свойства узлов][Image6]
</center>

## <a name="placement-constraint-and-node-property-syntax"></a>Синтаксис ограничений размещения и свойств узлов 
значение Hello в hello узла может быть строкой, bool, или длинное со знаком. оператор Hello hello службы называется расположение *ограничение* , так как он ограничивает, где можно запустить службу hello в кластере hello. Hello ограничение может быть любое логическое инструкцию, которая работает hello свойствами другой узел в кластере hello. Допустимые селекторы Hello в эти логические операторы являются:

1) Условные проверки для создания определенных операторов:

| Инструкция | Синтаксис |
| --- |:---:|
| "равно" | "==" |
| "не равно" | "!=" |
| "больше" | ">" |
| "больше или равно" | ">=" |
| "меньше" | "<" |
| "меньше или равно" | "<=" |

2) Логические операторы для группирования и логических операций:

| Инструкция | Синтаксис |
| --- |:---:|
| "и" | "&&" |
| "или" | "&#124;&#124;" |
| "не" | "!" |
| "группа как отдельный оператор" | "()" |

Ниже приведено несколько примеров основных операторов ограничения.

  * `"Value >= 5"`
  * `"NodeColor != green"`
  * `"((OneProperty < 100) || ((AnotherProperty == false) && (OneProperty >= 100)))"`

Только узлы, где hello в целом размещения ограничение результатом выполнения инструкции является слишком «True» можно использовать службу hello, примененное к нему. Узлы без определенного свойства не совпадают с какими-либо ограничениями на размещение, содержащими это свойство.

Предположим, что hello следующий узлом, свойства, определенные для типа данного узла:

ClusterManifest.xml

```xml
    <NodeType Name="NodeType01">
      <PlacementProperties>
        <Property Name="HasSSD" Value="true"/>
        <Property Name="NodeColor" Value="green"/>
        <Property Name="SomeProperty" Value="5"/>
      </PlacementProperties>
    </NodeType>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json. 

> [!NOTE]
> В вашей узел hello шаблона диспетчера ресурсов Azure типа обычно является параметризованным. Он будет иметь вид "[parameters('vmNodeType1Name')]", а не "NodeType01".
>

```json
"nodeTypes": [
    {
        "name": "NodeType01",
        "placementProperties": {
            "HasSSD": "true",
            "NodeColor": "green",
            "SomeProperty": "5"
        },
    }
],
```

Вы можете создать *ограничения* на размещение службы следующим образом:

C#

```csharp
FabricClient fabricClient = new FabricClient();
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
serviceDescription.PlacementConstraints = "(HasSSD == true && SomeProperty >= 4)";
// add other required servicedescription fields
//...
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceType -Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton -PlacementConstraint "HasSSD == true && SomeProperty >= 4"
```

Если все узлы NodeType01 являются допустимыми, можно также выбрать узел типа с ограничением hello "(NodeType == NodeType01)».

Одна из hello интересного о ограничений размещения службы — что они могут изменяться динамически во время выполнения. Поэтому если требуется, можно перемещать службы в кластере hello, добавлять и удалять требования и т. д. Гарантирует, что hello служба остается копии и доступной даже когда эти типы изменений, становятся занимается Service Fabric.

C#:

```csharp
StatefulServiceUpdateDescription updateDescription = new StatefulServiceUpdateDescription();
updateDescription.PlacementConstraints = "NodeType == NodeType01";
await fabricClient.ServiceManager.UpdateServiceAsync(new Uri("fabric:/app/service"), updateDescription);
```

PowerShell:

```posh
Update-ServiceFabricService -Stateful -ServiceName $serviceName -PlacementConstraints "NodeType == NodeType01"
```

Ограничения размещения задаются для каждого отдельного именованного экземпляра службы. Обновления всегда выполняются hello объекта (перезаписать) ранее заданного.

определения кластера Hello определяет свойства hello на узле. Чтобы изменить свойства узла, требуется обновить конфигурацию кластера. Обновление свойства узла требуется ее новые свойства tooreport toorestart каждого соответствующего узла. Этими последовательными обновлениям управляет Service Fabric.

## <a name="describing-and-managing-cluster-resources"></a>Описание кластерных ресурсов и управление ими
Одним из наиболее важных заданий из любой orchestrator — toohelp hello управления потреблением ресурсов в кластере hello. Управление кластерными ресурсами связано с несколькими аспектами. Во-первых, необходимо гарантировать, что компьютеры не будут перегружены. То есть нужно сделать так, чтобы на компьютерах не было запущено больше служб, чем они могут обрабатывать. Во-вторых нет балансировки и оптимизации, являющийся toorunning критические службы эффективно. Стоимость предложения услуг конфиденциальные или производительности не может разрешить некоторые узлы toobe горячей другие являются холодного. Горячий узлы привести tooresource конфликтов и снижению производительности и новых узлов представляют пустой тратой ресурсов и стоимости. 

В Service Fabric ресурсы представлены в виде метрик (`Metrics`). Показатели, логических или физических ресурсов, что требуется toodescribe tooService структуры. Метриками, например, являются атрибуты WorkQueueDepth или MemoryInMb. Сведения о hello физические ресурсы, которые могут управлять Service Fabric на узлах. в разделе [управление ресурсами](service-fabric-resource-governance.md). Дополнительные сведения о настройке пользовательских метрик см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).

Метрики отличаются от ограничений на размещение и свойств узлов. Свойства узла являются статические дескрипторы сами по себе узлы hello. Метрики описывают ресурсы на этих узлах, которые потребляются службами, запускаемыми на узле. Свойство узла может быть «HasSSD» и может быть установлена tootrue или false. Hello объем доступного места на SSD, и о том, сколько используется службами бы метрики, как «DriveSpaceInMb». 

Это важные toonote, так же, как для свойства узла и ограничения размещения hello диспетчер ресурсов кластера Service Fabric не понять, какие имена hello среднее значение метрики hello. Имена метрик — это просто строки. Это единицы toodeclare рекомендаций в составе имена метрик hello, создаваемые при может быть неоднозначным.

## <a name="capacity"></a>Capacity
При отключении *балансировки* всех ресурсов диспетчер кластерных ресурсов Service Fabric по-прежнему будет следить за тем, чтобы емкость ни одного узла не была превышена. Управление переполнения емкости возможна в том случае, если кластер hello не хватает свободного места или рабочей нагрузки hello больше, чем любой узел. Емкость — другой *ограничение* , hello диспетчер ресурсов кластера использует toounderstand объем ресурсов узел имеет. Осталось емкости также отслеживаются для hello кластер в целом. Показатели выражаются hello емкость и использование hello на уровне службы hello. Например hello показатель может быть «ClientConnections» и определенного узла, возможно, емкость для «ClientConnections» 32768 типа. Другие узлы могут иметь другие ограничения, некоторые службы, на который узел может сказать его в данный момент 32256 hello показателя «ClientConnections».

Во время выполнения диспетчер ресурсов кластера hello отслеживает оставшееся пространство в кластере hello и на узлах. В порядке tootrack емкости hello диспетчер ресурсов кластера вычитает использования каждой службы из узла емкости, где выполняется служба hello. С этой информацией hello диспетчер ресурсов кластера Service Fabric можно выяснить, где tooplace или переместите реплики, чтобы узлы не выходят емкости.

<center>
![Узлы и емкость кластера][Image7]
</center>

C#:

```csharp
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
ServiceLoadMetricDescription metric = new ServiceLoadMetricDescription();
metric.Name = "ClientConnections";
metric.PrimaryDefaultLoad = 1024;
metric.SecondaryDefaultLoad = 0;
metric.Weight = ServiceLoadMetricWeight.High;
serviceDescription.Metrics.Add(metric);
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("ClientConnections,High,1024,0)
```

Вы можете увидеть производственные мощности, определенных в манифесте кластера hello:

ClusterManifest.xml

```xml
    <NodeType Name="NodeType03">
      <Capacities>
        <Capacity Name="ClientConnections" Value="65536"/>
      </Capacities>
    </NodeType>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json. 

```json
"nodeTypes": [
    {
        "name": "NodeType03",
        "capacities": {
            "ClientConnections": "65536",
        }
    }
],
```

Нередко нагрузка службы динамически изменяется. Предположим, что реплика загрузку «ClientConnections» изменено с 1024 too2048, но узел hello работала на затем было всего 512 оставшейся для этой метрики емкости. В этом случае расположение, где в данный момент находится реплика или экземпляр, станет недопустимым, так как на этом узле недостаточно места. Hello диспетчер ресурсов кластера имеет tookick в и вернуть узел hello обратно ниже емкости. Он уменьшает нагрузку на узел hello, через емкости путем перемещения одного или нескольких реплик hello или экземпляров с узлов tooother этого узла. При перемещении реплик, hello диспетчер ресурсов кластера пытается toominimize hello стоимости тех перемещений. Стоимость перемещения рассматривается в [в этой статье](service-fabric-cluster-resource-manager-movement-cost.md) более о hello диспетчер ресурсов кластера перераспределения стратегии и правила описываются [здесь](service-fabric-cluster-resource-manager-metrics.md).

## <a name="cluster-capacity"></a>Емкость кластера
Так как hello hello keep диспетчер ресурсов кластера Service Fabric общего кластера слишком заполнением? Что ж, ввиду динамического характера нагрузки он мало что может сделать. Службы могут иметь свои пик нагрузки независимо от действия, производимые hello диспетчер ресурсов кластера. В результате кластер с большим резервом сегодня может оказаться довольно маломощным при повышении нагрузки завтра. С другой стороны, существует несколько элементов управления, помещенного в tooprevent проблем. Hello первое, что мы можем сделать это запретить создание новых рабочих нагрузок, приведет к полной toobecome кластера hello hello.

Предположим, вы создаете службу без отслеживания состояния, с которой связана некоторая нагрузка. Предположим, что служба hello интересуют метрики «DiskSpaceInMb» hello. Предположим также, что это будет tooconsume пять единиц «DiskSpaceInMb» для каждого экземпляра службы hello. Вы хотите toocreate три экземпляра службы hello. Отлично! Что означает необходимость 15 единиц «DiskSpaceInMb» toobe представить в кластере hello в порядке нам tooeven быть может toocreate экземпляры этих служб. Hello диспетчер ресурсов кластера постоянно вычисляет hello мощность и расход каждой метрики, чтобы можно было определить hello оставшееся пространство в кластере hello. Если не хватает места, hello hello отклоняет диспетчер ресурсов кластера создать вызова службы.

Поскольку требования hello только то, что существует доступны 15 единиц, это пространство может быть распределен несколькими различными способами. Например, это может быть одна оставшаяся единица емкости на 15 различных узлах или три оставшиеся единицы емкости на 5 разных узлах. Если hello диспетчер ресурсов кластера можно изменить порядок вещи, поэтому доступно пять единиц на трех узлах, он помещает hello службы. Расположения кластера hello обычно возможна, если почти заполнена hello кластера или для какой-либо причине нельзя объединять существующие службы hello.

## <a name="buffered-capacity"></a>Емкость буфера
Буферизованный емкость — еще одна функция hello диспетчер ресурсов кластера. Он позволяет резервирование некоторой части hello общую емкость узла. Этот буфер емкости — только используемые tooplace служб во время обновления и сбоев узлов. Буферизованная мощность указывается глобально для каждой метрики для всех узлов. hello защищены емкости Выбираемое значение Hello зависит от числа hello доменам сбоя и обновления в кластере hello. Большее количество доменов сбоя и обновления означает, что можно выбрать меньшее значение зарезервированной емкости. Если у вас есть несколько доменов, можно ожидать меньшее количество toobe вашего кластера недоступен во время обновления и сбои. Указание емкости буфер имеет смысл только при наличии заданного hello емкость узла для метрики.

Ниже приведен пример как toospecify буферизованный емкости:

ClusterManifest.xml

```xml
        <Section Name="NodeBufferPercentage">
            <Parameter Name="SomeMetric" Value="0.15" />
            <Parameter Name="SomeOtherMetric" Value="0.20" />
        </Section>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json.

```json
"fabricSettings": [
  {
    "name": "NodeBufferPercentage",
    "parameters": [
      {
          "name": "SomeMetric",
          "value": "0.15"
      },
      {
          "name": "SomeOtherMetric",
          "value": "0.20"
      }
    ]
  }
]
```

происходит сбой создания Hello новые службы, когда hello кластер находится вне буферизованный емкости для метрики. Предотвращение создания hello новый буфер hello toopreserve служб предотвращает обновления и сбои не toogo узлов по емкости. Емкость буфера указывать необязательно, но мы советуем добавить ее во всех кластерах, определяющих емкость для метрики.

Hello диспетчер ресурсов кластера предоставляет эти сведения для загрузки. Для каждой метрики эти сведения содержат: 
  - параметры вместимости буферизировать Hello
  - Общая емкость Hello
  - текущего потребления Hello
  - является ли каждая метрика сбалансированной;
  - статистические данные о hello стандартное отклонение
  - Hello узлы, имеющие hello большинство и как минимум нагрузки  
  
Ниже приведен пример таких выходных данных.

```posh
PS C:\Users\user> Get-ServiceFabricClusterLoadInformation
LastBalancingStartTimeUtc : 9/1/2016 12:54:59 AM
LastBalancingEndTimeUtc   : 9/1/2016 12:54:59 AM
LoadMetricInformation     :
                            LoadMetricName        : Metric1
                            IsBalancedBefore      : False
                            IsBalancedAfter       : False
                            DeviationBefore       : 0.192450089729875
                            DeviationAfter        : 0.192450089729875
                            BalancingThreshold    : 1
                            Action                : NoActionNeeded
                            ActivityThreshold     : 0
                            ClusterCapacity       : 189
                            ClusterLoad           : 45
                            ClusterRemainingCapacity : 144
                            NodeBufferPercentage  : 10
                            ClusterBufferedCapacity : 170
                            ClusterRemainingBufferedCapacity : 125
                            ClusterCapacityViolation : False
                            MinNodeLoadValue      : 0
                            MinNodeLoadNodeId     : 3ea71e8e01f4b0999b121abcbf27d74d
                            MaxNodeLoadValue      : 15
                            MaxNodeLoadNodeId     : 2cc648b6770be1bc9824fa995d5b68b1
```

## <a name="next-steps"></a>Дальнейшие действия
* Сведения о hello архитектуры и обмена информацией внутри hello диспетчер ресурсов кластера, извлечь [в данной статье](service-fabric-cluster-resource-manager-architecture.md)
* Определение метрики дефрагментации является одним из способов tooconsolidate нагрузки на узлах, а не его рассредоточения. toolearn как tooconfigure дефрагментации см. в слишком[в данной статье](service-fabric-cluster-resource-manager-defragmentation-metrics.md)
* Начать с начала hello и [получить введение toohello диспетчер ресурсов кластера Service Fabric](service-fabric-cluster-resource-manager-introduction.md)
* toofind out о управляет hello диспетчер ресурсов кластера и балансировку нагрузки в кластере hello Извлеките hello статьи на [балансировки нагрузки](service-fabric-cluster-resource-manager-balancing.md)

[Image1]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-domains.png
[Image2]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-uneven-fault-domain-layout.png
[Image3]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domains-with-placement.png
[Image4]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domain-layout-strategies.png
[Image5]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-layout-different-workloads.png
[Image6]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-placement-constraints-node-properties.png
[Image7]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-nodes-and-capacity.png
