---
title: "aaaAzure аварийного восстановления Service Fabric | Документы Microsoft"
description: "Azure Service Fabric предлагает toodeal необходимые возможности hello со всеми типами бедствия. В этой статье описываются hello аварийных ситуаций, которые могут возникать и как toodeal с ними."
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: ab49c4b9-74a8-4907-b75b-8d2ee84c6d90
ms.service: service-fabric
ms.devlang: dotNet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 04b8348fb63e8a1c76a8f722c4c8255b339908e2
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/06/2017
---
# <a name="disaster-recovery-in-azure-service-fabric"></a>Аварийное восстановление в Azure Service Fabric
Для обеспечения высокого уровня доступности крайне важно гарантировать продолжение работы всех типов служб при любых сбоях. Это особенно важно в ситуациях незапланированных сбоев, которые находятся вне вашего контроля. В этой статье описываются некоторые часто встречающиеся виды сбоев, которые могут привести к авариям, если их не смоделировать и не взять под контроль должным образом. В нем также рассматриваются действия и способы их устранения tootake при аварии, несмотря на это произошло. Цель Hello является toolimit или исключить риск hello простою или потере данных, когда они происходят сбои, плановой или в противном случае.

## <a name="avoiding-disaster"></a>Предотвращение аварий
Основная цель Service Fabric — toohelp модели среды и служб таким образом, что общие типы сбоев не аварий. 

В основном существует два типа сценариев аварий и сбоев:

1. Сбои оборудования или программного обеспечения.
2. Проблемы с работоспособностью.

### <a name="hardware-and-software-faults"></a>Сбои оборудования или программного обеспечения
Сбои оборудования и программного обеспечения — непредсказуемы. Hello простым способом toosurvive ошибок выполняется несколько копий службы hello, охваченных через границы сбоя оборудования или программного обеспечения. Например если служба выполняется только для одного конкретного компьютера, затем hello неудачного выполнения что один компьютер является аварийного для этой службы. простой способ tooavoid Hello этот аварии, составляет tooensure, который фактически выполняется служба hello на нескольких компьютерах. Тестирование — это тоже сбоя необходимости tooensure hello одной машины не нарушать hello службой. Планирование емкости гарантирует замены может быть создан экземпляр в другом месте и что снижение производительности не перегружать hello оставшихся служб. Здравствуйте, подход и работает независимо от того, выделяемый tooavoid сбой hello. Например, если выбран диапазон 10.0.0.0/20 для виртуальной сети, для пространства клиентских адресов можно выбрать 10.1.0.0/24. Если вас беспокоит hello сбоя сети хранения данных, при запуске через несколько сетей SAN. Если вас беспокоит hello потери стойки серверов, при запуске среди нескольких стоек. Если вы сомневаетесь hello потеря центров обработки данных, в нескольких регионах Azure или центров обработки данных будет запускаться служба. 

При выполнении такого режима вы по-прежнему toosome типов субъекта одновременных отказов, но single и даже нескольких неудачных попыток определенного типа (ex: одной виртуальной Машины или сети ссылку неуспешно) обрабатываются автоматически (и поэтому больше не «сбой»). Service Fabric предоставляет много механизмов развертывание кластера hello и дескрипторы возвращения неисправные узлы и службы. Service Fabric также позволяет запускать несколько экземпляров служб в порядке tooavoid эти типы незапланированных сбоев из Включение в реальных бедствия.

Могут существовать причины, почему работающих развертывания достаточно большой toospan за сбои не представляется возможным. Например, может потребоваться дополнительные аппаратные ресурсы не готов toopay для относительного toohello вероятность появления ошибки. При работе с распределенными приложениями дополнительные прыжки или репликация состояний между географическими регионами могут привести к неприемлемому уровню задержек. Этот уровень отличается для каждого приложения. Для сбоев программного обеспечения в частности, hello ошибки может быть в службе hello при попытке tooscale. В этом случае дополнительные копии не авария hello, так как условие сбоя hello связан во всех экземплярах hello.

### <a name="operational-faults"></a>Проблемы с работоспособностью
Даже если служба является распределены по земной шар hello с многих избыточных данных, он по-прежнему могут возникать катастрофических событий. Например, если кто-то случайно перенастраивает hello DNS-имя для службы hello, или удаляет сразу. Например, предположим, что имеется служба Service Fabric с отслеживанием состояния и кто-то случайно ее удалил. При отсутствии других по уменьшению эту службу и все состояния hello, существовавший — исчез. Для этих типов функциональных аварий (ошибок) требуются другие методы устранения рисков и шаги для восстановления, отличные от обычных незапланированных сбоев. 

лучшие способы tooavoid Hello эти типы ошибок оперативной предполагается
1. Ограничение доступа к рабочей среде toohello
2. Строгий аудит небезопасных операций.
3. применить автоматизации, предотвращает вручную или из внешнего изменения и проверять определенные изменения фактических среде hello перед их применение
4. Гарантирование, что разрушительные операции будут "мягкими". "Мягкие операции" не вступают в силу немедленно и могут быть отменены в рамках определенного временного окна.

Service Fabric предоставляет некоторые механизмы tooprevent оперативной сбоев, например обеспечении [ролей](service-fabric-cluster-security-roles.md) доступа элемента управления для операций кластера. Однако для предотвращения большинства таких функциональных сбоев требуются организационные усилия и другие методы. Service Fabric предоставляет ряд механизмов для преодоления функциональных ошибок, в частности резервное копирование и восстановление служб с отслеживанием состояния.

## <a name="managing-failures"></a>Обработка сбоев
целью Hello Service Fabric почти всегда является автоматическое управление ошибками. Тем не менее, чтобы toohandle некоторых типов сбоев, службы должны иметь дополнительный код. Другие типы сбоев _не_ должны обрабатываться автоматически по причинам обеспечения непрерывности бизнес-процессов и безопасности. 

### <a name="handling-single-failures"></a>Обработка единичных сбоев
Единичные компьютеры могут выйти из строя по множеству причин. В некоторых случаях — это сбои оборудования, как например сбои источников питания или сетевого оборудования, а в других — сбои программного обеспечения. К ним относятся сбоев hello операционной системы и саму службу hello. Service Fabric автоматически обнаруживает эти типы сбоев, включая ситуации, когда становится изолирован от других компьютеров, из-за проблем с toonetwork машины hello.

Hello тип службы независимо от одного экземпляра результатов приведет к простоям для этой службы, если для какой-либо причине не удается этой копии кода hello. 

В порядке toohandle любой сбой одного hello простой, что можно сделать, — tooensure запуска служб на более чем один узел по умолчанию. Для служб без отслеживания состояния это можно сделать, задав для параметра `InstanceCount` значение больше 1. Для служб с отслеживанием состояния hello минимальное рекомендуется всегда `TargetReplicaSetSize` и `MinReplicaSetSize` по крайней мере 3. Запуск нескольких копий кода службы гарантирует, что служба может автоматически обработать любой единичный отказ. 

### <a name="handling-coordinated-failures"></a>Обработка координированных сбоев
Скоординированное сбои могут происходить в кластере из-за tooeither плановой или инфраструктуры незапланированных сбоев и изменения или изменения запланированного программного обеспечения. Service Fabric моделирует зоны инфраструктуры, в которых возникают скоординированные сбои, как домены сбоя. Области, в которых будут возникать координируемые изменения программного обеспечения, моделируются как домены обновления. Дополнительные сведения о доменах сбоя и обновления см. [в этом документе](service-fabric-cluster-resource-manager-cluster-description.md), в котором описываются определение и топология кластера.

По умолчанию Service Fabric учитывает домены сбоя и обновления при планировании места запуска служб. По умолчанию Service Fabric предпринимает tooensure запуска служб в нескольких доменах сбоя и обновления, если изменения запланированного или незапланированного служб остаются доступными. 

Например предположим, что сбой источника питания вызывает стойки toofail машины одновременно. Несколько копий hello службой, работающей hello потере нескольких машин в сбоя домена сбоя превращается в только что еще один пример сбой одного для данной службы. Именно поэтому критических tooensuring высокий уровень доступности служб — Управление доменами отказоустойчивости. При запуске Service Fabric в Azure домены сбоя обрабатываются автоматически. В других средах процедура может быть отличной. При создании кластеров на локальном компьютере будет убедиться, что toomap и неправильно запланировали макет домена сбоя.

Домены обновления полезны для моделирования областей, где программного обеспечения будет обновлен toobe с hello же время. По этой причине домены обновления также часто определяют границы hello, где завершена программного обеспечения во время запланированного обновления. Обновления структуры службы и службы выполните hello одной модели. Дополнительные сведения о последовательного обновления, обновления доменов и модель работоспособности Service Fabric hello, которая помогает предотвратить непредвиденные изменения, влияющие на кластер hello и службы см. Эти документы:

 - [Обновление приложения](service-fabric-application-upgrade.md)
 - [Руководство по обновлению приложений Service Fabric с помощью Visual Studio](service-fabric-application-upgrade-tutorial.md)
 - [Общие сведения о наблюдении за работоспособностью системы в Service Fabric](service-fabric-health-introduction.md)

Вы можете визуализировать макета hello кластера с помощью схемы кластера hello в [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):

<center>
![Узлы, распределенные между доменами сбоев в Service Fabric Explorer][sfx-cluster-map]
</center>

> [!NOTE]
> Моделирование областей сбоя последовательного обновления, много экземпляров службы кода и состояние, tooensure правила размещения служб выполняться в доменах сбоя и обновления; Наблюдение за работоспособностью встроенных не так просто **некоторые** из hello функции, предоставляемые Service Fabric в порядке tookeep обычных рабочих проблем и сбоев из Включение в чрезвычайных происшествий. 
>

### <a name="handling-simultaneous-hardware-or-software-failures"></a>Обработка одновременных сбоев оборудования или программного обеспечения
Выше речь шла о единичных сбоях. Как видите, являются просто toohandle для служб без отслеживания состояния и с отслеживанием состояния с созданием нескольких копий кода hello (и состояние) работает сбоя и доменах обновления. Также возможно возникновение нескольких одновременных случайных сбоев. Это, скорее всего, фактический аварийного tooan toolead.


### <a name="random-failures-leading-tooservice-failures"></a>Случайные сбои начальные tooservice сбоев
Предположим, что hello имела `InstanceCount` 5 и несколько узлов под управлением этих экземпляров всех произошла ошибка на уровне hello же время. Service Fabric отвечает путем автоматического создания экземпляров для замены на других узлах. Он по-прежнему Создание замены до службы hello обратно tooits желаемое число экземпляров. Например, предположим, что было службы без отслеживания состояния с `InstanceCount`-1, то есть оно выполняется на всех узлах кластера hello допустимым. Предположим, что некоторые из этих экземпляров были toofail. В этом случае Service Fabric уведомления службы hello не находится в нужное состояние, которое пытается toocreate hello экземпляров на узлах hello, где они отсутствуют. 

Для служб с отслеживанием состояния ситуации hello зависит от того, ли служба hello имеет постоянное состояние, или нет. Он также зависит от сколько реплик hello имела и число неудачных. Определение, случился ли сбой службы с отслеживанием состояния, и его обработка состоят из трех этапов:

1. Определение, произошла ли потеря кворума.
 - Потери кворума, осуществляется большую часть hello реплики службы с отслеживанием состояния не работают в hello одновременно, включая hello первичной.
2. Определение, если потери кворума hello постоянной
 - Большую часть времени hello сбои являются временными. Перезапускаются процессы, узлы и виртуальные машины, а также выполняется автоматическое восстановление секций сети. Однако иногда сбои являются постоянными. 
    - Для служб без сохраненного состояния потеря кворума или нескольких реплик _немедленно_ приводит к постоянной потере кворума. Когда Service Fabric обнаруживает потери кворума в непостоянный службы с отслеживанием состояния, он проходит немедленно dataloss toostep 3, объявив (вероятность). Продолжить toodataloss смысл, что Service Fabric знает, что нет смысла в ожидание hello реплик toocome назад, поскольку даже если они были восстановлены, они будут пустыми.
    - Для постоянных служб с отслеживанием состояния сбой кворума или нескольких реплик вызывает toostart Service Fabric ожидания обратно toocome реплик hello и восстановления кворума. Это приводит к простою службы для любого _записывает_ toohello затронутые секции (или «реплика»), службы hello. Тем не менее операции чтения по-прежнему могут выполняться с пониженным уровнем обеспечения согласованности. количество времени, Service Fabric ожидает от кворума toobe восстановления по умолчанию Hello бесконечно, поскольку продолжением (возможного) события dataloss и выполняет другие риски. Переопределение по умолчанию hello `QuorumLossWaitDuration` значение возможна, но не рекомендуется. Вместо этого в данный момент все усилия должны быть доступны hello toorestore вниз реплик. Это требует переключать hello узлы, которые не работают резервное копирование и гарантирует, что можно заново подключить hello диски хранения локального состояния постоянных hello. Если потери кворума hello причиной является сбой процесса, Service Fabric автоматически пытается toorecreate hello процессов и перезапустите реплик hello внутри них. В случае сбоя Service Fabric сообщит об ошибках работоспособности. Если это не разрешается hello реплики обычно возобновить. В некоторых случаях, hello реплик не удается подключить обратно. Например диски hello все возможно, произошел сбой, или машин hello физически разрушается каким-либо образом. В таких случаях потеря кворума будет постоянной. toostop Service Fabric tootell ожидание hello вниз реплик, toocome администратор кластера необходимо определить, какие секции, в которых затрагиваются службы и вызывать hello `Repair-ServiceFabricPartition -PartitionId` или ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.  Этот API позволяет указывать идентификатор hello hello секции toomove из QuorumLoss и отправляются потенциальных dataloss.

> [!NOTE]
> Это _никогда не_ безопасном toouse этого API-интерфейса отличный от целевой для конкретных секций. 
>

3. Определение наличия фактической потери данных и восстановление из резервных копий
  - Когда Service Fabric вызывает hello `OnDataLossAsync` метод бывает из-за _подозревается_ dataloss. Service Fabric гарантирует, что этот вызов доставляется toohello _наиболее_ оставшиеся реплики. Это происходит, какая реплика завершает очередной этап hello большинство. Здравствуйте причина мы говорим всегда _подозревается_ dataloss заключается в возможности этой реплики оставшиеся hello фактически имеет все же состояние hello основной было указано при его вышел из строя. Однако без этого состояния toocompare его, никак не хорошо для tooknow Service Fabric или операторы для уверенности. На этом этапе Service Fabric узнает hello реплик не приходящие обратно. Это было hello, принятое остановив ожидание tooresolve потери кворума hello сам. Hello наилучшее решение для hello службы обычно является toofreeze и ожидания для определенных вмешательства администратора. Таким образом, что делает типичной реализации hello `OnDataLossAsync` метод выполнить?
  - Во-первых, запись о запуске `OnDataLossAsync` и запуск необходимых административных оповещений.
   - Обычно на этом этапе toopause и ожидания для дальнейшего решения и toobe ручные действия выполнены. Это, поскольку даже если резервные копии доступны, они должны toobe подготовлен. Например если две разные службы координации сведения, эти резервные копии может потребоваться toobe изменения в tooensure порядке, когда происходит восстановление hello, hello сведения эти две службы важна согласованной. 
  - Часто также есть некоторые другие телеметрии или стороне из службы hello. Эти метаданные могут содержаться в других службах или в журналах. Эти сведения можно использовать необходимые toodetermine будто все вызовы получаются и обрабатываются в основной hello, которые отсутствовали в некоторой репликой резервного копирования или реплицированной toothis hello. Эти может понадобиться резервного копирования toobe toohello воспроизводимой либо добавлен прежде, чем восстановление невозможно.  
   - Сравнения оставшихся toothat состояние реплики, содержащихся в резервные копии, доступные hello. Если с помощью надежного коллекций Service Fabric hello, то есть средства и процедуры для таким образом, описанной в [в этой статье](service-fabric-reliable-services-backup-restore.md). Цель Hello, toosee, если достаточно hello штата в рамках hello реплики, либо также могут отсутствовать какие hello резервного копирования.
  - После этого сравнения hello и при завершении восстановления необходимых hello, код службы hello должен возвращать значение true, если были внесены изменения состояния. Если реплика hello определил его hello лучше всего доступных копия hello состояния и не изменяются, возвращается значение false. Значение true указывает, что все _остальные_ оставшиеся реплики теперь не согласованы с этой репликой. Они будут удалены и пересозданы из этой реплики. Значение false указывает, что состояние было внесено никаких изменений, поэтому hello реплик, можно сохранить их. 

Очень важно, чтобы разработчики службы смоделировали потенциальную потерю данных и сценарии сбоев, прежде чем развертывать службы в рабочей среде. tooprotect от возможности hello объекта dataloss важно tooperiodically [резервное копирование состояния hello](service-fabric-reliable-services-backup-restore.md) любого географически избыточное хранилище tooa служб с отслеживанием состояния. Также необходимо убедиться, что имеется возможность toorestore hello его. Поскольку резервные копии различных служб создаются в разное время, необходимо tooensure, что после восстановления службы имеют согласованное представление друг с другом. Например рассмотрим ситуацию, где одна служба создает число и сохраняет его и отправляет ее tooanother службу, которая также сохраняет его. После восстановления можно обнаружить вторую службу hello имеет номер hello, но hello сначала не так, поскольку его резервная копия не включать эту операцию.

Если можно узнать, что осталось hello реплики находятся недостаточно toocontinue из в сценарии dataloss и не может восстановить состояние службы из телеметрии или выпуска, hello частоту архивации определяет вашей наиболее точки возможности восстановления (RPO) . Service Fabric предоставляет множество средств для тестирования различных сценариев сбоя, включая постоянный кворум и потерю данных, для которой требуется восстановление из резервной копии. Эти сценарии включены как часть средств тестирования Service Fabric, управляется hello ошибки Analysis Service. Дополнительные сведения об этих средствах и шаблонах см. [здесь](service-fabric-testability-overview.md). 

> [!NOTE]
> Системные службы также возможна потеря кворума, с hello влияние конкретных toohello службы в вопросе. Для экземпляра потери кворума в службе имен hello влияет на разрешение имен, тогда как потери кворума в службе диспетчера отработки отказа hello блокирует создание новой службы и переход на другой ресурс. Хотя службы system Service Fabric hello следуют hello же шаблон как служб для управления состоянием, не рекомендуется попытаться toomove их из потери кворума и в потенциальных dataloss. Hello рекомендуется вместо этого слишком[поиска поддержки](service-fabric-support.md) toodetermine решение, которое является целевым tooyour конкретной ситуации.  Обычно бывает предпочтительнее toosimply ожидания до hello вниз возвращаемого реплик.
>

## <a name="availability-of-hello-service-fabric-cluster"></a>Доступность кластера Service Fabric hello
Вообще говоря самого кластера Service Fabric hello является распределенной среде с единых точек отказа. Сбой одного из узлов не приведет доступности или проблемы с надежностью hello кластер, в основном из-за системных служб Service Fabric hello выполните hello же правилами ранее указанного: они всегда выполняются с менее трех реплик по по умолчанию, а также системные службы, которые не имеют состояния работу на всех узлах. полностью распределения базовой сети Service Fabric Hello и слои обнаружения сбоя. Большинство системных служб могут быть перестроены из метаданных в кластере hello или узнать, как tooresynchronize свое состояние из других местах. доступность Hello hello кластера может стать ненадежным, если системные службы завладеть кворума потери такой ситуации описанных выше. В таких случаях может оказаться определенные операции в кластере hello, такие как запуск обновления или развертывание новых служб, но самого кластера hello работает по-прежнему может tooperform. Службы уже после запуска сможет продолжить работу в таких условиях, если им требуется система служб записи toohello toocontinue функционирует. Например при потере кворума hello Диспетчер отказоустойчивости все службы будут toorun, но все службы, которые не удалось не будет возможности tooautomatically перезагрузки, поскольку для этого требуется участие hello hello Диспетчер отказоустойчивости. 

### <a name="failures-of-a-datacenter-or-azure-region"></a>Сбои в центре обработки данных или регионе Azure
В редких случаях физическом центре обработки данных может стать недоступным из-за tooloss из управления питанием или сетевое подключение. В этих случаях кластеры Service Fabric, а также службы в таких центрах обработки данных или в регионе Azure будут недоступны. Тем не менее _данные сохраняются_. Для кластеров, работающих в Azure, вы можете просмотреть обновления на сбоев на hello [страница Azure состояния][azure-status-dashboard]. В hello очень маловероятно, что физическом центре обработки данных является частично или полностью удаляется, любых кластеров Service Fabric размещенных существует или hello служб внутри них могут быть потеряны. Сюда входят все состояния, для которых не были созданы резервные копии за пределами этого центра обработки данных или региона.

Есть два различных подхода к оставшимся hello постоянными или неустранимого сбоя в одном центре обработки данных или. 

1. Запустите отдельные кластеры Service Fabric в нескольких регионах и используйте механизм для отработки отказа и восстановления размещения между этими средами. Для такой мультикластерной модели типа "активный — активный" или "активный — пассивный" требуется дополнительный код для управления и операций. Также требуется координации резервных копий из служб hello в одном центре обработки данных или области, чтобы они были доступны в других центрах обработки данных или регионах, когда один сбоя. 
2. Запустите кластер Service Fabric, охватывающий несколько центров обработки данных или регионов. Hello минимальные системные требования для данного — три центрах обработки данных или области. Hello рекомендуемое число областей или центрах обработки данных, равно 5. Это предполагает более сложную топологию кластера. Однако hello преимущество этой модели является, сбой одного центра обработки данных будет преобразован в обычном сбоя после аварии. Эти ошибки могут быть обработаны механизмы hello работающие для кластеров в одном регионе. Домены сбоя, домены обновления и правила размещения Service Fabric гарантируют, что нагрузки распределяются таким образом, чтобы выдерживать обычные сбои. Дополнительные сведения о политиках, которые помогают работать службам в таком типе кластеров, см. в статье [Политики размещения для служб Service Fabric](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)

### <a name="random-failures-leading-toocluster-failures"></a>Случайные сбои начальные toocluster сбоев
Service Fabric имеет концепции hello узлов начальное значение. Это, чтобы обеспечить доступность hello hello базовый кластер узлов. Эти узлы помогают tooensure hello кластер остается копию, установив аренды с другими узлами и выступает в роли таков во время определенных видов сбоев сети. Если они не будут возвращены случайные сбои удаления большинства hello начальное число узлов в кластере hello, hello кластера автоматически завершает работу. В Azure, автоматически управляет узлы начальное значение: они распределяются по hello доступных сбоя и доменах обновления, и если один начальный узел удаляется из кластера hello вместо него будет создан еще один. 

В автономных кластеров Service Fabric и Azure hello «Основной тип узла» является hello, обрабатывающий hello начальные значения. При определении тип основного узла, Service Fabric автоматически будет воспользоваться преимуществами hello количество узлов, предоставляемый создании узлов too9 начального значения и 9 репликах каждого hello системных служб. Если большинство этих реплик службы системы извлекает набор случайные сбои одновременно, hello системных служб вводит потери кворума, как было описано выше. Если потеряны большинства узлов hello начальное значение, hello кластера завершит работу после.

## <a name="next-steps"></a>Дальнейшие действия
- Узнайте, как toosimulate различных сбоях, с помощью hello [framework тестирования](service-fabric-testability-overview.md)
- Ознакомьтесь с другими материалами по аварийному восстановлению и обеспечению высокой доступности. Корпорация Майкрософт опубликовала множество руководств по этим темам. Хотя некоторые из этих документов см. методы toospecific для использования в других продуктах, они содержат общие рекомендации по, которые можно применять в контексте Service Fabric hello также:
  - [Контрольный список для обеспечения доступности](../best-practices-availability-checklist.md)
  - [Отработка аварийного восстановления](../sql-database/sql-database-disaster-recovery-drills.md)
  - [Аварийное восстановление и высокий уровень доступности для приложений Azure][dr-ha-guide]
- Узнайте о [вариантах поддержки Service Fabric](service-fabric-support.md).

<!-- External links -->

[repair-partition-ps]: https://msdn.microsoft.com/library/mt163522.aspx
[azure-status-dashboard]:https://azure.microsoft.com/status/
[azure-regions]: https://azure.microsoft.com/regions/
[dr-ha-guide]: https://msdn.microsoft.com/library/azure/dn251004.aspx


<!-- Images -->

[sfx-cluster-map]: ./media/service-fabric-disaster-recovery/sfx-clustermap.png
