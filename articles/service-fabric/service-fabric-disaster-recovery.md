---
title: "aaaAzure аварийного восстановления Service Fabric | Документы Microsoft"
description: "Azure Service Fabric предлагает toodeal необходимые возможности hello со всеми типами бедствия. В этой статье описываются hello аварийных ситуаций, которые могут возникать и как toodeal с ними."
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: ab49c4b9-74a8-4907-b75b-8d2ee84c6d90
ms.service: service-fabric
ms.devlang: dotNet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 04b8348fb63e8a1c76a8f722c4c8255b339908e2
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/06/2017
---
# <a name="disaster-recovery-in-azure-service-fabric"></a><span data-ttu-id="f2784-104">Аварийное восстановление в Azure Service Fabric</span><span class="sxs-lookup"><span data-stu-id="f2784-104">Disaster recovery in Azure Service Fabric</span></span>
<span data-ttu-id="f2784-105">Для обеспечения высокого уровня доступности крайне важно гарантировать продолжение работы всех типов служб при любых сбоях.</span><span class="sxs-lookup"><span data-stu-id="f2784-105">A critical part of delivering high-availability is ensuring that services can survive all different types of failures.</span></span> <span data-ttu-id="f2784-106">Это особенно важно в ситуациях незапланированных сбоев, которые находятся вне вашего контроля.</span><span class="sxs-lookup"><span data-stu-id="f2784-106">This is especially important for failures that are unplanned and outside of your control.</span></span> <span data-ttu-id="f2784-107">В этой статье описываются некоторые часто встречающиеся виды сбоев, которые могут привести к авариям, если их не смоделировать и не взять под контроль должным образом.</span><span class="sxs-lookup"><span data-stu-id="f2784-107">This article describes some common failure modes that could be disasters if not modeled and managed correctly.</span></span> <span data-ttu-id="f2784-108">В нем также рассматриваются действия и способы их устранения tootake при аварии, несмотря на это произошло.</span><span class="sxs-lookup"><span data-stu-id="f2784-108">It also discuss mitigations and actions tootake if a disaster happened anyway.</span></span> <span data-ttu-id="f2784-109">Цель Hello является toolimit или исключить риск hello простою или потере данных, когда они происходят сбои, плановой или в противном случае.</span><span class="sxs-lookup"><span data-stu-id="f2784-109">hello goal is toolimit or eliminate hello risk of downtime or data loss when they occur failures, planned or otherwise, occur.</span></span>

## <a name="avoiding-disaster"></a><span data-ttu-id="f2784-110">Предотвращение аварий</span><span class="sxs-lookup"><span data-stu-id="f2784-110">Avoiding disaster</span></span>
<span data-ttu-id="f2784-111">Основная цель Service Fabric — toohelp модели среды и служб таким образом, что общие типы сбоев не аварий.</span><span class="sxs-lookup"><span data-stu-id="f2784-111">Service Fabric's primary goal is toohelp you model both your environment and your services in such a way that common failure types are not disasters.</span></span> 

<span data-ttu-id="f2784-112">В основном существует два типа сценариев аварий и сбоев:</span><span class="sxs-lookup"><span data-stu-id="f2784-112">In general there are two types of disaster/failure scenarios:</span></span>

1. <span data-ttu-id="f2784-113">Сбои оборудования или программного обеспечения.</span><span class="sxs-lookup"><span data-stu-id="f2784-113">Hardware or software faults</span></span>
2. <span data-ttu-id="f2784-114">Проблемы с работоспособностью.</span><span class="sxs-lookup"><span data-stu-id="f2784-114">Operational faults</span></span>

### <a name="hardware-and-software-faults"></a><span data-ttu-id="f2784-115">Сбои оборудования или программного обеспечения</span><span class="sxs-lookup"><span data-stu-id="f2784-115">Hardware and software faults</span></span>
<span data-ttu-id="f2784-116">Сбои оборудования и программного обеспечения — непредсказуемы.</span><span class="sxs-lookup"><span data-stu-id="f2784-116">Hardware and software faults are unpredictable.</span></span> <span data-ttu-id="f2784-117">Hello простым способом toosurvive ошибок выполняется несколько копий службы hello, охваченных через границы сбоя оборудования или программного обеспечения.</span><span class="sxs-lookup"><span data-stu-id="f2784-117">hello easiest way toosurvive faults is running more copies of hello service  spanned across hardware or software fault boundaries.</span></span> <span data-ttu-id="f2784-118">Например если служба выполняется только для одного конкретного компьютера, затем hello неудачного выполнения что один компьютер является аварийного для этой службы.</span><span class="sxs-lookup"><span data-stu-id="f2784-118">For example, if your service is running only on one particular machine, then hello failure of that one machine is a disaster for that service.</span></span> <span data-ttu-id="f2784-119">простой способ tooavoid Hello этот аварии, составляет tooensure, который фактически выполняется служба hello на нескольких компьютерах.</span><span class="sxs-lookup"><span data-stu-id="f2784-119">hello simple way tooavoid this disaster is tooensure that hello service is actually running on multiple machines.</span></span> <span data-ttu-id="f2784-120">Тестирование — это тоже сбоя необходимости tooensure hello одной машины не нарушать hello службой.</span><span class="sxs-lookup"><span data-stu-id="f2784-120">Testing is also necessary tooensure hello failure of one machine doesn't disrupt hello running service.</span></span> <span data-ttu-id="f2784-121">Планирование емкости гарантирует замены может быть создан экземпляр в другом месте и что снижение производительности не перегружать hello оставшихся служб.</span><span class="sxs-lookup"><span data-stu-id="f2784-121">Capacity planning ensures a replacement instance can be created elsewhere and that reduction in capacity doesn't overload hello remaining services.</span></span> <span data-ttu-id="f2784-122">Здравствуйте, подход и работает независимо от того, выделяемый tooavoid сбой hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-122">hello same pattern works regardless of what you're trying tooavoid hello failure of.</span></span> <span data-ttu-id="f2784-123">Например, если выбран диапазон 10.0.0.0/20 для виртуальной сети, для пространства клиентских адресов можно выбрать 10.1.0.0/24.</span><span class="sxs-lookup"><span data-stu-id="f2784-123">For example.</span></span> <span data-ttu-id="f2784-124">Если вас беспокоит hello сбоя сети хранения данных, при запуске через несколько сетей SAN.</span><span class="sxs-lookup"><span data-stu-id="f2784-124">if you're concerned about hello failure of a SAN, you run across multiple SANs.</span></span> <span data-ttu-id="f2784-125">Если вас беспокоит hello потери стойки серверов, при запуске среди нескольких стоек.</span><span class="sxs-lookup"><span data-stu-id="f2784-125">If you're concerned about hello loss of a rack of servers, you run across multiple racks.</span></span> <span data-ttu-id="f2784-126">Если вы сомневаетесь hello потеря центров обработки данных, в нескольких регионах Azure или центров обработки данных будет запускаться служба.</span><span class="sxs-lookup"><span data-stu-id="f2784-126">If you're worried about hello loss of datacenters, your service should run across multiple Azure regions or datacenters.</span></span> 

<span data-ttu-id="f2784-127">При выполнении такого режима вы по-прежнему toosome типов субъекта одновременных отказов, но single и даже нескольких неудачных попыток определенного типа (ex: одной виртуальной Машины или сети ссылку неуспешно) обрабатываются автоматически (и поэтому больше не «сбой»).</span><span class="sxs-lookup"><span data-stu-id="f2784-127">When running in this type of spanned mode, you're still subject toosome types of simultaneous failures, but single and even multiple failures of a particular type (ex: a single VM or network link failing) are automatically handled (and so no longer a "disaster").</span></span> <span data-ttu-id="f2784-128">Service Fabric предоставляет много механизмов развертывание кластера hello и дескрипторы возвращения неисправные узлы и службы.</span><span class="sxs-lookup"><span data-stu-id="f2784-128">Service Fabric provides many mechanisms for expanding hello cluster and handles bringing failed nodes and services back.</span></span> <span data-ttu-id="f2784-129">Service Fabric также позволяет запускать несколько экземпляров служб в порядке tooavoid эти типы незапланированных сбоев из Включение в реальных бедствия.</span><span class="sxs-lookup"><span data-stu-id="f2784-129">Service Fabric also allows running many instances of your services in order tooavoid these types of unplanned failures from turning into real disasters.</span></span>

<span data-ttu-id="f2784-130">Могут существовать причины, почему работающих развертывания достаточно большой toospan за сбои не представляется возможным.</span><span class="sxs-lookup"><span data-stu-id="f2784-130">There may be reasons why running a deployment large enough toospan over failures is not feasible.</span></span> <span data-ttu-id="f2784-131">Например, может потребоваться дополнительные аппаратные ресурсы не готов toopay для относительного toohello вероятность появления ошибки.</span><span class="sxs-lookup"><span data-stu-id="f2784-131">For example, it may take more hardware resources than you're not willing toopay for relative toohello chance of failure.</span></span> <span data-ttu-id="f2784-132">При работе с распределенными приложениями дополнительные прыжки или репликация состояний между географическими регионами могут привести к неприемлемому уровню задержек.</span><span class="sxs-lookup"><span data-stu-id="f2784-132">When dealing with distributed applications, it could be that additional communication hops or state replication costs across geographic distances causes unacceptable latency.</span></span> <span data-ttu-id="f2784-133">Этот уровень отличается для каждого приложения.</span><span class="sxs-lookup"><span data-stu-id="f2784-133">Where this line is drawn differs for each application.</span></span> <span data-ttu-id="f2784-134">Для сбоев программного обеспечения в частности, hello ошибки может быть в службе hello при попытке tooscale.</span><span class="sxs-lookup"><span data-stu-id="f2784-134">For software faults specifically, hello fault could be in hello service that you are trying tooscale.</span></span> <span data-ttu-id="f2784-135">В этом случае дополнительные копии не авария hello, так как условие сбоя hello связан во всех экземплярах hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-135">In this case more copies don't prevent hello disaster, since hello failure condition is correlated across all hello instances.</span></span>

### <a name="operational-faults"></a><span data-ttu-id="f2784-136">Проблемы с работоспособностью</span><span class="sxs-lookup"><span data-stu-id="f2784-136">Operational faults</span></span>
<span data-ttu-id="f2784-137">Даже если служба является распределены по земной шар hello с многих избыточных данных, он по-прежнему могут возникать катастрофических событий.</span><span class="sxs-lookup"><span data-stu-id="f2784-137">Even if your service is spanned across hello globe with many redundancies, it can still experience disastrous events.</span></span> <span data-ttu-id="f2784-138">Например, если кто-то случайно перенастраивает hello DNS-имя для службы hello, или удаляет сразу.</span><span class="sxs-lookup"><span data-stu-id="f2784-138">For example, if someone accidentally reconfigures hello dns name for hello service, or deletes it outright.</span></span> <span data-ttu-id="f2784-139">Например, предположим, что имеется служба Service Fabric с отслеживанием состояния и кто-то случайно ее удалил.</span><span class="sxs-lookup"><span data-stu-id="f2784-139">As an example, let's say you had a stateful Service Fabric service, and someone deleted that service accidentally.</span></span> <span data-ttu-id="f2784-140">При отсутствии других по уменьшению эту службу и все состояния hello, существовавший — исчез.</span><span class="sxs-lookup"><span data-stu-id="f2784-140">Unless there's some other mitigation, that service and all of hello state it had is now gone.</span></span> <span data-ttu-id="f2784-141">Для этих типов функциональных аварий (ошибок) требуются другие методы устранения рисков и шаги для восстановления, отличные от обычных незапланированных сбоев.</span><span class="sxs-lookup"><span data-stu-id="f2784-141">These types of operational disasters ("oops") require different mitigations and steps for recovery than regular unplanned failures.</span></span> 

<span data-ttu-id="f2784-142">лучшие способы tooavoid Hello эти типы ошибок оперативной предполагается</span><span class="sxs-lookup"><span data-stu-id="f2784-142">hello best ways tooavoid these types of operational faults are to</span></span>
1. <span data-ttu-id="f2784-143">Ограничение доступа к рабочей среде toohello</span><span class="sxs-lookup"><span data-stu-id="f2784-143">restrict operational access toohello environment</span></span>
2. <span data-ttu-id="f2784-144">Строгий аудит небезопасных операций.</span><span class="sxs-lookup"><span data-stu-id="f2784-144">strictly audit dangerous operations</span></span>
3. <span data-ttu-id="f2784-145">применить автоматизации, предотвращает вручную или из внешнего изменения и проверять определенные изменения фактических среде hello перед их применение</span><span class="sxs-lookup"><span data-stu-id="f2784-145">impose automation, prevent manual or out of band changes, and validate specific changes against hello actual environment before enacting them</span></span>
4. <span data-ttu-id="f2784-146">Гарантирование, что разрушительные операции будут "мягкими".</span><span class="sxs-lookup"><span data-stu-id="f2784-146">ensure that destructive operations are "soft".</span></span> <span data-ttu-id="f2784-147">"Мягкие операции" не вступают в силу немедленно и могут быть отменены в рамках определенного временного окна.</span><span class="sxs-lookup"><span data-stu-id="f2784-147">Soft operations don't take effect immediately or can be undone within some time window</span></span>

<span data-ttu-id="f2784-148">Service Fabric предоставляет некоторые механизмы tooprevent оперативной сбоев, например обеспечении [ролей](service-fabric-cluster-security-roles.md) доступа элемента управления для операций кластера.</span><span class="sxs-lookup"><span data-stu-id="f2784-148">Service Fabric provides some mechanisms tooprevent operational faults, such as providing [role-based](service-fabric-cluster-security-roles.md) access control for cluster operations.</span></span> <span data-ttu-id="f2784-149">Однако для предотвращения большинства таких функциональных сбоев требуются организационные усилия и другие методы.</span><span class="sxs-lookup"><span data-stu-id="f2784-149">However, most of these operational faults require organizational efforts and other systems.</span></span> <span data-ttu-id="f2784-150">Service Fabric предоставляет ряд механизмов для преодоления функциональных ошибок, в частности резервное копирование и восстановление служб с отслеживанием состояния.</span><span class="sxs-lookup"><span data-stu-id="f2784-150">Service Fabric does provide some mechanism for surviving operational faults, most notably backup and restore for stateful services.</span></span>

## <a name="managing-failures"></a><span data-ttu-id="f2784-151">Обработка сбоев</span><span class="sxs-lookup"><span data-stu-id="f2784-151">Managing failures</span></span>
<span data-ttu-id="f2784-152">целью Hello Service Fabric почти всегда является автоматическое управление ошибками.</span><span class="sxs-lookup"><span data-stu-id="f2784-152">hello goal of Service Fabric is almost always automatic management of failures.</span></span> <span data-ttu-id="f2784-153">Тем не менее, чтобы toohandle некоторых типов сбоев, службы должны иметь дополнительный код.</span><span class="sxs-lookup"><span data-stu-id="f2784-153">However, in order toohandle some types of failures, services must have additional code.</span></span> <span data-ttu-id="f2784-154">Другие типы сбоев _не_ должны обрабатываться автоматически по причинам обеспечения непрерывности бизнес-процессов и безопасности.</span><span class="sxs-lookup"><span data-stu-id="f2784-154">Other types of failures should _not_ be automatically addressed because of safety and business continuity reasons.</span></span> 

### <a name="handling-single-failures"></a><span data-ttu-id="f2784-155">Обработка единичных сбоев</span><span class="sxs-lookup"><span data-stu-id="f2784-155">Handling single failures</span></span>
<span data-ttu-id="f2784-156">Единичные компьютеры могут выйти из строя по множеству причин.</span><span class="sxs-lookup"><span data-stu-id="f2784-156">Single machines can fail for all sorts of reasons.</span></span> <span data-ttu-id="f2784-157">В некоторых случаях — это сбои оборудования, как например сбои источников питания или сетевого оборудования,</span><span class="sxs-lookup"><span data-stu-id="f2784-157">Some of these are hardware causes, like power supplies and networking hardware failures.</span></span> <span data-ttu-id="f2784-158">а в других — сбои программного обеспечения.</span><span class="sxs-lookup"><span data-stu-id="f2784-158">Other failures are in software.</span></span> <span data-ttu-id="f2784-159">К ним относятся сбоев hello операционной системы и саму службу hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-159">These include failures of hello actual operating system and hello service itself.</span></span> <span data-ttu-id="f2784-160">Service Fabric автоматически обнаруживает эти типы сбоев, включая ситуации, когда становится изолирован от других компьютеров, из-за проблем с toonetwork машины hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-160">Service Fabric automatically detects these types of failures, including cases where hello machine becomes isolated from other machines due toonetwork issues.</span></span>

<span data-ttu-id="f2784-161">Hello тип службы независимо от одного экземпляра результатов приведет к простоям для этой службы, если для какой-либо причине не удается этой копии кода hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-161">Regardless of hello type of service, running a single instance results in downtime for that service if that single copy of hello code fails for any reason.</span></span> 

<span data-ttu-id="f2784-162">В порядке toohandle любой сбой одного hello простой, что можно сделать, — tooensure запуска служб на более чем один узел по умолчанию.</span><span class="sxs-lookup"><span data-stu-id="f2784-162">In order toohandle any single failure, hello simplest thing you can do is tooensure that your services run on more than one node by default.</span></span> <span data-ttu-id="f2784-163">Для служб без отслеживания состояния это можно сделать, задав для параметра `InstanceCount` значение больше 1.</span><span class="sxs-lookup"><span data-stu-id="f2784-163">For stateless services, this can be accomplished by having an `InstanceCount` greater than 1.</span></span> <span data-ttu-id="f2784-164">Для служб с отслеживанием состояния hello минимальное рекомендуется всегда `TargetReplicaSetSize` и `MinReplicaSetSize` по крайней мере 3.</span><span class="sxs-lookup"><span data-stu-id="f2784-164">For stateful services, hello minimum recommendation is always a `TargetReplicaSetSize` and `MinReplicaSetSize` of at least 3.</span></span> <span data-ttu-id="f2784-165">Запуск нескольких копий кода службы гарантирует, что служба может автоматически обработать любой единичный отказ.</span><span class="sxs-lookup"><span data-stu-id="f2784-165">Running more copies of your service code ensures that your service can handle any single failure automatically.</span></span> 

### <a name="handling-coordinated-failures"></a><span data-ttu-id="f2784-166">Обработка координированных сбоев</span><span class="sxs-lookup"><span data-stu-id="f2784-166">Handling coordinated failures</span></span>
<span data-ttu-id="f2784-167">Скоординированное сбои могут происходить в кластере из-за tooeither плановой или инфраструктуры незапланированных сбоев и изменения или изменения запланированного программного обеспечения.</span><span class="sxs-lookup"><span data-stu-id="f2784-167">Coordinated failures can happen in a cluster due tooeither planned or unplanned infrastructure failures and changes, or planned software changes.</span></span> <span data-ttu-id="f2784-168">Service Fabric моделирует зоны инфраструктуры, в которых возникают скоординированные сбои, как домены сбоя.</span><span class="sxs-lookup"><span data-stu-id="f2784-168">Service Fabric models infrastructure zones that experience coordinated failures as Fault Domains.</span></span> <span data-ttu-id="f2784-169">Области, в которых будут возникать координируемые изменения программного обеспечения, моделируются как домены обновления.</span><span class="sxs-lookup"><span data-stu-id="f2784-169">Areas that will experience coordinated software changes are modeled as Upgrade Domains.</span></span> <span data-ttu-id="f2784-170">Дополнительные сведения о доменах сбоя и обновления см. [в этом документе](service-fabric-cluster-resource-manager-cluster-description.md), в котором описываются определение и топология кластера.</span><span class="sxs-lookup"><span data-stu-id="f2784-170">More information about fault and upgrade domains is in [this document](service-fabric-cluster-resource-manager-cluster-description.md) that describes cluster topology and definition.</span></span>

<span data-ttu-id="f2784-171">По умолчанию Service Fabric учитывает домены сбоя и обновления при планировании места запуска служб.</span><span class="sxs-lookup"><span data-stu-id="f2784-171">By default Service Fabric considers fault and upgrade domains when planning where your services should run.</span></span> <span data-ttu-id="f2784-172">По умолчанию Service Fabric предпринимает tooensure запуска служб в нескольких доменах сбоя и обновления, если изменения запланированного или незапланированного служб остаются доступными.</span><span class="sxs-lookup"><span data-stu-id="f2784-172">By default, Service Fabric tries tooensure that your services run across several fault and upgrade domains so if planned or unplanned changes happen your services remain available.</span></span> 

<span data-ttu-id="f2784-173">Например предположим, что сбой источника питания вызывает стойки toofail машины одновременно.</span><span class="sxs-lookup"><span data-stu-id="f2784-173">For example, let's say that failure of a power source causes a rack of machines toofail simultaneously.</span></span> <span data-ttu-id="f2784-174">Несколько копий hello службой, работающей hello потере нескольких машин в сбоя домена сбоя превращается в только что еще один пример сбой одного для данной службы.</span><span class="sxs-lookup"><span data-stu-id="f2784-174">With multiple copies of hello service running hello loss of many machines in fault domain failure turns into just another example of single failure for a given service.</span></span> <span data-ttu-id="f2784-175">Именно поэтому критических tooensuring высокий уровень доступности служб — Управление доменами отказоустойчивости.</span><span class="sxs-lookup"><span data-stu-id="f2784-175">This is why managing fault domains is critical tooensuring high availability of your services.</span></span> <span data-ttu-id="f2784-176">При запуске Service Fabric в Azure домены сбоя обрабатываются автоматически.</span><span class="sxs-lookup"><span data-stu-id="f2784-176">When running Service Fabric in Azure, fault domains are managed automatically.</span></span> <span data-ttu-id="f2784-177">В других средах процедура может быть отличной.</span><span class="sxs-lookup"><span data-stu-id="f2784-177">In other environments they may not be.</span></span> <span data-ttu-id="f2784-178">При создании кластеров на локальном компьютере будет убедиться, что toomap и неправильно запланировали макет домена сбоя.</span><span class="sxs-lookup"><span data-stu-id="f2784-178">If you're building your own clusters on premises, be sure toomap and plan your fault domain layout correctly.</span></span>

<span data-ttu-id="f2784-179">Домены обновления полезны для моделирования областей, где программного обеспечения будет обновлен toobe с hello же время.</span><span class="sxs-lookup"><span data-stu-id="f2784-179">Upgrade Domains are useful for modeling areas where software is going toobe upgraded at hello same time.</span></span> <span data-ttu-id="f2784-180">По этой причине домены обновления также часто определяют границы hello, где завершена программного обеспечения во время запланированного обновления.</span><span class="sxs-lookup"><span data-stu-id="f2784-180">Because of this, Upgrade Domains also often define hello boundaries where software is taken down during planned upgrades.</span></span> <span data-ttu-id="f2784-181">Обновления структуры службы и службы выполните hello одной модели.</span><span class="sxs-lookup"><span data-stu-id="f2784-181">Upgrades of both Service Fabric and your services follow hello same model.</span></span> <span data-ttu-id="f2784-182">Дополнительные сведения о последовательного обновления, обновления доменов и модель работоспособности Service Fabric hello, которая помогает предотвратить непредвиденные изменения, влияющие на кластер hello и службы см. Эти документы:</span><span class="sxs-lookup"><span data-stu-id="f2784-182">For more on rolling upgrades, upgrade domains, and hello Service Fabric health model that helps prevent unintended changes from impacting hello cluster and your service, see these documents:</span></span>

 - [<span data-ttu-id="f2784-183">Обновление приложения</span><span class="sxs-lookup"><span data-stu-id="f2784-183">Application Upgrade</span></span>](service-fabric-application-upgrade.md)
 - [<span data-ttu-id="f2784-184">Руководство по обновлению приложений Service Fabric с помощью Visual Studio</span><span class="sxs-lookup"><span data-stu-id="f2784-184">Application Upgrade Tutorial</span></span>](service-fabric-application-upgrade-tutorial.md)
 - [<span data-ttu-id="f2784-185">Общие сведения о наблюдении за работоспособностью системы в Service Fabric</span><span class="sxs-lookup"><span data-stu-id="f2784-185">Service Fabric Health Model</span></span>](service-fabric-health-introduction.md)

<span data-ttu-id="f2784-186">Вы можете визуализировать макета hello кластера с помощью схемы кластера hello в [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span><span class="sxs-lookup"><span data-stu-id="f2784-186">You can visualize hello layout of your cluster using hello cluster map provided in [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span></span>

<span data-ttu-id="f2784-187"><center>
![Узлы, распределенные между доменами сбоев в Service Fabric Explorer][sfx-cluster-map]
</center></span><span class="sxs-lookup"><span data-stu-id="f2784-187"><center>
![Nodes spread across fault domains in Service Fabric Explorer][sfx-cluster-map]
</center></span></span>

> [!NOTE]
> <span data-ttu-id="f2784-188">Моделирование областей сбоя последовательного обновления, много экземпляров службы кода и состояние, tooensure правила размещения служб выполняться в доменах сбоя и обновления; Наблюдение за работоспособностью встроенных не так просто **некоторые** из hello функции, предоставляемые Service Fabric в порядке tookeep обычных рабочих проблем и сбоев из Включение в чрезвычайных происшествий.</span><span class="sxs-lookup"><span data-stu-id="f2784-188">Modeling areas of failure, rolling upgrades, running many instances of your service code and state, placement rules tooensure your services run across fault and upgrade domains, and built-in health monitoring are just **some** of hello features that Service Fabric provides in order tookeep normal operational issues and failures from turning into disasters.</span></span> 
>

### <a name="handling-simultaneous-hardware-or-software-failures"></a><span data-ttu-id="f2784-189">Обработка одновременных сбоев оборудования или программного обеспечения</span><span class="sxs-lookup"><span data-stu-id="f2784-189">Handling simultaneous hardware or software failures</span></span>
<span data-ttu-id="f2784-190">Выше речь шла о единичных сбоях.</span><span class="sxs-lookup"><span data-stu-id="f2784-190">Above we talked about single failures.</span></span> <span data-ttu-id="f2784-191">Как видите, являются просто toohandle для служб без отслеживания состояния и с отслеживанием состояния с созданием нескольких копий кода hello (и состояние) работает сбоя и доменах обновления.</span><span class="sxs-lookup"><span data-stu-id="f2784-191">As you can see, are easy toohandle for both stateless and stateful services just by keeping more copies of hello code (and state) running across fault and upgrade domains.</span></span> <span data-ttu-id="f2784-192">Также возможно возникновение нескольких одновременных случайных сбоев.</span><span class="sxs-lookup"><span data-stu-id="f2784-192">Multiple simultaneous random failures can also happen.</span></span> <span data-ttu-id="f2784-193">Это, скорее всего, фактический аварийного tooan toolead.</span><span class="sxs-lookup"><span data-stu-id="f2784-193">These are more likely toolead tooan actual disaster.</span></span>


### <a name="random-failures-leading-tooservice-failures"></a><span data-ttu-id="f2784-194">Случайные сбои начальные tooservice сбоев</span><span class="sxs-lookup"><span data-stu-id="f2784-194">Random failures leading tooservice failures</span></span>
<span data-ttu-id="f2784-195">Предположим, что hello имела `InstanceCount` 5 и несколько узлов под управлением этих экземпляров всех произошла ошибка на уровне hello же время.</span><span class="sxs-lookup"><span data-stu-id="f2784-195">Let's say that hello service had an `InstanceCount` of 5, and several nodes running those instances all failed at hello same time.</span></span> <span data-ttu-id="f2784-196">Service Fabric отвечает путем автоматического создания экземпляров для замены на других узлах.</span><span class="sxs-lookup"><span data-stu-id="f2784-196">Service Fabric responds by automatically creating replacement instances on other nodes.</span></span> <span data-ttu-id="f2784-197">Он по-прежнему Создание замены до службы hello обратно tooits желаемое число экземпляров.</span><span class="sxs-lookup"><span data-stu-id="f2784-197">It will continue creating replacements until hello service is back tooits desired instance count.</span></span> <span data-ttu-id="f2784-198">Например, предположим, что было службы без отслеживания состояния с `InstanceCount`-1, то есть оно выполняется на всех узлах кластера hello допустимым.</span><span class="sxs-lookup"><span data-stu-id="f2784-198">As another example, let's say there was a stateless service with an `InstanceCount`of -1, meaning it runs on all valid nodes in hello cluster.</span></span> <span data-ttu-id="f2784-199">Предположим, что некоторые из этих экземпляров были toofail.</span><span class="sxs-lookup"><span data-stu-id="f2784-199">Let's say that some of those instances were toofail.</span></span> <span data-ttu-id="f2784-200">В этом случае Service Fabric уведомления службы hello не находится в нужное состояние, которое пытается toocreate hello экземпляров на узлах hello, где они отсутствуют.</span><span class="sxs-lookup"><span data-stu-id="f2784-200">In this case, Service Fabric notices that hello service is not in its desired state, and tries toocreate hello instances on hello nodes where they are missing.</span></span> 

<span data-ttu-id="f2784-201">Для служб с отслеживанием состояния ситуации hello зависит от того, ли служба hello имеет постоянное состояние, или нет.</span><span class="sxs-lookup"><span data-stu-id="f2784-201">For stateful services hello situation depends on whether hello service has persisted state or not.</span></span> <span data-ttu-id="f2784-202">Он также зависит от сколько реплик hello имела и число неудачных.</span><span class="sxs-lookup"><span data-stu-id="f2784-202">It also depends on how many replicas hello service had and how many failed.</span></span> <span data-ttu-id="f2784-203">Определение, случился ли сбой службы с отслеживанием состояния, и его обработка состоят из трех этапов:</span><span class="sxs-lookup"><span data-stu-id="f2784-203">Determining whether a disaster occurred for a stateful service and managing it follows three stages:</span></span>

1. <span data-ttu-id="f2784-204">Определение, произошла ли потеря кворума.</span><span class="sxs-lookup"><span data-stu-id="f2784-204">Determining if there has been quorum loss or not</span></span>
 - <span data-ttu-id="f2784-205">Потери кворума, осуществляется большую часть hello реплики службы с отслеживанием состояния не работают в hello одновременно, включая hello первичной.</span><span class="sxs-lookup"><span data-stu-id="f2784-205">A quorum loss is any time a majority of hello replicas of a stateful service are down at hello same time, including hello Primary.</span></span>
2. <span data-ttu-id="f2784-206">Определение, если потери кворума hello постоянной</span><span class="sxs-lookup"><span data-stu-id="f2784-206">Determining if hello quorum loss is permanent or not</span></span>
 - <span data-ttu-id="f2784-207">Большую часть времени hello сбои являются временными.</span><span class="sxs-lookup"><span data-stu-id="f2784-207">Most of hello time, failures are transient.</span></span> <span data-ttu-id="f2784-208">Перезапускаются процессы, узлы и виртуальные машины, а также выполняется автоматическое восстановление секций сети.</span><span class="sxs-lookup"><span data-stu-id="f2784-208">Processes are restarted, nodes are restarted, VMs are relaunched, network partitions heal.</span></span> <span data-ttu-id="f2784-209">Однако иногда сбои являются постоянными.</span><span class="sxs-lookup"><span data-stu-id="f2784-209">Sometimes though, failures are permanent.</span></span> 
    - <span data-ttu-id="f2784-210">Для служб без сохраненного состояния потеря кворума или нескольких реплик _немедленно_ приводит к постоянной потере кворума.</span><span class="sxs-lookup"><span data-stu-id="f2784-210">For services without persisted state, a failure of a quorum or more of replicas results _immediately_ in permanent quorum loss.</span></span> <span data-ttu-id="f2784-211">Когда Service Fabric обнаруживает потери кворума в непостоянный службы с отслеживанием состояния, он проходит немедленно dataloss toostep 3, объявив (вероятность).</span><span class="sxs-lookup"><span data-stu-id="f2784-211">When Service Fabric detects quorum loss in a stateful non-persistent service, it immediately proceeds toostep 3 by declaring (potential) dataloss.</span></span> <span data-ttu-id="f2784-212">Продолжить toodataloss смысл, что Service Fabric знает, что нет смысла в ожидание hello реплик toocome назад, поскольку даже если они были восстановлены, они будут пустыми.</span><span class="sxs-lookup"><span data-stu-id="f2784-212">Proceeding toodataloss makes sense because Service Fabric knows that there's no point in waiting for hello replicas toocome back, because even if they were recovered they would be empty.</span></span>
    - <span data-ttu-id="f2784-213">Для постоянных служб с отслеживанием состояния сбой кворума или нескольких реплик вызывает toostart Service Fabric ожидания обратно toocome реплик hello и восстановления кворума.</span><span class="sxs-lookup"><span data-stu-id="f2784-213">For stateful persistent services, a failure of a quorum or more of replicas causes Service Fabric toostart waiting for hello replicas toocome back and restore quorum.</span></span> <span data-ttu-id="f2784-214">Это приводит к простою службы для любого _записывает_ toohello затронутые секции (или «реплика»), службы hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-214">This results in a service outage for any _writes_ toohello affected partition (or "replica set") of hello service.</span></span> <span data-ttu-id="f2784-215">Тем не менее операции чтения по-прежнему могут выполняться с пониженным уровнем обеспечения согласованности.</span><span class="sxs-lookup"><span data-stu-id="f2784-215">However, reads may still be possible with reduced consistency guarantees.</span></span> <span data-ttu-id="f2784-216">количество времени, Service Fabric ожидает от кворума toobe восстановления по умолчанию Hello бесконечно, поскольку продолжением (возможного) события dataloss и выполняет другие риски.</span><span class="sxs-lookup"><span data-stu-id="f2784-216">hello default amount of time that Service Fabric waits for quorum toobe restored is infinite, since proceeding is a (potential) dataloss event and carries other risks.</span></span> <span data-ttu-id="f2784-217">Переопределение по умолчанию hello `QuorumLossWaitDuration` значение возможна, но не рекомендуется.</span><span class="sxs-lookup"><span data-stu-id="f2784-217">Overriding hello default `QuorumLossWaitDuration` value is possible but is not recommended.</span></span> <span data-ttu-id="f2784-218">Вместо этого в данный момент все усилия должны быть доступны hello toorestore вниз реплик.</span><span class="sxs-lookup"><span data-stu-id="f2784-218">Instead at this time, all efforts should be made toorestore hello down replicas.</span></span> <span data-ttu-id="f2784-219">Это требует переключать hello узлы, которые не работают резервное копирование и гарантирует, что можно заново подключить hello диски хранения локального состояния постоянных hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-219">This requires bringing hello nodes that are down back up, and ensuring that they can remount hello drives where they stored hello local persistent state.</span></span> <span data-ttu-id="f2784-220">Если потери кворума hello причиной является сбой процесса, Service Fabric автоматически пытается toorecreate hello процессов и перезапустите реплик hello внутри них.</span><span class="sxs-lookup"><span data-stu-id="f2784-220">If hello quorum loss is caused by process failure, Service Fabric automatically tries toorecreate hello processes and restart hello replicas inside them.</span></span> <span data-ttu-id="f2784-221">В случае сбоя Service Fabric сообщит об ошибках работоспособности.</span><span class="sxs-lookup"><span data-stu-id="f2784-221">If this fails, Service Fabric reports health errors.</span></span> <span data-ttu-id="f2784-222">Если это не разрешается hello реплики обычно возобновить.</span><span class="sxs-lookup"><span data-stu-id="f2784-222">If these can be resolved then hello replicas usually come back.</span></span> <span data-ttu-id="f2784-223">В некоторых случаях, hello реплик не удается подключить обратно.</span><span class="sxs-lookup"><span data-stu-id="f2784-223">Sometimes, though, hello replicas can't be brought back.</span></span> <span data-ttu-id="f2784-224">Например диски hello все возможно, произошел сбой, или машин hello физически разрушается каким-либо образом.</span><span class="sxs-lookup"><span data-stu-id="f2784-224">For example, hello drives may all have failed, or hello machines physically destroyed somehow.</span></span> <span data-ttu-id="f2784-225">В таких случаях потеря кворума будет постоянной.</span><span class="sxs-lookup"><span data-stu-id="f2784-225">In these cases, we now have a permanent quorum loss event.</span></span> <span data-ttu-id="f2784-226">toostop Service Fabric tootell ожидание hello вниз реплик, toocome администратор кластера необходимо определить, какие секции, в которых затрагиваются службы и вызывать hello `Repair-ServiceFabricPartition -PartitionId` или ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span><span class="sxs-lookup"><span data-stu-id="f2784-226">tootell Service Fabric toostop waiting for hello down replicas toocome back, a cluster administrator must determine which partitions of which services are affected and call hello `Repair-ServiceFabricPartition -PartitionId` or ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span></span>  <span data-ttu-id="f2784-227">Этот API позволяет указывать идентификатор hello hello секции toomove из QuorumLoss и отправляются потенциальных dataloss.</span><span class="sxs-lookup"><span data-stu-id="f2784-227">This API allows specifying hello ID of hello partition toomove out of QuorumLoss and into potential dataloss.</span></span>

> [!NOTE]
> <span data-ttu-id="f2784-228">Это _никогда не_ безопасном toouse этого API-интерфейса отличный от целевой для конкретных секций.</span><span class="sxs-lookup"><span data-stu-id="f2784-228">It is _never_ safe toouse this API other than in a targeted way against specific partitions.</span></span> 
>

3. <span data-ttu-id="f2784-229">Определение наличия фактической потери данных и восстановление из резервных копий</span><span class="sxs-lookup"><span data-stu-id="f2784-229">Determining if there has been actual data loss, and restoring from backups</span></span>
  - <span data-ttu-id="f2784-230">Когда Service Fabric вызывает hello `OnDataLossAsync` метод бывает из-за _подозревается_ dataloss.</span><span class="sxs-lookup"><span data-stu-id="f2784-230">When Service Fabric calls hello `OnDataLossAsync` method it is always because of _suspected_ dataloss.</span></span> <span data-ttu-id="f2784-231">Service Fabric гарантирует, что этот вызов доставляется toohello _наиболее_ оставшиеся реплики.</span><span class="sxs-lookup"><span data-stu-id="f2784-231">Service Fabric ensures that this call is delivered toohello _best_ remaining replica.</span></span> <span data-ttu-id="f2784-232">Это происходит, какая реплика завершает очередной этап hello большинство.</span><span class="sxs-lookup"><span data-stu-id="f2784-232">This is whichever replica has made hello most progress.</span></span> <span data-ttu-id="f2784-233">Здравствуйте причина мы говорим всегда _подозревается_ dataloss заключается в возможности этой реплики оставшиеся hello фактически имеет все же состояние hello основной было указано при его вышел из строя.</span><span class="sxs-lookup"><span data-stu-id="f2784-233">hello reason we always say _suspected_ dataloss is that it is possible that hello remaining replica actually has all same state as hello Primary did when it went down.</span></span> <span data-ttu-id="f2784-234">Однако без этого состояния toocompare его, никак не хорошо для tooknow Service Fabric или операторы для уверенности.</span><span class="sxs-lookup"><span data-stu-id="f2784-234">However, without that state toocompare it to, there's no good way for Service Fabric or operators tooknow for sure.</span></span> <span data-ttu-id="f2784-235">На этом этапе Service Fabric узнает hello реплик не приходящие обратно.</span><span class="sxs-lookup"><span data-stu-id="f2784-235">At this point, Service Fabric also knows hello other replicas are not coming back.</span></span> <span data-ttu-id="f2784-236">Это было hello, принятое остановив ожидание tooresolve потери кворума hello сам.</span><span class="sxs-lookup"><span data-stu-id="f2784-236">That was hello decision made when we stopped waiting for hello quorum loss tooresolve itself.</span></span> <span data-ttu-id="f2784-237">Hello наилучшее решение для hello службы обычно является toofreeze и ожидания для определенных вмешательства администратора.</span><span class="sxs-lookup"><span data-stu-id="f2784-237">hello best course of action for hello service is usually toofreeze and wait for specific administrative intervention.</span></span> <span data-ttu-id="f2784-238">Таким образом, что делает типичной реализации hello `OnDataLossAsync` метод выполнить?</span><span class="sxs-lookup"><span data-stu-id="f2784-238">So what does a typical implementation of hello `OnDataLossAsync` method do?</span></span>
  - <span data-ttu-id="f2784-239">Во-первых, запись о запуске `OnDataLossAsync` и запуск необходимых административных оповещений.</span><span class="sxs-lookup"><span data-stu-id="f2784-239">First, log that `OnDataLossAsync` has been triggered, and fire off any necessary administrative alerts.</span></span>
   - <span data-ttu-id="f2784-240">Обычно на этом этапе toopause и ожидания для дальнейшего решения и toobe ручные действия выполнены.</span><span class="sxs-lookup"><span data-stu-id="f2784-240">Usually at this point, toopause and wait for further decisions and manual actions toobe taken.</span></span> <span data-ttu-id="f2784-241">Это, поскольку даже если резервные копии доступны, они должны toobe подготовлен.</span><span class="sxs-lookup"><span data-stu-id="f2784-241">This is because even if backups are available they may need toobe prepared.</span></span> <span data-ttu-id="f2784-242">Например если две разные службы координации сведения, эти резервные копии может потребоваться toobe изменения в tooensure порядке, когда происходит восстановление hello, hello сведения эти две службы важна согласованной.</span><span class="sxs-lookup"><span data-stu-id="f2784-242">For example, if two different services coordinate information, those backups may need toobe modified in order tooensure that once hello restore happens that hello information those two services care about is consistent.</span></span> 
  - <span data-ttu-id="f2784-243">Часто также есть некоторые другие телеметрии или стороне из службы hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-243">Often there is also some other telemetry or exhaust from hello service.</span></span> <span data-ttu-id="f2784-244">Эти метаданные могут содержаться в других службах или в журналах.</span><span class="sxs-lookup"><span data-stu-id="f2784-244">This metadata may be contained in other services or in logs.</span></span> <span data-ttu-id="f2784-245">Эти сведения можно использовать необходимые toodetermine будто все вызовы получаются и обрабатываются в основной hello, которые отсутствовали в некоторой репликой резервного копирования или реплицированной toothis hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-245">This information can be used needed toodetermine if there were any calls received and processed at hello primary that were not present in hello backup or replicated toothis particular replica.</span></span> <span data-ttu-id="f2784-246">Эти может понадобиться резервного копирования toobe toohello воспроизводимой либо добавлен прежде, чем восстановление невозможно.</span><span class="sxs-lookup"><span data-stu-id="f2784-246">These may need toobe replayed or added toohello backup before restoration is feasible.</span></span>  
   - <span data-ttu-id="f2784-247">Сравнения оставшихся toothat состояние реплики, содержащихся в резервные копии, доступные hello.</span><span class="sxs-lookup"><span data-stu-id="f2784-247">Comparisons of hello remaining replica's state toothat contained in any backups that are available.</span></span> <span data-ttu-id="f2784-248">Если с помощью надежного коллекций Service Fabric hello, то есть средства и процедуры для таким образом, описанной в [в этой статье](service-fabric-reliable-services-backup-restore.md).</span><span class="sxs-lookup"><span data-stu-id="f2784-248">If using hello Service Fabric reliable collections then there are tools and processes available for doing so, described in [this article](service-fabric-reliable-services-backup-restore.md).</span></span> <span data-ttu-id="f2784-249">Цель Hello, toosee, если достаточно hello штата в рамках hello реплики, либо также могут отсутствовать какие hello резервного копирования.</span><span class="sxs-lookup"><span data-stu-id="f2784-249">hello goal is toosee if hello state within hello replica is sufficient, or also what hello backup may be missing.</span></span>
  - <span data-ttu-id="f2784-250">После этого сравнения hello и при завершении восстановления необходимых hello, код службы hello должен возвращать значение true, если были внесены изменения состояния.</span><span class="sxs-lookup"><span data-stu-id="f2784-250">Once hello comparison is done, and if necessary hello restore completed, hello service code should return true if any state changes were made.</span></span> <span data-ttu-id="f2784-251">Если реплика hello определил его hello лучше всего доступных копия hello состояния и не изменяются, возвращается значение false.</span><span class="sxs-lookup"><span data-stu-id="f2784-251">If hello replica determined that it was hello best available copy of hello state and made no changes, then return false.</span></span> <span data-ttu-id="f2784-252">Значение true указывает, что все _остальные_ оставшиеся реплики теперь не согласованы с этой репликой.</span><span class="sxs-lookup"><span data-stu-id="f2784-252">True indicates that any _other_ remaining replicas may now be inconsistent with this one.</span></span> <span data-ttu-id="f2784-253">Они будут удалены и пересозданы из этой реплики.</span><span class="sxs-lookup"><span data-stu-id="f2784-253">They will be dropped and rebuilt from this replica.</span></span> <span data-ttu-id="f2784-254">Значение false указывает, что состояние было внесено никаких изменений, поэтому hello реплик, можно сохранить их.</span><span class="sxs-lookup"><span data-stu-id="f2784-254">False indicates that no state changes were made, so hello other replicas can keep what they have.</span></span> 

<span data-ttu-id="f2784-255">Очень важно, чтобы разработчики службы смоделировали потенциальную потерю данных и сценарии сбоев, прежде чем развертывать службы в рабочей среде.</span><span class="sxs-lookup"><span data-stu-id="f2784-255">It is critically important that service authors practice potential dataloss and failure scenarios before services are ever deployed in production.</span></span> <span data-ttu-id="f2784-256">tooprotect от возможности hello объекта dataloss важно tooperiodically [резервное копирование состояния hello](service-fabric-reliable-services-backup-restore.md) любого географически избыточное хранилище tooa служб с отслеживанием состояния.</span><span class="sxs-lookup"><span data-stu-id="f2784-256">tooprotect against hello possibility of dataloss, it is important tooperiodically [back up hello state](service-fabric-reliable-services-backup-restore.md) of any of your stateful services tooa geo-redundant store.</span></span> <span data-ttu-id="f2784-257">Также необходимо убедиться, что имеется возможность toorestore hello его.</span><span class="sxs-lookup"><span data-stu-id="f2784-257">You must also ensure that you have hello ability toorestore it.</span></span> <span data-ttu-id="f2784-258">Поскольку резервные копии различных служб создаются в разное время, необходимо tooensure, что после восстановления службы имеют согласованное представление друг с другом.</span><span class="sxs-lookup"><span data-stu-id="f2784-258">Since backups of many different services are taken at different times, you need tooensure that after a restore your services have a consistent view of each other.</span></span> <span data-ttu-id="f2784-259">Например рассмотрим ситуацию, где одна служба создает число и сохраняет его и отправляет ее tooanother службу, которая также сохраняет его.</span><span class="sxs-lookup"><span data-stu-id="f2784-259">For example, consider a situation where one service generates a number and stores it, then sends it tooanother service that also stores it.</span></span> <span data-ttu-id="f2784-260">После восстановления можно обнаружить вторую службу hello имеет номер hello, но hello сначала не так, поскольку его резервная копия не включать эту операцию.</span><span class="sxs-lookup"><span data-stu-id="f2784-260">After a restore, you might discover that hello second service has hello number but hello first does not, because it's backup didn't include that operation.</span></span>

<span data-ttu-id="f2784-261">Если можно узнать, что осталось hello реплики находятся недостаточно toocontinue из в сценарии dataloss и не может восстановить состояние службы из телеметрии или выпуска, hello частоту архивации определяет вашей наиболее точки возможности восстановления (RPO) .</span><span class="sxs-lookup"><span data-stu-id="f2784-261">If you find out that hello remaining replicas are insufficient toocontinue from in a dataloss scenario, and you can't reconstruct service state from telemetry or exhaust, hello frequency of your backups determines your best possible recovery point objective (RPO).</span></span> <span data-ttu-id="f2784-262">Service Fabric предоставляет множество средств для тестирования различных сценариев сбоя, включая постоянный кворум и потерю данных, для которой требуется восстановление из резервной копии.</span><span class="sxs-lookup"><span data-stu-id="f2784-262">Service Fabric provides many tools for testing various failure scenarios, including permanent quorum and dataloss requiring restoration from a backup.</span></span> <span data-ttu-id="f2784-263">Эти сценарии включены как часть средств тестирования Service Fabric, управляется hello ошибки Analysis Service.</span><span class="sxs-lookup"><span data-stu-id="f2784-263">These scenarios are included as a part of Service Fabric's testability tools, managed by hello Fault Analysis Service.</span></span> <span data-ttu-id="f2784-264">Дополнительные сведения об этих средствах и шаблонах см. [здесь](service-fabric-testability-overview.md).</span><span class="sxs-lookup"><span data-stu-id="f2784-264">More info on those tools and patterns is available [here](service-fabric-testability-overview.md).</span></span> 

> [!NOTE]
> <span data-ttu-id="f2784-265">Системные службы также возможна потеря кворума, с hello влияние конкретных toohello службы в вопросе.</span><span class="sxs-lookup"><span data-stu-id="f2784-265">System services can also suffer quorum loss, with hello impact being specific toohello service in question.</span></span> <span data-ttu-id="f2784-266">Для экземпляра потери кворума в службе имен hello влияет на разрешение имен, тогда как потери кворума в службе диспетчера отработки отказа hello блокирует создание новой службы и переход на другой ресурс.</span><span class="sxs-lookup"><span data-stu-id="f2784-266">For instance, quorum loss in hello naming service impacts name resolution, whereas quorum loss in hello failover manager service blocks new service creation and failovers.</span></span> <span data-ttu-id="f2784-267">Хотя службы system Service Fabric hello следуют hello же шаблон как служб для управления состоянием, не рекомендуется попытаться toomove их из потери кворума и в потенциальных dataloss.</span><span class="sxs-lookup"><span data-stu-id="f2784-267">While hello Service Fabric system services follow hello same pattern as your services for state management, it is not recommended that you should attempt toomove them out of Quorum Loss and into potential dataloss.</span></span> <span data-ttu-id="f2784-268">Hello рекомендуется вместо этого слишком[поиска поддержки](service-fabric-support.md) toodetermine решение, которое является целевым tooyour конкретной ситуации.</span><span class="sxs-lookup"><span data-stu-id="f2784-268">hello recommendation is instead too[seek support](service-fabric-support.md) toodetermine a solution that is targeted tooyour specific situation.</span></span>  <span data-ttu-id="f2784-269">Обычно бывает предпочтительнее toosimply ожидания до hello вниз возвращаемого реплик.</span><span class="sxs-lookup"><span data-stu-id="f2784-269">Usually it is preferable toosimply wait until hello down replicas return.</span></span>
>

## <a name="availability-of-hello-service-fabric-cluster"></a><span data-ttu-id="f2784-270">Доступность кластера Service Fabric hello</span><span class="sxs-lookup"><span data-stu-id="f2784-270">Availability of hello Service Fabric cluster</span></span>
<span data-ttu-id="f2784-271">Вообще говоря самого кластера Service Fabric hello является распределенной среде с единых точек отказа.</span><span class="sxs-lookup"><span data-stu-id="f2784-271">Generally speaking, hello Service Fabric cluster itself is a highly distributed environment with no single points of failure.</span></span> <span data-ttu-id="f2784-272">Сбой одного из узлов не приведет доступности или проблемы с надежностью hello кластер, в основном из-за системных служб Service Fabric hello выполните hello же правилами ранее указанного: они всегда выполняются с менее трех реплик по по умолчанию, а также системные службы, которые не имеют состояния работу на всех узлах.</span><span class="sxs-lookup"><span data-stu-id="f2784-272">A failure of any one node will not cause availability or reliability issues for hello cluster, primarily because hello Service Fabric system services follow hello same guidelines provided earlier: they always run with three or more replicas by default, and those system services that are stateless run on all nodes.</span></span> <span data-ttu-id="f2784-273">полностью распределения базовой сети Service Fabric Hello и слои обнаружения сбоя.</span><span class="sxs-lookup"><span data-stu-id="f2784-273">hello underlying Service Fabric networking and failure detection layers are fully distributed.</span></span> <span data-ttu-id="f2784-274">Большинство системных служб могут быть перестроены из метаданных в кластере hello или узнать, как tooresynchronize свое состояние из других местах.</span><span class="sxs-lookup"><span data-stu-id="f2784-274">Most system services can be rebuilt from metadata in hello cluster, or know how tooresynchronize their state from other places.</span></span> <span data-ttu-id="f2784-275">доступность Hello hello кластера может стать ненадежным, если системные службы завладеть кворума потери такой ситуации описанных выше.</span><span class="sxs-lookup"><span data-stu-id="f2784-275">hello availability of hello cluster can become compromised if system services get into quorum loss situations like those described above.</span></span> <span data-ttu-id="f2784-276">В таких случаях может оказаться определенные операции в кластере hello, такие как запуск обновления или развертывание новых служб, но самого кластера hello работает по-прежнему может tooperform.</span><span class="sxs-lookup"><span data-stu-id="f2784-276">In these cases you may not be able tooperform certain operations on hello cluster like starting an upgrade or deploying new services, but hello cluster itself is still up.</span></span> <span data-ttu-id="f2784-277">Службы уже после запуска сможет продолжить работу в таких условиях, если им требуется система служб записи toohello toocontinue функционирует.</span><span class="sxs-lookup"><span data-stu-id="f2784-277">Services on already running will remain running in these conditions unless they require writes toohello system services toocontinue functioning.</span></span> <span data-ttu-id="f2784-278">Например при потере кворума hello Диспетчер отказоустойчивости все службы будут toorun, но все службы, которые не удалось не будет возможности tooautomatically перезагрузки, поскольку для этого требуется участие hello hello Диспетчер отказоустойчивости.</span><span class="sxs-lookup"><span data-stu-id="f2784-278">For example, if hello Failover Manager is in quorum loss all services will continue toorun, but any services that fail will not be able tooautomatically restart, since this requires hello involvement of hello Failover Manager.</span></span> 

### <a name="failures-of-a-datacenter-or-azure-region"></a><span data-ttu-id="f2784-279">Сбои в центре обработки данных или регионе Azure</span><span class="sxs-lookup"><span data-stu-id="f2784-279">Failures of a datacenter or Azure region</span></span>
<span data-ttu-id="f2784-280">В редких случаях физическом центре обработки данных может стать недоступным из-за tooloss из управления питанием или сетевое подключение.</span><span class="sxs-lookup"><span data-stu-id="f2784-280">In rare cases, a physical data center can become temporarily unavailable due tooloss of power or network connectivity.</span></span> <span data-ttu-id="f2784-281">В этих случаях кластеры Service Fabric, а также службы в таких центрах обработки данных или в регионе Azure будут недоступны.</span><span class="sxs-lookup"><span data-stu-id="f2784-281">In these cases, your Service Fabric clusters and services in that datacenter or Azure region will be unavailable.</span></span> <span data-ttu-id="f2784-282">Тем не менее _данные сохраняются_.</span><span class="sxs-lookup"><span data-stu-id="f2784-282">However, _your data is preserved_.</span></span> <span data-ttu-id="f2784-283">Для кластеров, работающих в Azure, вы можете просмотреть обновления на сбоев на hello [страница Azure состояния][azure-status-dashboard].</span><span class="sxs-lookup"><span data-stu-id="f2784-283">For clusters running in Azure, you can view updates on outages on hello [Azure status page][azure-status-dashboard].</span></span> <span data-ttu-id="f2784-284">В hello очень маловероятно, что физическом центре обработки данных является частично или полностью удаляется, любых кластеров Service Fabric размещенных существует или hello служб внутри них могут быть потеряны.</span><span class="sxs-lookup"><span data-stu-id="f2784-284">In hello highly unlikely event that a physical data center is partially or fully destroyed, any Service Fabric clusters hosted there or hello services inside them could be lost.</span></span> <span data-ttu-id="f2784-285">Сюда входят все состояния, для которых не были созданы резервные копии за пределами этого центра обработки данных или региона.</span><span class="sxs-lookup"><span data-stu-id="f2784-285">This includes any state not backed up outside of that datacenter or region.</span></span>

<span data-ttu-id="f2784-286">Есть два различных подхода к оставшимся hello постоянными или неустранимого сбоя в одном центре обработки данных или.</span><span class="sxs-lookup"><span data-stu-id="f2784-286">There's two different strategies for surviving hello permanent or sustained failure of a single datacenter or region.</span></span> 

1. <span data-ttu-id="f2784-287">Запустите отдельные кластеры Service Fabric в нескольких регионах и используйте механизм для отработки отказа и восстановления размещения между этими средами.</span><span class="sxs-lookup"><span data-stu-id="f2784-287">Run separate Service Fabric clusters in multiple such regions, and utilize some mechanism for failover and fail-back between these environments.</span></span> <span data-ttu-id="f2784-288">Для такой мультикластерной модели типа "активный — активный" или "активный — пассивный" требуется дополнительный код для управления и операций.</span><span class="sxs-lookup"><span data-stu-id="f2784-288">This sort of multi-cluster active-active or active-passive model requires additional management and operations code.</span></span> <span data-ttu-id="f2784-289">Также требуется координации резервных копий из служб hello в одном центре обработки данных или области, чтобы они были доступны в других центрах обработки данных или регионах, когда один сбоя.</span><span class="sxs-lookup"><span data-stu-id="f2784-289">This also requires coordination of backups from hello services in one datacenter or region so that they are available in other datacenters or regions when one fails.</span></span> 
2. <span data-ttu-id="f2784-290">Запустите кластер Service Fabric, охватывающий несколько центров обработки данных или регионов.</span><span class="sxs-lookup"><span data-stu-id="f2784-290">Run a single Service Fabric cluster that spans multiple datacenters or regions.</span></span> <span data-ttu-id="f2784-291">Hello минимальные системные требования для данного — три центрах обработки данных или области.</span><span class="sxs-lookup"><span data-stu-id="f2784-291">hello minimum supported configuration for this is three datacenters or regions.</span></span> <span data-ttu-id="f2784-292">Hello рекомендуемое число областей или центрах обработки данных, равно 5.</span><span class="sxs-lookup"><span data-stu-id="f2784-292">hello recommended number of regions or datacenters is five.</span></span> <span data-ttu-id="f2784-293">Это предполагает более сложную топологию кластера.</span><span class="sxs-lookup"><span data-stu-id="f2784-293">This requires a more complex cluster topology.</span></span> <span data-ttu-id="f2784-294">Однако hello преимущество этой модели является, сбой одного центра обработки данных будет преобразован в обычном сбоя после аварии.</span><span class="sxs-lookup"><span data-stu-id="f2784-294">However, hello benefit of this model is that failure of one datacenter or region is converted from a disaster into a normal failure.</span></span> <span data-ttu-id="f2784-295">Эти ошибки могут быть обработаны механизмы hello работающие для кластеров в одном регионе.</span><span class="sxs-lookup"><span data-stu-id="f2784-295">These failures can be handled by hello mechanisms that work for clusters within a single region.</span></span> <span data-ttu-id="f2784-296">Домены сбоя, домены обновления и правила размещения Service Fabric гарантируют, что нагрузки распределяются таким образом, чтобы выдерживать обычные сбои.</span><span class="sxs-lookup"><span data-stu-id="f2784-296">Fault domains, upgrade domains, and Service Fabric's placement rules ensure workloads are distributed so that they tolerate normal failures.</span></span> <span data-ttu-id="f2784-297">Дополнительные сведения о политиках, которые помогают работать службам в таком типе кластеров, см. в статье [Политики размещения для служб Service Fabric](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span><span class="sxs-lookup"><span data-stu-id="f2784-297">For more information on policies that can help operate services in this type of cluster, read up on [placement policies](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span></span>

### <a name="random-failures-leading-toocluster-failures"></a><span data-ttu-id="f2784-298">Случайные сбои начальные toocluster сбоев</span><span class="sxs-lookup"><span data-stu-id="f2784-298">Random failures leading toocluster failures</span></span>
<span data-ttu-id="f2784-299">Service Fabric имеет концепции hello узлов начальное значение.</span><span class="sxs-lookup"><span data-stu-id="f2784-299">Service Fabric has hello concept of Seed Nodes.</span></span> <span data-ttu-id="f2784-300">Это, чтобы обеспечить доступность hello hello базовый кластер узлов.</span><span class="sxs-lookup"><span data-stu-id="f2784-300">These are nodes that maintain hello availability of hello underlying cluster.</span></span> <span data-ttu-id="f2784-301">Эти узлы помогают tooensure hello кластер остается копию, установив аренды с другими узлами и выступает в роли таков во время определенных видов сбоев сети.</span><span class="sxs-lookup"><span data-stu-id="f2784-301">These nodes help tooensure hello cluster remains up by establishing leases with other nodes and serving as tiebreakers during certain kinds of network failures.</span></span> <span data-ttu-id="f2784-302">Если они не будут возвращены случайные сбои удаления большинства hello начальное число узлов в кластере hello, hello кластера автоматически завершает работу.</span><span class="sxs-lookup"><span data-stu-id="f2784-302">If random failures remove a majority of hello seed nodes in hello cluster and they are not brought back, hello cluster automatically shuts down.</span></span> <span data-ttu-id="f2784-303">В Azure, автоматически управляет узлы начальное значение: они распределяются по hello доступных сбоя и доменах обновления, и если один начальный узел удаляется из кластера hello вместо него будет создан еще один.</span><span class="sxs-lookup"><span data-stu-id="f2784-303">In Azure, Seed Nodes are automatically managed: they are distributed over hello available fault and upgrade domains, and if a single seed node is removed from hello cluster another one will be created in its place.</span></span> 

<span data-ttu-id="f2784-304">В автономных кластеров Service Fabric и Azure hello «Основной тип узла» является hello, обрабатывающий hello начальные значения.</span><span class="sxs-lookup"><span data-stu-id="f2784-304">In both standalone Service Fabric clusters and Azure, hello "Primary Node Type" is hello one that runs hello seeds.</span></span> <span data-ttu-id="f2784-305">При определении тип основного узла, Service Fabric автоматически будет воспользоваться преимуществами hello количество узлов, предоставляемый создании узлов too9 начального значения и 9 репликах каждого hello системных служб.</span><span class="sxs-lookup"><span data-stu-id="f2784-305">When defining a primary node type, Service Fabric will automatically take advantage of hello number of nodes provided by creating up too9 seed nodes and 9 replicas of each of hello system services.</span></span> <span data-ttu-id="f2784-306">Если большинство этих реплик службы системы извлекает набор случайные сбои одновременно, hello системных служб вводит потери кворума, как было описано выше.</span><span class="sxs-lookup"><span data-stu-id="f2784-306">If a set of random failures takes out a majority of those system service replicas simultaneously, hello system services will enter quorum loss, as we described above.</span></span> <span data-ttu-id="f2784-307">Если потеряны большинства узлов hello начальное значение, hello кластера завершит работу после.</span><span class="sxs-lookup"><span data-stu-id="f2784-307">If a majority of hello seed nodes are lost, hello cluster will shut down soon after.</span></span>

## <a name="next-steps"></a><span data-ttu-id="f2784-308">Дальнейшие действия</span><span class="sxs-lookup"><span data-stu-id="f2784-308">Next steps</span></span>
- <span data-ttu-id="f2784-309">Узнайте, как toosimulate различных сбоях, с помощью hello [framework тестирования](service-fabric-testability-overview.md)</span><span class="sxs-lookup"><span data-stu-id="f2784-309">Learn how toosimulate various failures using hello [testability framework](service-fabric-testability-overview.md)</span></span>
- <span data-ttu-id="f2784-310">Ознакомьтесь с другими материалами по аварийному восстановлению и обеспечению высокой доступности.</span><span class="sxs-lookup"><span data-stu-id="f2784-310">Read other disaster-recovery and high-availability resources.</span></span> <span data-ttu-id="f2784-311">Корпорация Майкрософт опубликовала множество руководств по этим темам.</span><span class="sxs-lookup"><span data-stu-id="f2784-311">Microsoft has published a large amount of guidance on these topics.</span></span> <span data-ttu-id="f2784-312">Хотя некоторые из этих документов см. методы toospecific для использования в других продуктах, они содержат общие рекомендации по, которые можно применять в контексте Service Fabric hello также:</span><span class="sxs-lookup"><span data-stu-id="f2784-312">While some of these documents refer toospecific techniques for use in other products, they contain many general best practices you can apply in hello Service Fabric context as well:</span></span>
  - [<span data-ttu-id="f2784-313">Контрольный список для обеспечения доступности</span><span class="sxs-lookup"><span data-stu-id="f2784-313">Availability checklist</span></span>](../best-practices-availability-checklist.md)
  - [<span data-ttu-id="f2784-314">Отработка аварийного восстановления</span><span class="sxs-lookup"><span data-stu-id="f2784-314">Performing a disaster recovery drill</span></span>](../sql-database/sql-database-disaster-recovery-drills.md)
  - <span data-ttu-id="f2784-315">[Аварийное восстановление и высокий уровень доступности для приложений Azure][dr-ha-guide]</span><span class="sxs-lookup"><span data-stu-id="f2784-315">[Disaster recovery and high availability for Azure applications][dr-ha-guide]</span></span>
- <span data-ttu-id="f2784-316">Узнайте о [вариантах поддержки Service Fabric](service-fabric-support.md).</span><span class="sxs-lookup"><span data-stu-id="f2784-316">Learn about [Service Fabric support options](service-fabric-support.md)</span></span>

<!-- External links -->

[repair-partition-ps]: https://msdn.microsoft.com/library/mt163522.aspx
[azure-status-dashboard]:https://azure.microsoft.com/status/
[azure-regions]: https://azure.microsoft.com/regions/
[dr-ha-guide]: https://msdn.microsoft.com/library/azure/dn251004.aspx


<!-- Images -->

[sfx-cluster-map]: ./media/service-fabric-disaster-recovery/sfx-clustermap.png
