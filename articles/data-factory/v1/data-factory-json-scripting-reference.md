---
title: "Справочник по написанию скриптов JSON фабрики данных Azure | Документация Майкрософт"
description: "Содержит схемы JSON для сущностей фабрики данных."
services: data-factory
documentationcenter: 
author: spelluru
manager: jhubbard
editor: 
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 01/10/2018
ms.author: spelluru
robots: noindex
ms.openlocfilehash: 9457e90f378cf7b30810ca9cadfcad139e91e2d4
ms.sourcegitcommit: 9cc3d9b9c36e4c973dd9c9028361af1ec5d29910
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/23/2018
---
# <a name="azure-data-factory---json-scripting-reference"></a>Справочник по написанию скриптов JSON фабрики данных Azure
> [!NOTE]
> Статья относится к версии 1 фабрики данных, которая является общедоступной версией.


Эта статья содержит схемы JSON и примеры для определения сущностей фабрики данных Azure (конвейер, действие, набор данных и связанная служба).  

## <a name="pipeline"></a>Конвейер 
Высокоуровневая структура определения конвейера выглядит следующим образом: 

```json
{
  "name": "SamplePipeline",
  "properties": {
    "description": "Describe what pipeline does",
    "activities": [
    ],
    "start": "2016-07-12T00:00:00",
    "end": "2016-07-13T00:00:00"
  }
} 
```

В приведенной ниже таблице описаны свойства, используемые в определениях JSON конвейера.

| Свойство | ОПИСАНИЕ | Обязательно
-------- | ----------- | --------
| name | Имя конвейера. Укажите имя, представляющее операцию, для выполнения которой настроено действие или конвейер.<br/><ul><li>Максимальное количество знаков: 260.</li><li>Должно начинаться с буквы, цифры или символа подчеркивания (_).</li><li>Следующие знаки не допускаются: ".", "+", "?", "/", "<", ">", "*", "%", "&", ":", "\\".</li></ul> |Yes |
| description |Текст, описывающий, для чего используется действие или конвейер | Нет  |
| Действия | Содержит список действий. | Yes |
| start |Дата и время начала работы конвейера. Задается в [формате ISO](http://en.wikipedia.org/wiki/ISO_8601). Например: 2014-10-14T16:32:41. <br/><br/>Можно указать местное время, например восточное поясное время (EST). Пример: `2016-02-27T06:00:00**-05:00`. Это 6:00 по восточному стандартному времени.<br/><br/>Свойства start и end определяют активный период работы конвейера. Срезы выходных данных создаются только в этот активный период. |Нет <br/><br/>Если вы указываете значение свойства end, вы также должны указать значение свойства start.<br/><br/>Для создания конвейера значения времени начала и времени окончания могут быть пустыми. Если требуется задать активный период работы конвейера, следует указать оба значения. Если вы не указали время начала и окончания при создании конвейера, их можно установить позже с помощью командлета Set-AzureRmDataFactoryPipelineActivePeriod. |
| end |Дата и время завершения работы конвейера. Не является обязательным и задается в формате ISO. Например: 2014-10-14T17:32:41 <br/><br/>Можно указать местное время, например восточное поясное время (EST). Пример: `2016-02-27T06:00:00**-05:00`. Это 6:00 по восточному стандартному времени.<br/><br/>Чтобы работа конвейера не была ограничена во времени, укажите для свойства end значение 9999-09-09. |Нет  <br/><br/>Если вы указываете значение свойства end, вы также должны указать значение свойства start.<br/><br/>Ознакомьтесь с примечаниями к свойству **start**. |
| isPaused |Если задано значение true, конвейер не запускается. Значение по умолчанию — false. Это свойство можно использовать для включения или отключения. |Нет  |
| pipelineMode |Определяет метод планирования работы конвейера. Допустимые значения: scheduled (по умолчанию), onetime.<br/><br/>Значение scheduled означает, что конвейер будет запускаться с указанной периодичностью в соответствии с его активным периодом (временем начала и окончания). Значение onetime означает, что конвейер будет запускаться только один раз. В настоящее время изменить или обновить однократные конвейеры после их создания нельзя. Подробные сведения об однократном запуске см. в разделе [Однократный конвейер](data-factory-create-pipelines.md#onetime-pipeline). |Нет  |
| expirationTime; |Период времени после создания, в течение которого конвейер является допустимым и должен оставаться подготовленным. Если на момент завершения этого периода у конвейера не будет активных, невыполненных или ожидающих выполнения запусков, конвейер будет автоматически удален. |Нет  |


## <a name="activity"></a>Действие 
Высокоуровневая структура действия в рамках определения конвейера (элемент действий) выглядит следующим образом:

```json
{
    "name": "ActivityName",
    "description": "description", 
    "type": "<ActivityType>",
    "inputs":  "[]",
    "outputs":  "[]",
    "linkedServiceName": "MyLinkedService",
    "typeProperties":
    {

    },
    "policy":
    {
    }
    "scheduler":
    {
    }
}
```

В приведенной ниже таблице описаны свойства, используемые в определениях JSON действия.

| Тег | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| name |Имя действия. Укажите имя, представляющее операцию, для выполнения которой настроено действие.<br/><ul><li>Максимальное количество знаков: 260.</li><li>Должно начинаться с буквы, цифры или символа подчеркивания (_).</li><li>Следующие знаки не допускаются: ".", "+", "?", "/", "<", ">", "*", "%", "&", ":", "\\".</li></ul> |Yes |
| description |Текст, описывающий для чего используется действие |Нет  |
| Тип |Задает тип действия. Разные типы действий описаны в разделах [ХРАНИЛИЩА ДАННЫХ](#data-stores) и [Действия преобразования данных](#data-transformation-activities). |Yes |
| inputs |Входные таблицы, используемые действием:<br/><br/>`// one input table`<br/>`"inputs":  [ { "name": "inputtable1"  } ],`<br/><br/>`// two input tables` <br/>`"inputs":  [ { "name": "inputtable1"  }, { "name": "inputtable2"  } ],` |"Нет" для действий HDInsightStreaming и SqlServerStoredProcedure <br/> <br/> "Да" — для всех остальных |
| outputs |Выходные таблицы, используемые действием.<br/><br/>`// one output table`<br/>`"outputs":  [ { "name": “outputtable1” } ],`<br/><br/>`//two output tables`<br/>`"outputs":  [ { "name": “outputtable1” }, { "name": “outputtable2” }  ],` |Yes |
| linkedServiceName |Имя связанной службы, используемой действием. <br/><br/>Для действия может потребоваться указать службу, связанную с обязательной вычислительной средой. |Да, для действий HDInsight, Машинного обучения Azure и действия хранимой процедуры. <br/><br/>Нет — для всех остальных |
| typeProperties |Свойства в разделе typeProperties зависят от типа действия. |Нет  |
| policy |Политики, которые влияют на поведение во время выполнения действия. Если для этого свойства не задано значение, используются стандартные политики. |Нет  |
| scheduler |Свойство scheduler позволяет задать расписание выполнения действия. Для него предусмотрен такой же набор подсвойств, что и для [свойства availability в наборе данных](data-factory-create-datasets.md#dataset-availability). |Нет  |

### <a name="policies"></a>Политики
Политики влияют на поведение во время выполнения действия, особенно при обработке среза таблицы. В следующей таблице приведено несколько примеров.

| Свойство | Допустимые значения | По умолчанию | ОПИСАНИЕ |
| --- | --- | --- | --- |
| concurrency |Целое число  <br/><br/>Максимальное значение — 10 |1 |Число одновременных выполнений действия.<br/><br/>Определяет количество параллельных выполнений одного действия для обработки разных срезов. Например, высокое значение этого свойства ускорит обработку большого набора доступных данных. |
| executionPriorityOrder |NewestFirst<br/><br/>OldestFirst |OldestFirst |Определяет порядок обработки срезов данных.<br/><br/>Предположим, есть два ожидающих обработки среза (от 16:00 и от 17:00). Если для свойства executionPriorityOrder задано значение NewestFirst, срез от 17:00 будет обработан первым. Точно так же, если для executionPriorityORder задано значение OldestFIrst, первым будет обработан срез от 16:00. |
| retry |Целое число <br/><br/>Максимальное значение — 10 |0 |Число повторных попыток обработки данных до того, как срез перейдет в состояние Failure (сбой). Выполнение действия со срезом данных повторяется указанное количество раз. Повторная попытка выполняется сразу после неудачной. |
| timeout |Интервал времени |00:00:00 |Время ожидания для действия. Пример: 00:10:00 (время ожидания — 10 минут).<br/><br/>Если значение не указано или равно 0, то время ожидания не ограничено.<br/><br/>Если время обработки среза превышает время ожидания, система отменяет текущую обработку и начинает новую. Количество повторов зависит от значения свойства retry. Когда время ожидания истекает, состояние среза меняется на TimedOut. |
| delay |Интервал времени |00:00:00 |Задайте задержку перед обработкой данных после начала выполнения среза.<br/><br/>Действие для среза данных запускается в ожидаемое время выполнения с указанной задержкой.<br/><br/>Пример: 00:10:00 (означает задержку в 10 минут). |
| longRetry |Целое число <br/><br/>Максимальное значение — 10 |1 |Количество длительных повторных попыток перед завершением сбоем выполнения среза.<br/><br/>Интервал между этими попытками задается свойством longRetryInterval. Используйте свойство longRetry, если повторные попытки необходимо выполнять с паузами. Если указаны свойства Retry и longRetry, то каждая попытка longRetry включает в себя попытки Retry, и максимальное число попыток равно Retry * longRetry.<br/><br/>Например, в политике действия указаны следующие параметры:<br/>Retry: 3<br/>longRetry: 2<br/>longRetryInterval: 01:00:00<br/><br/>Предположим, что существует только один выполняемый срез (в состоянии Waiting), и каждый раз при выполнении действия происходит сбой. Первые три попытки будут выполнены подряд. После каждой повторной попытки срез будет находиться в состоянии Retry. После выполнения первых трех попыток состоянием среза станет LongRetry.<br/><br/>Через час (значение свойства longRetryInterval) будут выполнены еще три попытки подряд. После этого состояние среза изменится на Failed и дальнейшие попытки предприниматься не будут. Поэтому всего было предпринято 6 попыток.<br/><br/>Если какое-либо выполнение завершится успешно, то состоянием среза станет Ready и дальнейшие попытки выполняться не будут.<br/><br/>Свойство longRetry можно использовать в ситуациях, когда зависимые данные поступают в неопределенное время или вся среда, в которой происходит обработка данных, непредсказуема. В таких случаях последовательные повторные попытки могут оказаться бесполезными, а выполненные через некоторое время, напротив, могут привести к желаемому результату.<br/><br/>Предупреждение. Не задавайте высокие значения для свойств longRetry и longRetryInterval. Как правило, более высокие значения приводят к появлению других системных проблем. |
| longRetryInterval |Интервал времени |00:00:00 |Период времени между длительными повторными попытками. |

### <a name="typeproperties-section"></a>Раздел typeProperties
Разделы typeProperties для каждого действия отличаются. Действия преобразования имеют только свойства типа. В разделе [ДЕЙСТВИЯ ПРЕОБРАЗОВАНИЯ ДАННЫХ](#data-transformation-activities) этой статьи представлены примеры JSON, определяющие действия преобразования в конвейере. 

**Действие копирования** имеет два подраздела в разделе typeProperties: **source** и **sink**. В разделе [ХРАНИЛИЩА ДАННЫХ](#data-stores) этой статьи представлены примеры JSON, в которых показано, как использовать хранилище данных как источник или приемник. 

### <a name="sample-copy-pipeline"></a>Пример конвейера копирования
В следующем примере конвейера содержится одно действие типа **Copy** in the **действий** . В этом примере [действие копирования](data-factory-data-movement-activities.md) копирует данные из хранилища BLOB-объектов Azure в базу данных SQL Azure. 

```json
{
  "name": "CopyPipeline",
  "properties": {
    "description": "Copy data from a blob to Azure SQL table",
    "activities": [
      {
        "name": "CopyFromBlobToSQL",
        "type": "Copy",
        "inputs": [
          {
            "name": "InputDataset"
          }
        ],
        "outputs": [
          {
            "name": "OutputDataset"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "BlobSource"
          },
          "sink": {
            "type": "SqlSink",
            "writeBatchSize": 10000,
            "writeBatchTimeout": "60:00:00"
          }
        },
        "Policy": {
          "concurrency": 1,
          "executionPriorityOrder": "NewestFirst",
          "retry": 0,
          "timeout": "01:00:00"
        }
      }
    ],
    "start": "2016-07-12T00:00:00",
    "end": "2016-07-13T00:00:00"
  }
} 
```

Обратите внимание на следующие моменты.

* В разделе действий доступно только одно действие, параметр **type** которого имеет значение **Copy**.
* Для этого действия параметру input присвоено значение **InputDataset**, а параметру output — значение **OutputDataset**.
* В разделе **typeProperties** в качестве типа источника указано **BlobSource**, а в качестве типа приемника — **SqlSink**.

В разделе [ХРАНИЛИЩА ДАННЫХ](#data-stores) этой статьи представлены примеры JSON, в которых показано, как использовать хранилище данных как источник или приемник.    

Полное пошаговое руководство по созданию этого конвейера см. в разделе [Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md). 

### <a name="sample-transformation-pipeline"></a>Пример конвейера преобразования
В следующем примере конвейера содержится одно действие типа **HDInsightHive** in the **действий** . В этом примере [действие HDInsight Hive](data-factory-hive-activity.md) преобразовывает данные из хранилища BLOB-объектов Azure, запуская файл сценария Hive в кластере Azure HDInsight Hadoop. 

```json
{
    "name": "TransformPipeline",
    "properties": {
        "description": "My first Azure Data Factory pipeline",
        "activities": [
            {
                "type": "HDInsightHive",
                "typeProperties": {
                    "scriptPath": "adfgetstarted/script/partitionweblogs.hql",
                    "scriptLinkedService": "AzureStorageLinkedService",
                    "defines": {
                        "inputtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/inputdata",
                        "partitionedtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/partitioneddata"
                    }
                },
                "inputs": [
                    {
                        "name": "AzureBlobInput"
                    }
                ],
                "outputs": [
                    {
                        "name": "AzureBlobOutput"
                    }
                ],
                "policy": {
                    "concurrency": 1,
                    "retry": 3
                },
                "scheduler": {
                    "frequency": "Month",
                    "interval": 1
                },
                "name": "RunSampleHiveActivity",
                "linkedServiceName": "HDInsightOnDemandLinkedService"
            }
        ],
        "start": "2016-04-01T00:00:00",
        "end": "2016-04-02T00:00:00",
        "isPaused": false
    }
}
```

Обратите внимание на следующие моменты. 

* В разделе activities есть только одно действие, параметр **type** которого имеет значение **HDInsightHive**.
* Файл сценария Hive, **partitionweblogs.hql**, хранится в учетной записи хранения Azure (указывается с помощью свойства scriptLinkedService, имеющего значение **AzureStorageLinkedService**) в папке **script** в контейнере **adfgetstarted**.
* Раздел **defines** используется, чтобы указать параметры среды выполнения, передаваемые скрипту Hive в качестве значений конфигурации Hive (например, `${hiveconf:inputtable}`, `${hiveconf:partitionedtable}`).

В разделе [ДЕЙСТВИЯ ПРЕОБРАЗОВАНИЯ ДАННЫХ](#data-transformation-activities) этой статьи представлены примеры JSON, определяющие действия преобразования в конвейере.

Полное пошаговое руководство по созданию данного конвейера см. в разделе [Учебник. Создание первого конвейера для обработки данных с помощью кластера Hadoop](data-factory-build-your-first-pipeline.md). 

## <a name="linked-service"></a>Связанные службы
Высокоуровневая структура определения связанной службы выглядит следующим образом:

```json
{
    "name": "<name of the linked service>",
    "properties": {
        "type": "<type of the linked service>",
        "typeProperties": {
        }
    }
}
```

В приведенной ниже таблице описаны свойства, используемые в определениях JSON действия.

| Свойство | ОПИСАНИЕ | Обязательно |
| -------- | ----------- | -------- | 
| name | Имя связанной службы. | Yes | 
| properties - type | Тип связанной службы. Например, служба хранилища Azure, база данных SQL Azure. |
| typeProperties | Раздел typeProperties содержит элементы, различные для каждого хранилища данных или вычислительной среды. В разделе о [хранилищах данных](#datastores) указаны все связанные службы хранилищ данных, а в разделе о [вычислительных средах](#compute-environments) — все связанные службы вычислений. |   

## <a name="dataset"></a>Выборка 
Набор данных в фабрике Azure имеет определение следующего вида.

```json
{
    "name": "<name of dataset>",
    "properties": {
        "type": "<type of dataset: AzureBlob, AzureSql etc...>",
        "external": <boolean flag to indicate external data. only for input datasets>,
        "linkedServiceName": "<Name of the linked service that refers to a data store.>",
        "structure": [
            {
                "name": "<Name of the column>",
                "type": "<Name of the type>"
            }
        ],
        "typeProperties": {
            "<type specific property>": "<value>",
            "<type specific property 2>": "<value 2>",
        },
        "availability": {
            "frequency": "<Specifies the time unit for data slice production. Supported frequency: Minute, Hour, Day, Week, Month>",
            "interval": "<Specifies the interval within the defined frequency. For example, frequency set to 'Hour' and interval set to 1 indicates that new data slices should be produced hourly>"
        },
       "policy":
        {      
        }
    }
}
```

В следующей таблице описаны свойства приведенного выше объекта JSON.   

| Свойство | ОПИСАНИЕ | Обязательно | значение по умолчанию |
| --- | --- | --- | --- |
| name | Имя набора данных. Правила именования для фабрики данных Azure описаны [здесь](data-factory-naming-rules.md) . |Yes |Нет данных |
| Тип | Тип набора данных. Укажите один из типов, которые поддерживаются фабрикой данных Azure (например: AzureBlob, AzureSqlTable). В разделе [ХРАНИЛИЩА ДАННЫХ](#data-stores) представлены все хранилища данных и типы наборов данных, поддерживаемые фабрикой данных. | 
| structure | Схема набора данных. Она содержит столбцы, их типы и т. д. | Нет  |Нет данных |
| typeProperties | Свойства, соответствующие выбранному типу. В разделе [ХРАНИЛИЩА ДАННЫХ](#data-stores) указаны поддерживаемые типы и их свойства. |Yes |Нет данных |
| external | Этот логический флаг указывает, создается ли набор данных конвейером фабрики данных явным образом. |Нет  |false |
| availability | Определяет окно обработки или модель среза для создания набора данных. Дополнительные сведения о модели срезов набора данных см. в статье [Планирование и исполнение с использованием фабрики данных](data-factory-scheduling-and-execution.md). |Yes |Нет данных |
| policy |Определяет условия, которым должен соответствовать срез данных. <br/><br/>Дополнительные сведения см. в разделе [Политика наборов данных](#Policy). |Нет  |Нет данных |

В каждом столбце раздела **structure** содержатся следующие свойства:

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| name |Имя столбца. |Yes |
| Тип |Тип данных столбца.  |Нет  |
| culture |Язык и региональные параметры на основе .NET, используемые при указании типа, если это тип .NET `Datetime` или `Datetimeoffset`. Значение по умолчанию — `en-us`. |Нет  |
| свойства |Строка формата, используемая при указании типа, если это тип .NET `Datetime` или `Datetimeoffset`. |Нет  |

В приведенном ниже примере набор данных содержит три столбца: `slicetimestamp`, `projectname` и `pageviews` с типом String, String и Decimal соответственно.

```json
structure:  
[
    { "name": "slicetimestamp", "type": "String"},
    { "name": "projectname", "type": "String"},
    { "name": "pageviews", "type": "Decimal"}
]
```

В таблице ниже перечислены свойства, которые можно использовать в разделе **availability**.

| Свойство | ОПИСАНИЕ | Обязательно | значение по умолчанию |
| --- | --- | --- | --- |
| frequency |Указывает единицу времени, которая определяет частоту создания среза данных.<br/><br/><b>Поддерживаемые значения</b>: Minute, Hour, Day, Week, Month. |Yes |Нет данных |
| interval |Задает множитель для частоты.<br/><br/>"Интервал х частоты" определяет частоту создания срезов.<br/><br/>Если нужно, чтобы срез в наборе данных создавался каждый час, задайте для параметра <b>frequency</b> значение <b>Hour</b>, а для параметра <b>interval</b> — значение <b>1</b>.<br/><br/><b>Примечание.</b> Если вы выбрали значение Minute для параметра Frequency, рекомендуем, чтобы интервал длился не менее 15 минут. |Yes |Нет данных |
| style |Указывает, когда выполняется срез: в начале или в конце интервала.<ul><li>StartOfInterval</li><li>EndOfInterval</li></ul><br/><br/>Если для Frequency задано значение Month, а для Style — EndOfInterval, срез данных будет создаваться в последний день месяца. Если для Style задано значение StartOfInterval, срез создается в первый день месяца.<br/><br/>Если для frequency задано значение Day, а для style — EndOfInterval, срез данных будет создаваться в последний час дня.<br/><br/>Если для Frequency задано значение Hour, а для Style — EndOfInterval, срез создается в конце часа. Например, для периода с 13:00 до 14:00 срез создается в 14:00. |Нет  |EndOfInterval |
| anchorDateTime |Определяет момент времени, на основе которого планировщик вычисляет границы среза набора данных. <br/><br/><b>Примечание</b>. Если параметр AnchorDateTime содержит элементы с большей степенью детализации, чем значение параметра frequency, эти элементы игнорируются. <br/><br/>Например, если для <b>интервала</b> задано значение <b>ежечасно</b> (frequency = Hour, interval = 1), а <b>AnchorDateTime</b> содержит <b>минуты и секунды</b>, то <b>минуты и секунды</b> из параметра AnchorDateTime не учитываются. |Нет  |01/01/0001 |
| offset |Интервал времени, на который сдвигаются начало и конец всех срезов данных. <br/><br/><b>Примечание</b>. Если указаны значения для обоих параметров (anchorDateTime и offset), сдвиг вычисляется с учетом обоих значений. |Нет  |Нет данных |

В следующем примере раздел availability определяет то, что набор выходных данных создается ежечасно (или) доступен каждый час.

```json
"availability":    
{    
    "frequency": "Hour",        
    "interval": 1    
}
```

Раздел **policy** в определении набора данных содержит условия, которым должен соответствовать срез данных.

| Имя политики | ОПИСАНИЕ | Применяется к | Обязательно | значение по умолчанию |
| --- | --- | --- | --- | --- |
| minimumSizeMB |Проверяет, удовлетворяют ли данные в **большом двоичном объекте Azure** требованиям к минимальному размеру (в мегабайтах). |большом двоичном объекте Azure |Нет  |Нет данных |
| minimumRows |Проверяет, содержат ли данные в **базе данных SQL Azure** или **таблице Azure** минимально необходимое количество строк. |<ul><li>Базы данных SQL Azure</li><li>таблице Azure</li></ul> |Нет  |Нет данных |

**Пример.**

```json
"policy":

{
    "validation":
    {
        "minimumSizeMB": 10.0
    }
}
```

Если набор данных не создается фабрикой данных Azure, он должен быть помечен как **external**. Обычно этот параметр относится к входным данным для первого действия в конвейере, если не используется цепочка действий или конвейеров.

| name | ОПИСАНИЕ | Обязательно | По умолчанию |
| --- | --- | --- | --- |
| dataDelay |Время задержки проверки на наличие внешних данных для определенного среза. Например, если данные доступны ежечасно, проверку доступности внешних данных (и того, находится ли соответствующий срез в состоянии Ready) можно отложить, используя политику dataDelay.<br/><br/>Применяется только к настоящему времени.  Например, если сейчас 13:00 и для dataDelay задано значение "10 минут", то проверка начинается в 13:10.<br/><br/>Этот параметр не влияет на срезы в прошлом (срезы, у которых параметры Slice End Time и dataDelay имеют значение раньше текущего времени), и они обрабатываются без задержки.<br/><br/>Время после 23:59 необходимо указать в формате `day.hours:minutes:seconds`. Например, чтобы задать 24 часа, не используйте формат 24:00:00; вместо этого укажите 1.00:00:00. Значение 24:00:00 обозначает 24 дня (24.00:00:00). Чтобы задать 1 день и 4 часа, укажите 1:04:00:00. |Нет  |0 |
| retryInterval |Время ожидания после сбоя до повторной попытки. В случае сбоя попытки следующая попытка выполняется после retryInterval. <br/><br/>Предположим, что первая попытка началась в 13:00. Если она длится одну минуту и завершается сбоем, то повторная попытка начнется в 13:02 (13:00 + время выполнения первой проверки (1 минута) + интервал повтора (1 минута)). <br/><br/>Срезы в прошлом обрабатываются без задержки. Повторная попытка происходит незамедлительно. |Нет  |00:01:00 (1 минута) |
| retryTimeout |Время ожидания для каждой следующей повторной попытки.<br/><br/>Если это значение составляет 10 минут, проверка должна занимать не более 10 минут. Если проверка длится больше 10 минут, возникает ошибка времени ожидания.<br/><br/>Если такая ошибка возникает во время всех попыток, срез помечается как TimedOut. |Нет  |00:10:00 (10 минут) |
| maximumRetry |Максимальное количество попыток проверки доступности внешних данных. Максимальное допустимое значение — 10. |Нет  |3 |


## <a name="data-stores"></a>ХРАНИЛИЩА ДАННЫХ
В разделе [Связанная служба](#linked-service) приводятся описания стандартных элементов JSON для всех типов связанных служб. Этот раздел содержит сведения об элементах JSON, характерных для каждого хранилища данных.

В разделе [Набор данных](#dataset) приводятся описания стандартных элементов JSON для всех типов наборов данных. Этот раздел содержит сведения об элементах JSON, характерных для каждого хранилища данных.

В разделе [Действие](#activity) приводятся описания стандартных элементов JSON для всех типов действий. Этот раздел содержит сведения об элементах JSON, характерных для каждого хранилища данных, когда оно используется как источник или приемник в действии копирования.  

Щелкните ссылку для интересующего вас хранилища, чтобы просмотреть схемы JSON для связанной службы, набора данных, источника и приемника для действия копирования.

| Категория | Хранилище данных 
|:--- |:--- |
| **Таблицы Azure** |[хранилище BLOB-объектов Azure](#azure-blob-storage) |
| &nbsp; |[Хранилище озера данных Azure](#azure-datalake-store) |
| &nbsp; |[База данных Azure Cosmos](#azure-cosmos-db) |
| &nbsp; |[база данных SQL Azure;](#azure-sql-database) |
| &nbsp; |[Хранилище данных Azure SQL](#azure-sql-data-warehouse) |
| &nbsp; |[Поиск Azure;](#azure-search) |
| &nbsp; |[Хранилище таблиц Azure](#azure-table-storage) |
| **Базы данных** |[Amazon Redshift](#amazon-redshift) |
| &nbsp; |[IBM DB2](#ibm-db2) |
| &nbsp; |[MySQL](#mysql) |
| &nbsp; |[Oracle](#oracle) |
| &nbsp; |[PostgreSQL](#postgresql) |
| &nbsp; |[SAP Business Warehouse](#sap-business-warehouse) |
| &nbsp; |[SAP HANA](#sap-hana) |
| &nbsp; |[SQL Server](#sql-server) |
| &nbsp; |[Sybase](#sybase) |
| &nbsp; |[Teradata](#teradata) |
| **NoSQL** |[Cassandra](#cassandra) |
| &nbsp; |[MongoDB](#mongodb) |
| **File** |[Amazon S3](#amazon-s3) |
| &nbsp; |[Перемещение данных в локальную файловую систему или из нее с помощью фабрики данных Azure](#file-system) |
| &nbsp; |[FTP](#ftp) |
| &nbsp; |[HDFS](#hdfs) |
| &nbsp; |[SFTP](#sftp) |
| **Прочее** |[HTTP](#http) |
| &nbsp; |[OData](#odata) |
| &nbsp; |[ODBC](#odbc) |
| &nbsp; |[Salesforce](#salesforce) |
| &nbsp; |[Веб-таблица](#web-table) |

## <a name="azure-blob-storage"></a>Хранилище больших двоичных объектов Azure

### <a name="linked-service"></a>Связанные службы
Существует два типа связанных служб: связанная служба хранилища Azure и связанная служба SAS хранилища Azure.

#### <a name="azure-storage-linked-service"></a>Связанная служба хранилища Azure
Чтобы связать учетную запись хранения Azure с фабрикой данных Azure с помощью **ключа учетной записи**, создайте связанную службу хранилища Azure. Для определения связанной службы хранилища Azure задайте **AzureStorage** в качестве **типа** связанной службы. Затем укажите следующие свойства в разделе **typeProperties**:  

| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| connectionString |В свойстве connectionString указываются сведения, необходимые для подключения к службе хранилища Azure. |Yes |

##### <a name="example"></a>Пример  

```json
{
    "name": "StorageLinkedService",
    "properties": {
        "type": "AzureStorage",
        "typeProperties": {
            "connectionString": "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"
        }
    }
}
```

#### <a name="azure-storage-sas-linked-service"></a>Связанная служба SAS хранилища Azure
Связанная служба SAS хранилища Azure позволяет связать учетную запись хранения Azure с фабрикой данных Azure с помощью подписанного URL-адреса (SAS). В этом случае фабрика данных получает ограниченный или привязанный ко времени доступ ко всем или конкретным ресурсам (BLOB-объектам или контейнерам) в хранилище. Чтобы связать вашу учетную запись хранения Azure с фабрикой данных с помощью подписанного URL-адреса, создайте связанную службу SAS хранилища Azure. Для определения связанной службы SAS хранилища Azure задайте **AzureStorageSas** в качестве **типа** связанной службы. Затем укажите следующие свойства в разделе **typeProperties**:   

| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| sasUri |Укажите URI подписанного URL-адреса к ресурсам хранилища Azure, например BLOB-объектам, контейнерам или таблицам. |Yes |

##### <a name="example"></a>Пример

```json
{  
    "name": "StorageSasLinkedService",  
    "properties": {  
        "type": "AzureStorageSas",  
        "typeProperties": {  
            "sasUri": "<storageUri>?<sasToken>"   
        }  
    }  
}  
```

Дополнительные сведения об этих связанных службах см. в статье о [соединителе хранилища BLOB-объектов Azure](data-factory-azure-blob-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Чтобы определить набор данных большого двоичного объекта Azure, задайте **AzureBlob** в качестве **типа** набора данных. Затем укажите следующие свойства большого двоичного объекта Azure в разделе **typeProperties**: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| folderPath |Путь контейнеру и папке в хранилище BLOB-объектов. Пример: myblobcontainer\myblobfolder\ |Yes |
| fileName |Имя большого двоичного объекта. Свойство fileName является необязательным и в нем учитывается регистр знаков.<br/><br/>Если указать имя файла, то действие (включая копирование) работает с определенным большим двоичным объектом.<br/><br/>Если значение fileName не указано, то копируются все большие двоичные объекты в folderPath для входного набора данных.<br/><br/>Если свойство fileName не указано для выходного набора данных, то имя созданного файла будет иметь следующий формат: Data.<Guid>.txt (например, Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.). |Нет  |
| partitionedBy |Необязательное свойство. Его можно использовать, чтобы указать динамические путь к папке и имя файла для временного ряда данных. Например, путь к папке (значение folderPath) каждый час может быть другим. |Нет  |
| свойства | Поддерживаются следующие типы формата: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat**, **ParquetFormat**. Свойству **type** в разделе format необходимо присвоить одно из этих значений. Дополнительные сведения см. в разделах о [текстовом формате](data-factory-supported-file-and-compression-formats.md#text-format), [формате Json](data-factory-supported-file-and-compression-formats.md#json-format), [формате Avro](data-factory-supported-file-and-compression-formats.md#avro-format), [формате Orc](data-factory-supported-file-and-compression-formats.md#orc-format) и [ формате Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format). <br><br> Если требуется скопировать файлы между файловыми хранилищами **как есть** (двоичное копирование), можно пропустить раздел форматирования в определениях входного и выходного наборов данных. |Нет  |
| compression | Укажите тип и уровень сжатия данных. Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**. Поддерживаемые уровни: **Optimal** и **Fastest**. Узнайте больше о [форматах файлов и сжатия данных в фабрике данных Azure](data-factory-supported-file-and-compression-formats.md#compression-support). |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "AzureBlobInput",
    "properties": {
        "type": "AzureBlob",
        "linkedServiceName": "AzureStorageLinkedService",
        "typeProperties": {
            "fileName": "input.log",
            "folderPath": "adfgetstarted/inputdata",
            "format": {
                "type": "TextFormat",
                "columnDelimiter": ","
            }
        },
        "availability": {
            "frequency": "Month",
            "interval": 1
        },
        "external": true,
        "policy": {}
    }
}
 ```


Дополнительные сведения см. в статье о [соединителе больших двоичных объектов Azure](data-factory-azure-blob-connector.md#dataset-properties).

### <a name="blobsource-in-copy-activity"></a>BlobSource в действии копирования
При копировании данных из хранилища BLOB-объектов Azure задайте **BlobSource** в качестве **типа источника** для действия копирования и задайте следующие свойства в разделе **source**:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| recursive |Указывает, следует ли читать данные рекурсивно из вложенных папок или только из указанной папки. |True (значение по умолчанию), False |Нет  |

#### <a name="example-blobsource"></a>Пример: BlobSource **
```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "AzureBlobtoSQL",
            "description": "Copy Activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureBlobInput"
            }],
            "outputs": [{
                "name": "AzureSqlOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "BlobSource"
                },
                "sink": {
                    "type": "SqlSink"
                }
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```
### <a name="blobsink-in-copy-activity"></a>BlobSink в действии копирования
При копировании данных в хранилище BLOB-объектов Azure задайте **BlobSink** в качестве **типа приемника** для действия копирования и задайте следующие свойства в разделе **sink**:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| copyBehavior |Это свойство определяет поведение функции копирования, когда в качестве источника используется BlobSource или FileSystem. |<b>PreserveHierarchy:</b> сохраняет иерархию файлов в целевой папке. Относительный путь исходного файла в исходной папке идентичен относительному пути целевого файла в целевой папке.<br/><br/><b>FlattenHierarchy</b>: все файлы из исходной папки размещаются на первом уровне в целевой папке. Целевые файлы имеют автоматически сформированное имя. <br/><br/><b>MergeFiles (по умолчанию):</b> объединяет все файлы из исходной папки в один файл. Если указано имя файла или большого двоичного объекта, именем объединенного файла будет указанное имя; в противном случае имя файла будет автоматически сформировано. |Нет  |

#### <a name="example-blobsink"></a>Пример: BlobSink

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline for copy activity",
        "activities": [{
            "name": "AzureSQLtoBlob",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureSQLInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "SqlSource",
                    "SqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе больших двоичных объектов Azure](data-factory-azure-blob-connector.md#copy-activity-properties). 

## <a name="azure-data-lake-store"></a>Хранилище озера данных Azure

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы Azure Data Lake Store задайте **AzureDataLakeStore** в качестве типа связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| Тип | Для свойства type следует задать значение **AzureDataLakeStore** | Yes |
| dataLakeStoreUri | Указывает сведения об учетной записи хранилища озера данных Azure Используемый формат: `https://[accountname].azuredatalakestore.net/webhdfs/v1` или `adl://[accountname].azuredatalakestore.net/`. | Yes |
| subscriptionId | Идентификатор подписки Azure, к которой принадлежит Data Lake Store. | Необходимо для приемника |
| имя_группы_ресурсов | Имя группы ресурсов Azure, к которой принадлежит Data Lake Store. | Необходимо для приемника |
| servicePrincipalId | Укажите идентификатора клиента приложения. | Да (для проверки подлинности на основе субъекта-службы) |
| servicePrincipalKey | Укажите ключ приложения. | Да (для проверки подлинности на основе субъекта-службы) |
| tenant | Укажите сведения о клиенте (доменное имя или идентификатор клиента), в котором находится приложение. Его можно получить, наведя указатель мыши на правый верхний угол страницы портала Azure. | Да (для проверки подлинности на основе субъекта-службы) |
| authorization | Нажмите кнопку **Авторизовать** в **редакторе фабрики данных** и введите учетные данные. URL-адрес авторизации будет создан автоматически и присвоен этому свойству. | Да (для проверки подлинности на основе учетных данных пользователя)|
| sessionid | Идентификатор сеанса OAuth из сеанса авторизации OAuth. Каждый идентификатор сеанса является уникальным и используется только один раз. Этот параметр создается автоматически при использовании редактора фабрики данных. | Да (для проверки подлинности на основе учетных данных пользователя) |

#### <a name="example-using-service-principal-authentication"></a>Пример: использование проверки подлинности на основе субъекта-службы
```json
{
    "name": "AzureDataLakeStoreLinkedService",
    "properties": {
        "type": "AzureDataLakeStore",
        "typeProperties": {
            "dataLakeStoreUri": "https://<accountname>.azuredatalakestore.net/webhdfs/v1",
            "servicePrincipalId": "<service principal id>",
            "servicePrincipalKey": "<service principal key>",
            "tenant": "<tenant info. Example: microsoft.onmicrosoft.com>"
        }
    }
}
```

#### <a name="example-using-user-credential-authentication"></a>Пример: использование проверки подлинности на основе учетных данных пользователя
```json
{
    "name": "AzureDataLakeStoreLinkedService",
    "properties": {
        "type": "AzureDataLakeStore",
        "typeProperties": {
            "dataLakeStoreUri": "https://<accountname>.azuredatalakestore.net/webhdfs/v1",
            "sessionId": "<session ID>",
            "authorization": "<authorization URL>",
            "subscriptionId": "<subscription of ADLS>",
            "resourceGroupName": "<resource group of ADLS>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Azure Data Lake Store](data-factory-azure-datalake-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных Azure Data Lake Store задайте **AzureDataLakeStore** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| folderPath |Путь к контейнеру и папке в хранилище озера данных Azure. |Yes |
| fileName |Имя файла в хранилище озера данных Azure. Свойство fileName является необязательным и в нем учитывается регистр знаков. <br/><br/>Если указать имя файла, то действие (включая копирование) работает с определенным файлом.<br/><br/>Если значение fileName не указано, то копируются все файлы в folderPath для входного набора данных.<br/><br/>Если свойство fileName не указано для выходного набора данных, то имя созданного файла будет иметь следующий формат: Data<Guid>.txt (например, Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.). |Нет  |
| partitionedBy |Необязательное свойство. Его можно использовать, чтобы указать динамические путь к папке и имя файла для временного ряда данных. Например, путь к папке (значение folderPath) каждый час может быть другим. |Нет  |
| свойства | Поддерживаются следующие типы формата: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat**, **ParquetFormat**. Свойству **type** в разделе format необходимо присвоить одно из этих значений. Дополнительные сведения см. в разделах о [текстовом формате](data-factory-supported-file-and-compression-formats.md#text-format), [формате Json](data-factory-supported-file-and-compression-formats.md#json-format), [формате Avro](data-factory-supported-file-and-compression-formats.md#avro-format), [формате Orc](data-factory-supported-file-and-compression-formats.md#orc-format) и [ формате Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format). <br><br> Если требуется скопировать файлы между файловыми хранилищами **как есть** (двоичное копирование), можно пропустить раздел форматирования в определениях входного и выходного наборов данных. |Нет  |
| compression | Укажите тип и уровень сжатия данных. Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**. Поддерживаемые уровни: **Optimal** и **Fastest**. Узнайте больше о [форматах файлов и сжатия данных в фабрике данных Azure](data-factory-supported-file-and-compression-formats.md#compression-support). |Нет  |

#### <a name="example"></a>Пример
```json
{
    "name": "AzureDataLakeStoreInput",
    "properties": {
        "type": "AzureDataLakeStore",
        "linkedServiceName": "AzureDataLakeStoreLinkedService",
        "typeProperties": {
            "folderPath": "datalake/input/",
            "fileName": "SearchLog.tsv",
            "format": {
                "type": "TextFormat",
                "rowDelimiter": "\n",
                "columnDelimiter": "\t"
            }
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Azure Data Lake Store](data-factory-azure-datalake-connector.md#dataset-properties). 

### <a name="azure-data-lake-store-source-in-copy-activity"></a>Источник Azure Data Lake Store в действии копирования
При копировании данных из Azure Data Lake Store задайте **AzureDataLakeStoreSource** в качестве **типа источника** для действия копирования и укажите следующие свойства в разделе **source**:

**AzureDataLakeStoreSource** поддерживает следующие свойства в разделе **typeProperties**.

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| recursive |Указывает, следует ли читать данные рекурсивно из вложенных папок или только из указанной папки. |True (значение по умолчанию), False |Нет  |

#### <a name="example-azuredatalakestoresource"></a>Пример: AzureDataLakeStoreSource

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline for copy activity",
        "activities": [{
            "name": "AzureDakeLaketoBlob",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureDataLakeStoreInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "AzureDataLakeStoreSource"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе Azure Data Lake Store](data-factory-azure-datalake-connector.md#copy-activity-properties).

### <a name="azure-data-lake-store-sink-in-copy-activity"></a>Приемник Azure Data Lake Store в действии копирования
При копировании данных в Azure Data Lake Store задайте **AzureDataLakeStoreSink** в качестве **типа приемника** для действия копирования и укажите следующие свойства в разделе **sink**:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| copyBehavior |Определяет поведение копирования. |<b>PreserveHierarchy:</b> сохраняет иерархию файлов в целевой папке. Относительный путь исходного файла в исходной папке идентичен относительному пути целевого файла в целевой папке.<br/><br/><b>FlattenHierarchy</b>: все файлы из исходной папки создаются на первом уровне в целевой папке. Целевые файлы создаются с автоматически создаваемыми именами.<br/><br/><b>MergeFiles</b>: объединяет все файлы из исходной папки в один файл. Если указано имя файла или большого двоичного объекта, именем объединенного файла будет указанное имя; в противном случае имя файла будет автоматически сформировано. |Нет  |

#### <a name="example-azuredatalakestoresink"></a>Пример: AzureDataLakeStoreSink
```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "AzureBlobtoDataLake",
            "description": "Copy Activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureBlobInput"
            }],
            "outputs": [{
                "name": "AzureDataLakeStoreOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "BlobSource"
                },
                "sink": {
                    "type": "AzureDataLakeStoreSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе Azure Data Lake Store](data-factory-azure-datalake-connector.md#copy-activity-properties). 

## <a name="azure-cosmos-db"></a>Azure Cosmos DB  

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы Azure Cosmos DB задайте **DocumentDb** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| **Свойство** | **Описание** | **Обязательный** |
| --- | --- | --- |
| connectionString |Укажите сведения, необходимые для подключения к базе данных Azure Cosmos DB. |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "CosmosDBLinkedService",
    "properties": {
        "type": "DocumentDb",
        "typeProperties": {
            "connectionString": "AccountEndpoint=<EndpointUrl>;AccountKey=<AccessKey>;Database=<Database>"
        }
    }
}
```
Дополнительные сведения см. в статье о [соединителе Azure Cosmos DB](data-factory-azure-documentdb-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных Azure Cosmos DB задайте **DocumentDbCollection** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| **Свойство** | **Описание** | **Обязательный** |
| --- | --- | --- |
| collectionName |Имя коллекции Azure Cosmos DB. |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "PersonCosmosDBTable",
    "properties": {
        "type": "DocumentDbCollection",
        "linkedServiceName": "CosmosDBLinkedService",
        "typeProperties": {
            "collectionName": "Person"
        },
        "external": true,
        "availability": {
            "frequency": "Day",
            "interval": 1
        }
    }
}
```
Дополнительные сведения см. в статье о [соединителе Azure Cosmos DB](data-factory-azure-documentdb-connector.md#dataset-properties).

### <a name="azure-cosmos-db-collection-source-in-copy-activity"></a>Источник коллекции Azure Cosmos DB в действии копирования
При копировании данных из Azure Cosmos DB задайте **DocumentDbCollectionSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:


| **Свойство** | **Описание** | **Допустимые значения** | **Обязательный** |
| --- | --- | --- | --- |
| query |Запрос, нужный для чтения данных. |Строка запроса, поддерживаемая Azure Cosmos DB. <br/><br/>Пример: `SELECT c.BusinessEntityID, c.PersonType, c.NameStyle, c.Title, c.Name.First AS FirstName, c.Name.Last AS LastName, c.Suffix, c.EmailPromotion FROM c WHERE c.ModifiedDate > \"2009-01-01T00:00:00\"` |Нет  <br/><br/>Если не указано, то выполняется инструкция SQL `select <columns defined in structure> from mycollection`. |
| nestingSeparator |Специальный символ, обозначающий, что документ является вложенным. |Любой символ. <br/><br/>Azure Cosmos DB — это хранилище NoSQL для JSON-документов, в которых разрешено использовать вложенные структуры. Фабрика данных Azure позволяет обозначать иерархию с помощью разделителя nestingSeparator. В приведенных выше примерах это точка. Благодаря этому разделителю действие копирование создаст объект Name с тремя дочерними элементами, First, Middle и Last, в соответствии с элементами Name.First, Name.Middle и Name.Last в определении таблицы. |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "DocDbToBlobPipeline",
    "properties": {
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "DocumentDbCollectionSource",
                    "query": "SELECT Person.Id, Person.Name.First AS FirstName, Person.Name.Middle as MiddleName, Person.Name.Last AS LastName FROM Person",
                    "nestingSeparator": "."
                },
                "sink": {
                    "type": "BlobSink",
                    "blobWriterAddHeader": true,
                    "writeBatchSize": 1000,
                    "writeBatchTimeout": "00:00:59"
                }
            },
            "inputs": [{
                "name": "PersonCosmosDBTable"
            }],
            "outputs": [{
                "name": "PersonBlobTableOut"
            }],
            "policy": {
                "concurrency": 1
            },
            "name": "CopyFromCosmosDbToBlob"
        }],
        "start": "2016-04-01T00:00:00",
        "end": "2016-04-02T00:00:00"
    }
}
```

### <a name="azure-cosmos-db-collection-sink-in-copy-activity"></a>Приемник коллекции Azure Cosmos DB в действии копирования
При копировании данных в Azure Cosmos DB задайте **DocumentDbCollectionSink** в качестве **типа приемника** для действия копирования и укажите в разделе **sink** следующие свойства:

| **Свойство** | **Описание** | **Допустимые значения** | **Обязательный** |
| --- | --- | --- | --- |
| nestingSeparator |Такой специальный символ в имени исходного столбца, который указывает, что нужен вложенный документ. <br/><br/>См. пример выше: элемент `Name.First` в выходной таблице обуславливает в документе Cosmos DB такую структуру JSON:<br/><br/>"Name": {<br/>    "First": "John"<br/>}, |Символ, используемый для разделения уровней вложенности.<br/><br/>Значение по умолчанию — `.` (точка). |Символ, используемый для разделения уровней вложенности. <br/><br/>Значение по умолчанию — `.` (точка). |
| writeBatchSize |Число параллельных запросов к службе Azure Cosmos DB для создания документов.<br/><br/>При копировании данных из или в Azure Cosmos DB с помощью этого свойства можно оптимизировать производительность. Если увеличить значение свойства writeBatchSize, то производительность повышается, потому что в Azure Cosmos DB начинает поступать больше параллельных запросов. Однако необходимо избежать регулирования, которое может вызвать сообщение об ошибке: "Высокая частота запросов".<br/><br/>Регулирование может произойти по ряду причин, включая размер документов, количество терминов в документах, политику индексации целевой коллекции и т. д. Для операций копирования вы можете использовать коллекцию получше (например, S3), чтобы обеспечить максимальную пропускную способность (2500 единиц запроса в секунду). |Целое число  |Нет (значение по умолчанию — 5) |
| writeBatchTimeout |Время ожидания до выполнения операции, пока не завершится срок ее действия. |Интервал времени<br/><br/> Пример: 00:30:00 (30 минут). |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "BlobToDocDbPipeline",
    "properties": {
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "BlobSource"
                },
                "sink": {
                    "type": "DocumentDbCollectionSink",
                    "nestingSeparator": ".",
                    "writeBatchSize": 2,
                    "writeBatchTimeout": "00:00:00"
                },
                "translator": {
                    "type": "TabularTranslator",
                    "ColumnMappings": "FirstName: Name.First, MiddleName: Name.Middle, LastName: Name.Last, BusinessEntityID: BusinessEntityID, PersonType: PersonType, NameStyle: NameStyle, Title: Title, Suffix: Suffix"
                }
            },
            "inputs": [{
                "name": "PersonBlobTableIn"
            }],
            "outputs": [{
                "name": "PersonCosmosDbTableOut"
            }],
            "policy": {
                "concurrency": 1
            },
            "name": "CopyFromBlobToCosmosDb"
        }],
        "start": "2016-04-14T00:00:00",
        "end": "2016-04-15T00:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе Azure Cosmos DB](data-factory-azure-documentdb-connector.md#copy-activity-properties).

## <a name="azure-sql-database"></a>Базы данных SQL Azure

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы базы данных SQL Azure задайте **AzureSqlDatabase** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| connectionString |В свойстве connectionString указываются сведения, необходимые для подключения к экземпляру базы данных SQL Azure. |Yes |

#### <a name="example"></a>Пример
```json
{
    "name": "AzureSqlLinkedService",
    "properties": {
        "type": "AzureSqlDatabase",
        "typeProperties": {
            "connectionString": "Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;User ID=<username>@<servername>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Azure SQL](data-factory-azure-sql-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных базы данных SQL Azure задайте **AzureSqlTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы или представления в экземпляре базы данных SQL Azure, на которое ссылается связанная служба. |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "AzureSqlInput",
    "properties": {
        "type": "AzureSqlTable",
        "linkedServiceName": "AzureSqlLinkedService",
        "typeProperties": {
            "tableName": "MyTable"
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```
Дополнительные сведения см. в статье о [соединителе Azure SQL](data-factory-azure-sql-connector.md#dataset-properties). 

### <a name="sql-source-in-copy-activity"></a>Источник SQL в действии копирования
При копировании данных из базы данных SQL Azure задайте **SqlSource** в качестве **типа источника** для действия копирования и задайте в разделе **source** следующие свойства:


| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| SqlReaderQuery |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Пример: `select * from MyTable`. |Нет  |
| sqlReaderStoredProcedureName |Имя хранимой процедуры, которая считывает данные из исходной таблицы. |Имя хранимой процедуры. |Нет  |
| storedProcedureParameters |Параметры для хранимой процедуры. |Пары имен и значений. Имена и регистр параметров должны совпадать с именами и регистром параметров хранимой процедуры. |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline for copy activity",
        "activities": [{
            "name": "AzureSQLtoBlob",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureSQLInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "SqlSource",
                    "SqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```
Дополнительные сведения см. в статье о [соединителе Azure SQL](data-factory-azure-sql-connector.md#copy-activity-properties). 

### <a name="sql-sink-in-copy-activity"></a>Приемник SQL в действии копирования
При копировании данных в базу данных SQL Azure задайте **SqlSink** в качестве **типа приемника** для действия копирования и задайте в разделе **sink** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| writeBatchTimeout |Время ожидания до выполнения операции пакетной вставки, пока не завершится срок ее действия. |Интервал времени<br/><br/> Пример: 00:30:00 (30 минут). |Нет  |
| writeBatchSize |Вставляет данные в таблицу SQL, когда размер буфера достигает значения writeBatchSize. |Целое число (количество строк) |Нет (значение по умолчанию — 10 000). |
| sqlWriterCleanupScript |Укажите запрос на выполнение действия копирования, позволяющий убедиться в том, что данные конкретного среза очищены. |Инструкция запроса. |Нет  |
| sliceIdentifierColumnName |Укажите имя столбца, в которое действие копирования добавляет автоматически созданный идентификатор среза. Этот идентификатор используется для очистки данных конкретного среза при повторном запуске. |Имя столбца с типом данных binary(32). |Нет  |
| sqlWriterStoredProcedureName |Имя хранимой процедуры, обновляющей данные или вставляющей их в целевую таблицу. |Имя хранимой процедуры. |Нет  |
| storedProcedureParameters |Параметры для хранимой процедуры. |Пары имен и значений. Имена и регистр параметров должны совпадать с именами и регистром параметров хранимой процедуры. |Нет  |
| sqlWriterTableType |Укажите имя типа таблицы для использования в хранимой процедуре. Действие копирования делает перемещаемые данные доступными во временной таблице этого типа. Код хранимой процедуры затем можно использовать для объединения копируемых и существующих данных. |Имя типа таблицы. |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "AzureBlobtoSQL",
            "description": "Copy Activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureBlobInput"
            }],
            "outputs": [{
                "name": "AzureSqlOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "BlobSource",
                    "blobColumnSeparators": ","
                },
                "sink": {
                    "type": "SqlSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе Azure SQL](data-factory-azure-sql-connector.md#copy-activity-properties). 

## <a name="azure-sql-data-warehouse"></a>Хранилище данных SQL Azure

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы хранилища данных SQL Azure задайте **AzureSqlDW** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| connectionString |Укажите сведения, необходимые для подключения к экземпляру хранилища данных SQL Azure, для свойства connectionString. |Yes |



#### <a name="example"></a>Пример

```json
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "connectionString": "Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;User ID=<username>@<servername>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе хранилища данных SQL Azure](data-factory-azure-sql-data-warehouse-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных хранилища данных SQL Azure задайте **AzureSqlDWTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы или представления в базе данных хранилища данных SQL Azure, на которое ссылается связанная служба. |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "AzureSqlDWInput",
    "properties": {
    "type": "AzureSqlDWTable",
        "linkedServiceName": "AzureSqlDWLinkedService",
        "typeProperties": {
            "tableName": "MyTable"
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе хранилища данных SQL Azure](data-factory-azure-sql-data-warehouse-connector.md#dataset-properties). 

### <a name="sql-dw-source-in-copy-activity"></a>Источник хранилища данных SQL в действии копирования
При копировании данных из хранилища данных SQL Azure задайте **SqlDWSource** в качестве **типа источника** для действия копирования и задайте в разделе **source** следующие свойства:


| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| SqlReaderQuery |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например, `select * from MyTable`. |Нет  |
| sqlReaderStoredProcedureName |Имя хранимой процедуры, которая считывает данные из исходной таблицы. |Имя хранимой процедуры. |Нет  |
| storedProcedureParameters |Параметры для хранимой процедуры. |Пары имен и значений. Имена и регистр параметров должны совпадать с именами и регистром параметров хранимой процедуры. |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline for copy activity",
        "activities": [{
            "name": "AzureSQLDWtoBlob",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureSqlDWInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "SqlDWSource",
                    "sqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе хранилища данных SQL Azure](data-factory-azure-sql-data-warehouse-connector.md#copy-activity-properties). 

### <a name="sql-dw-sink-in-copy-activity"></a>Приемник хранилища данных SQL в действии копирования
При копировании данных в хранилище данных SQL Azure задайте **SqlDWSink** в качестве **типа приемника** для действия копирования и укажите в разделе **sink** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| sqlWriterCleanupScript |Укажите запрос на выполнение действия копирования, позволяющий убедиться в том, что данные конкретного среза очищены. |Инструкция запроса. |Нет  |
| allowPolyBase |Указывает, следует ли использовать PolyBase (если применимо) вместо механизма BULKINSERT. <br/><br/> **Использование PolyBase — рекомендуемый способ загрузки данных в хранилище данных SQL.** |Истина <br/>False (по умолчанию) |Нет  |
| polyBaseSettings |Группа свойств, которые можно задать, если свойство **allowPolybase** имеет значение **true**. |&nbsp; |Нет  |
| rejectValue |Указывает количество или процент строк, которые могут быть отклонены, прежде чем запрос завершится с ошибкой. <br/><br/>Дополнительные сведения о параметрах отклонения PolyBase см. в подразделе **Аргументы** раздела [CREATE EXTERNAL TABLE (Transact-SQL)](https://msdn.microsoft.com/library/dn935021.aspx). |0 (по умолчанию), 1, 2, ... |Нет  |
| rejectType |Указывает тип значения, заданного для параметра rejectValue: литеральное значение или процент. |Value (по умолчанию), Percentage |Нет  |
| rejectSampleValue |Определяет количество строк, которое PolyBase следует получить до повторного вычисления процента отклоненных строк. |1, 2, … |Да, если **rejectType** имеет значение **percentage**. |
| useTypeDefault |Указывает способ обработки отсутствующих значений в текстовых файлах с разделителями, когда PolyBase получает данные из текстового файла.<br/><br/>Дополнительные сведения об этом свойстве см. в подразделе "Аргументы" раздела [CREATE EXTERNAL FILE FORMAT (Transact-SQL)](https://msdn.microsoft.com/library/dn935026.aspx). |True, False (по умолчанию) |Нет  |
| writeBatchSize |Вставляет данные в таблицу SQL, когда размер буфера достигает значения writeBatchSize. |Целое число (количество строк) |Нет (значение по умолчанию — 10 000). |
| writeBatchTimeout |Время ожидания до выполнения операции пакетной вставки, пока не завершится срок ее действия. |Интервал времени<br/><br/> Пример: 00:30:00 (30 минут). |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "AzureBlobtoSQLDW",
            "description": "Copy Activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureBlobInput"
            }],
            "outputs": [{
                "name": "AzureSqlDWOutput"
            }],
            "typeProperties": {
                "source": {
                "type": "BlobSource",
                    "blobColumnSeparators": ","
                },
                "sink": {
                    "type": "SqlDWSink",
                    "allowPolyBase": true
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе хранилища данных SQL Azure](data-factory-azure-sql-data-warehouse-connector.md#copy-activity-properties). 

## <a name="azure-search"></a>поиск Azure;

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы Поиска Azure задайте **AzureSearch** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| -------- | ----------- | -------- |
| URL-адрес | URL-адрес службы Поиска Azure. | Yes |
| key | Ключ администратора службы Поиска Azure. | Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "AzureSearchLinkedService",
    "properties": {
        "type": "AzureSearch",
        "typeProperties": {
            "url": "https://<service>.search.windows.net",
            "key": "<AdminKey>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Поиска Azure](data-factory-azure-search-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных Поиска Azure задайте **AzureSearchIndex** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| -------- | ----------- | -------- |
| Тип | Для свойства type необходимо задать значение **AzureSearchIndex**| Yes |
| indexName | Имя индекса Поиска Azure. Фабрика данных не создает индекс. Индекс должен существовать в Поиске Azure. | Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "AzureSearchIndexDataset",
    "properties": {
        "type": "AzureSearchIndex",
        "linkedServiceName": "AzureSearchLinkedService",
        "typeProperties": {
            "indexName": "products"
        },
        "availability": {
            "frequency": "Minute",
            "interval": 15
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Поиска Azure](data-factory-azure-search-connector.md#dataset-properties).

### <a name="azure-search-index-sink-in-copy-activity"></a>Приемник индекса Поиска Azure в действии копирования
При копировании данных в индекс Поиска Azure задайте **AzureSearchIndexSink** в качестве **типа приемника** для действия копирования и укажите в разделе **sink** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| -------- | ----------- | -------------- | -------- |
| WriteBehavior | Указывает действие (объединение или замена), выполняемое, если документ уже существует в индексе. | Merge (по умолчанию)<br/>Передать| Нет  |
| WriteBatchSize | Передает данные в индекс Поиска Azure, когда размер буфера достигает значения writeBatchSize. | 1–1000. Значение по умолчанию — 1000. | Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline for copy activity",
        "activities": [{
            "name": "SqlServertoAzureSearchIndex",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": " SqlServerInput"
            }],
            "outputs": [{
                "name": "AzureSearchIndexDataset"
            }],
            "typeProperties": {
                "source": {
                    "type": "SqlSource",
                    "SqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "AzureSearchIndexSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе Поиска Azure](data-factory-azure-search-connector.md#copy-activity-properties).

## <a name="azure-table-storage"></a>Хранилище таблиц Azure

### <a name="linked-service"></a>Связанные службы
Существует два типа связанных служб: связанная служба хранилища Azure и связанная служба SAS хранилища Azure.

#### <a name="azure-storage-linked-service"></a>Связанная служба хранилища Azure
Чтобы связать учетную запись хранения Azure с фабрикой данных Azure с помощью **ключа учетной записи**, создайте связанную службу хранилища Azure. Для определения связанной службы хранилища Azure задайте **AzureStorage** в качестве **типа** связанной службы. Затем укажите следующие свойства в разделе **typeProperties**:  

| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| Тип |Для свойства type необходимо задать значение **AzureStorage** |Yes |
| connectionString |В свойстве connectionString указываются сведения, необходимые для подключения к службе хранилища Azure. |Yes |

**Пример.**  

```json
{  
    "name": "StorageLinkedService",  
    "properties": {  
        "type": "AzureStorage",  
        "typeProperties": {  
            "connectionString": "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"  
        }  
    }  
}  
```

#### <a name="azure-storage-sas-linked-service"></a>Связанная служба SAS хранилища Azure
Связанная служба SAS хранилища Azure позволяет связать учетную запись хранения Azure с фабрикой данных Azure с помощью подписанного URL-адреса (SAS). В этом случае фабрика данных получает ограниченный или привязанный ко времени доступ ко всем или конкретным ресурсам (BLOB-объектам или контейнерам) в хранилище. Чтобы связать вашу учетную запись хранения Azure с фабрикой данных с помощью подписанного URL-адреса, создайте связанную службу SAS хранилища Azure. Для определения связанной службы SAS хранилища Azure задайте **AzureStorageSas** в качестве **типа** связанной службы. Затем укажите следующие свойства в разделе **typeProperties**:   

| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| Тип |Для свойства type необходимо задать значение **AzureStorageSas** |Yes |
| sasUri |Укажите URI подписанного URL-адреса к ресурсам хранилища Azure, например BLOB-объектам, контейнерам или таблицам. |Yes |

**Пример.**

```json
{  
    "name": "StorageSasLinkedService",  
    "properties": {  
        "type": "AzureStorageSas",  
        "typeProperties": {  
            "sasUri": "<storageUri>?<sasToken>"   
        }  
    }  
}  
```

Дополнительные сведения об этих связанных службах см. в статье о [соединителе Хранилища таблиц Azure](data-factory-azure-table-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных таблицы Azure задайте **AzureTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы в экземпляре базы данных таблиц Azure, на которое ссылается связанная служба. |Да. Если tableName указывается без azureTableSourceQuery, все записи из таблицы копируются в целевое расположение. Если azureTableSourceQuery указывается, записи из таблицы, удовлетворяющие запросу, копируются в целевое расположение. |

#### <a name="example"></a>Пример

```json
{
    "name": "AzureTableInput",
    "properties": {
        "type": "AzureTable",
        "linkedServiceName": "StorageLinkedService",
        "typeProperties": {
            "tableName": "MyTable"
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения об этих связанных службах см. в статье о [соединителе Хранилища таблиц Azure](data-factory-azure-table-connector.md#dataset-properties). 

### <a name="azure-table-source-in-copy-activity"></a>Источник таблицы Azure в действии копирования
При копировании данных из Хранилища таблиц Azure задайте **AzureTableSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| AzureTableSourceQuery |Используйте пользовательский запрос для чтения данных. |Строка запроса таблицы Azure. Примеры приведены в следующем разделе. |Нет. Если tableName указывается без azureTableSourceQuery, все записи из таблицы копируются в целевое расположение. Если azureTableSourceQuery указывается, записи из таблицы, удовлетворяющие запросу, копируются в целевое расположение. |
| azureTableSourceIgnoreTableNotFound |Указывает, игнорируются ли исключения таблицы. |TRUE<br/>FALSE |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline for copy activity",
        "activities": [{
            "name": "AzureTabletoBlob",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureTableInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "AzureTableSource",
                    "AzureTableSourceQuery": "PartitionKey eq 'DefaultPartitionKey'"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения об этих связанных службах см. в статье о [соединителе Хранилища таблиц Azure](data-factory-azure-table-connector.md#copy-activity-properties). 

### <a name="azure-table-sink-in-copy-activity"></a>Приемник таблицы Azure в действии копирования
При копировании данных в Хранилище таблиц Azure задайте **AzureTableSink** в качестве **типа приемника** для действия копирования и задайте следующие свойства в разделе **sink**:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| azureTableDefaultPartitionKeyValue |Значение ключа раздела по умолчанию, которое может использоваться приемником. |Строковое значение. |Нет  |
| azureTablePartitionKeyName |Укажите имя столбца, значения которого используются в качестве ключей секций. Если не указано, в качестве ключа раздела используется AzureTableDefaultPartitionKeyValue. |Имя столбца. |Нет  |
| azureTableRowKeyName |Укажите имя столбца, значения которого используются в качестве ключа строки. Если имя не указано, используйте для каждой строки идентификатор GUID. |Имя столбца. |Нет  |
| azureTableInsertType |Режим для вставки данных в таблицу Azure.<br/><br/>Это свойство контролирует, будут ли заменены или объединены значения в существующих строках в выходной таблице с совпадающими ключами секций и строк. <br/><br/>Чтобы узнать о действии этих параметров (merge и replace), ознакомьтесь со статьями [Insert or Merge Entity](https://msdn.microsoft.com/library/azure/hh452241.aspx) (Вставка или слияние сущностей) и [Insert or Replace Entity](https://msdn.microsoft.com/library/azure/hh452242.aspx) (Вставка или замена сущности). <br/><br> Этот параметр применяется на уровне строки, а не таблицы, и ни один из параметров не приводит к удалению строк в выходной таблице, которые отсутствуют во входных данных. |merge (по умолчанию)<br/>replace |Нет  |
| writeBatchSize |Вставка данных в таблицу Azure при достижении writeBatchSize или writeBatchTimeout. |Целое число (количество строк) |Нет (значение по умолчанию — 10 000). |
| writeBatchTimeout |Вставка данных в таблицу Azure при достижении writeBatchSize или writeBatchTimeout |Интервал времени<br/><br/>Пример: 00:20:00 (20 минут). |Нет (по умолчанию используется время ожидания, стандартное для клиента хранения — 90 секунд) |

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "AzureBlobtoTable",
            "description": "Copy Activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureBlobInput"
            }],
            "outputs": [{
                "name": "AzureTableOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "BlobSource"
                },
                "sink": {
                    "type": "AzureTableSink",
                    "writeBatchSize": 100,
                    "writeBatchTimeout": "01:00:00"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```
Дополнительные сведения об этих связанных службах см. в статье о [соединителе Хранилища таблиц Azure](data-factory-azure-table-connector.md#copy-activity-properties). 

## <a name="amazon-redshift"></a>Amazon Redshift

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы Amazon Redshift задайте **AmazonRedshift** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| server |IP-адрес или имя узла сервера Amazon Redshift. |Yes |
| порт |Номер TCP-порта, используемого сервером Amazon Redshift для прослушивания клиентских подключений. |Нет, значение по умолчанию — 5439. |
| database |Имя базы данных Amazon Redshift. |Yes |
| Имя пользователя |Имя пользователя, имеющего доступ к базе данных. |Yes |
| password |Пароль для учетной записи пользователя. |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "AmazonRedshiftLinkedService",
    "properties": {
        "type": "AmazonRedshift",
        "typeProperties": {
            "server": "<Amazon Redshift host name or IP address>",
            "port": 5439,
            "database": "<database name>",
            "username": "user",
            "password": "password"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Amazon Redshift](#data-factory-amazon-redshift-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных Amazon Redshift задайте **RelationalTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы в базе данных Amazon Redshift, на которое ссылается связанная служба. |Нет (если для свойства **RelationalSource** задано значение **query**). |


#### <a name="example"></a>Пример

```json
{
    "name": "AmazonRedshiftInputDataset",
    "properties": {
        "type": "RelationalTable",
        "linkedServiceName": "AmazonRedshiftLinkedService",
        "typeProperties": {
            "tableName": "<Table name>"
        },
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true
    }
}
```
Дополнительные сведения см. в статье о [соединителе Amazon Redshift](#data-factory-amazon-redshift-connector.md#dataset-properties).

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования 
При копировании данных из Amazon Redshift задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например, `select * from MyTable`. |Нет (если для свойства **tableName** задано значение **dataset**). |

#### <a name="example"></a>Пример

```json
{
    "name": "CopyAmazonRedshiftToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "$$Text.Format('select * from MyTable where timestamp >= \\'{0:yyyy-MM-ddTHH:mm:ss}\\' AND timestamp < \\'{1:yyyy-MM-ddTHH:mm:ss}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "BlobSink",
                    "writeBatchSize": 0,
                    "writeBatchTimeout": "00:00:00"
                }
            },
            "inputs": [{
                "name": "AmazonRedshiftInputDataset"
            }],
            "outputs": [{
                "name": "AzureBlobOutputDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "AmazonRedshiftToBlob"
        }],
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00"
    }
}
```
Дополнительные сведения см. в статье о [соединителе Amazon Redshift](#data-factory-amazon-redshift-connector.md#copy-activity-properties).

## <a name="ibm-db2"></a>IBM DB2

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы IBM DB2 задайте **OnPremisesDB2** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| server |Имя сервера DB2. |Yes |
| database |Имя базы данных DB2. |Yes |
| schema |Имя схемы в базе данных. Имя схемы чувствительно к регистру. |Нет  |
| authenticationType |Тип проверки подлинности, используемый для подключения к базе данных DB2. Возможными значениями являются: анонимная, обычная и Windows. |Yes |
| Имя пользователя |При использовании обычной проверки подлинности или проверки подлинности Windows укажите имя пользователя. |Нет  |
| password |Введите пароль для учетной записи пользователя, указанной для выбранного имени пользователя. |Нет  |
| gatewayName |Имя шлюза, который следует использовать службе фабрики данных для подключения к локальной базе данных DB2. |Yes |

#### <a name="example"></a>Пример
```json
{
    "name": "OnPremDb2LinkedService",
    "properties": {
        "type": "OnPremisesDb2",
        "typeProperties": {
            "server": "<server>",
            "database": "<database>",
            "schema": "<schema>",
            "authenticationType": "<authentication type>",
            "username": "<username>",
            "password": "<password>",
            "gatewayName": "<gatewayName>"
        }
    }
}
```
Дополнительные сведения см. в статье о [соединителе IBM DB2](#data-factory-onprem-db2-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных DB2 задайте **RelationalTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства:

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы в экземпляре базы данных DB2, на которое ссылается связанная служба. Свойство tableName чувствительно к регистру. |Нет (если для свойства **RelationalSource** задано значение **query**). 

#### <a name="example"></a>Пример
```json
{
    "name": "Db2DataSet",
    "properties": {
        "type": "RelationalTable",
        "linkedServiceName": "OnPremDb2LinkedService",
        "typeProperties": {},
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true,
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе IBM DB2](#data-factory-onprem-db2-connector.md#dataset-properties).

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из IBM DB2 задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:


| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например, `"query": "select * from "MySchema"."MyTable""`. |Нет (если для свойства **tableName** задано значение **dataset**). |

#### <a name="example"></a>Пример
```json
{
    "name": "CopyDb2ToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "select * from \"Orders\""
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "inputs": [{
                "name": "Db2DataSet"
            }],
            "outputs": [{
                "name": "AzureBlobDb2DataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "Db2ToBlob"
        }],
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00"
    }
}
```
Дополнительные сведения см. в статье о [соединителе IBM DB2](#data-factory-onprem-db2-connector.md#copy-activity-properties).

## <a name="mysql"></a>MySQL

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы MySQL задайте **OnPremisesMySql** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| server |Имя сервера MySQL. |Yes |
| database |Имя базы данных MySQL. |Yes |
| schema |Имя схемы в базе данных. |Нет  |
| authenticationType |Тип проверки подлинности, используемый для подключения к базе данных MySQL. Возможные значения: `Basic`. |Yes |
| Имя пользователя |Укажите имя пользователя для подключения к базе данных MySQL. |Yes |
| password |Введите пароль для указанной вами учетной записи пользователя. |Yes |
| gatewayName |Имя шлюза, который следует использовать службе фабрики данных для подключения к локальной базе данных MySQL. |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "OnPremMySqlLinkedService",
    "properties": {
        "type": "OnPremisesMySql",
        "typeProperties": {
            "server": "<server name>",
            "database": "<database name>",
            "schema": "<schema name>",
            "authenticationType": "<authentication type>",
            "userName": "<user name>",
            "password": "<password>",
            "gatewayName": "<gateway>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе MySQL](data-factory-onprem-mysql-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных MySQL задайте **RelationalTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы в экземпляре базы данных MySQL, на которое ссылается связанная служба. |Нет (если для свойства **RelationalSource** задано значение **query**). |

#### <a name="example"></a>Пример

```json
{
    "name": "MySqlDataSet",
    "properties": {
        "type": "RelationalTable",
        "linkedServiceName": "OnPremMySqlLinkedService",
        "typeProperties": {},
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true,
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```
Дополнительные сведения см. в статье о [соединителе MySQL](data-factory-onprem-mysql-connector.md#dataset-properties). 

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из базы данных MySQL задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:


| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например, `select * from MyTable`. |Нет (если для свойства **tableName** задано значение **dataset**). |


#### <a name="example"></a>Пример
```json
{
    "name": "CopyMySqlToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "$$Text.Format('select * from MyTable where timestamp >= \\'{0:yyyy-MM-ddTHH:mm:ss}\\' AND timestamp < \\'{1:yyyy-MM-ddTHH:mm:ss}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "BlobSink",
                    "writeBatchSize": 0,
                    "writeBatchTimeout": "00:00:00"
                }
            },
            "inputs": [{
                "name": "MySqlDataSet"
            }],
            "outputs": [{
                "name": "AzureBlobMySqlDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "MySqlToBlob"
        }],
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе MySQL](data-factory-onprem-mysql-connector.md#copy-activity-properties). 

## <a name="oracle"></a>Oracle 

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы Oracle задайте **OnPremisesOracle** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| driverType | Укажите, какой драйвер следует использовать для копирования данных в базу данных Oracle и из нее. Допустимые значения: **Майкрософт** или **ODP** (по умолчанию). Дополнительные сведения о драйверах см. в разделе [Поддерживаемые версии и установка](#supported-versions-and-installation). | Нет  |
| connectionString | В свойстве connectionString указываются сведения, необходимые для подключения к экземпляру базы данных Oracle. | Yes |
| gatewayName | Имя шлюза, который будет использоваться для подключения к локальному серверу Oracle. |Yes |

#### <a name="example"></a>Пример
```json
{
    "name": "OnPremisesOracleLinkedService",
    "properties": {
        "type": "OnPremisesOracle",
        "typeProperties": {
            "driverType": "Microsoft",
            "connectionString": "Host=<host>;Port=<port>;Sid=<sid>;User Id=<username>;Password=<password>;",
            "gatewayName": "<gateway name>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Oracle](data-factory-onprem-oracle-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных Oracle задайте **OracleTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы в базе данных Oracle, на которое ссылается связанная служба. |Нет (если указан параметр **oracleReaderQuery** объекта **OracleSource**) |

#### <a name="example"></a>Пример

```json
{
    "name": "OracleInput",
    "properties": {
        "type": "OracleTable",
        "linkedServiceName": "OnPremisesOracleLinkedService",
        "typeProperties": {
            "tableName": "MyTable"
        },
        "external": true,
        "availability": {
            "offset": "01:00:00",
            "interval": "1",
            "anchorDateTime": "2016-02-27T12:00:00",
            "frequency": "Hour"
        },
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```
Дополнительные сведения см. в статье о [соединителе Oracle](data-factory-onprem-oracle-connector.md#dataset-properties).

### <a name="oracle-source-in-copy-activity"></a>Источник Oracle в действии копирования
При копировании данных из базы данных Oracle задайте **OracleSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| oracleReaderQuery |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например: `select * from MyTable` <br/><br/>Если не указано, то выполняется инструкция SQL `select * from MyTable`. |Нет (если для свойства **tableName** задано значение **dataset**). |

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline for copy activity",
        "activities": [{
            "name": "OracletoBlob",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": " OracleInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "OracleSource",
                    "oracleReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
            "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе Oracle](data-factory-onprem-oracle-connector.md#copy-activity-properties).

### <a name="oracle-sink-in-copy-activity"></a>Приемник Oracle в действии копирования
При копировании данных в базу данных Oracle задайте **OracleSink** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| writeBatchTimeout |Время ожидания до выполнения операции пакетной вставки, пока не завершится срок ее действия. |Интервал времени<br/><br/> Пример: 00:30:00 (30 минут). |Нет  |
| writeBatchSize |Вставляет данные в таблицу SQL, когда размер буфера достигает значения writeBatchSize. |Целое число (количество строк) |Нет (значение по умолчанию — 100) |
| sqlWriterCleanupScript |Укажите запрос на выполнение действия копирования, позволяющий убедиться в том, что данные конкретного среза очищены. |Инструкция запроса. |Нет  |
| sliceIdentifierColumnName |Укажите имя столбца, в которое действие копирования добавляет автоматически созданный идентификатор среза. Этот идентификатор используется для очистки данных конкретного среза при повторном запуске. |Имя столбца с типом данных binary(32). |Нет  |

#### <a name="example"></a>Пример
```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-05T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "AzureBlobtoOracle",
            "description": "Copy Activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureBlobInput"
            }],
            "outputs": [{
                "name": "OracleOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "BlobSource"
                },
                "sink": {
                    "type": "OracleSink"
                }
            },
            "scheduler": {
                "frequency": "Day",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```
Дополнительные сведения см. в статье о [соединителе Oracle](data-factory-onprem-oracle-connector.md#copy-activity-properties).

## <a name="postgresql"></a>PostgreSQL

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы PostgreSQL задайте **OnPremisesPostgreSql** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| server |Имя сервера, PostgreSQL. |Yes |
| database |Имя базы данных PostgreSQL. |Yes |
| schema |Имя схемы в базе данных. Имя схемы чувствительно к регистру. |Нет  |
| authenticationType |Тип проверки подлинности, используемый для подключения к базе данных PostgreSQL. Возможными значениями являются: анонимная, обычная и Windows. |Yes |
| Имя пользователя |При использовании обычной проверки подлинности или проверки подлинности Windows укажите имя пользователя. |Нет  |
| password |Введите пароль для учетной записи пользователя, указанной для выбранного имени пользователя. |Нет  |
| gatewayName |Имя шлюза, который следует использовать службе фабрики данных для подключения к локальной базе данных PostgreSQL. |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "OnPremPostgreSqlLinkedService",
    "properties": {
        "type": "OnPremisesPostgreSql",
        "typeProperties": {
            "server": "<server>",
            "database": "<database>",
            "schema": "<schema>",
            "authenticationType": "<authentication type>",
            "username": "<username>",
            "password": "<password>",
            "gatewayName": "<gatewayName>"
        }
    }
}
```
Дополнительные сведения см. в статье о [соединителе PostgreSQL](data-factory-onprem-postgresql-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных PostgreSQL задайте **RelationalTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы в экземпляре базы данных PostgreSQL, на которое ссылается связанная служба. Свойство tableName чувствительно к регистру. |Нет (если для свойства **RelationalSource** задано значение **query**). |

#### <a name="example"></a>Пример
```json
{
    "name": "PostgreSqlDataSet",
    "properties": {
        "type": "RelationalTable",
        "linkedServiceName": "OnPremPostgreSqlLinkedService",
        "typeProperties": {},
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true,
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```
Дополнительные сведения см. в статье о [соединителе PostgreSQL](data-factory-onprem-postgresql-connector.md#dataset-properties).

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из базы данных PostgreSQL задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:


| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например, "query": "select * from \"MySchema\".\"MyTable\"". |Нет (если для свойства **tableName** задано значение **dataset**). |

#### <a name="example"></a>Пример

```json
{
    "name": "CopyPostgreSqlToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "select * from \"public\".\"usstates\""
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "inputs": [{
                "name": "PostgreSqlDataSet"
            }],
            "outputs": [{
                "name": "AzureBlobPostgreSqlDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "PostgreSqlToBlob"
        }],
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе PostgreSQL](data-factory-onprem-postgresql-connector.md#copy-activity-properties).

## <a name="sap-business-warehouse"></a>SAP Business Warehouse


### <a name="linked-service"></a>Связанные службы
Для определения связанной службы SAP Business Warehouse (BW) задайте **SapBw** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно
-------- | ----------- | -------------- | --------
server | Имя сервера, на котором размещен экземпляр SAP Business Warehouse. | строка | Yes
systemNumber | Номер системы SAP Business Warehouse. | Двузначное десятичное число, представленное в виде строки. | Yes
clientid | Идентификатор клиента в системе SAP Business Warehouse. | Трехзначное десятичное число, представленное в виде строки. | Yes
Имя пользователя | Имя пользователя, имеющего доступ к серверу SAP. | строка | Yes
password | Пароль для пользователя | строка | Yes
gatewayName | Имя шлюза, который следует использовать службе фабрики данных для подключения к локальному экземпляру SAP Business Warehouse. | строка | Yes
encryptedCredential | Строка зашифрованных учетных данных. | строка | Нет 

#### <a name="example"></a>Пример

```json
{
    "name": "SapBwLinkedService",
    "properties": {
        "type": "SapBw",
        "typeProperties": {
            "server": "<server name>",
            "systemNumber": "<system number>",
            "clientId": "<client id>",
            "username": "<SAP user>",
            "password": "<Password for SAP user>",
            "gatewayName": "<gateway name>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе SAP Business Warehouse ](data-factory-sap-business-warehouse-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных SAP BW задайте **RelationalTable** в качестве **типа** набора данных. Для набора данных SAP Business Warehouse типа **RelationalTable** не поддерживаются какие-либо свойства типа.  

#### <a name="example"></a>Пример

```json
{
    "name": "SapBwDataset",
    "properties": {
        "type": "RelationalTable",
        "linkedServiceName": "SapBwLinkedService",
        "typeProperties": {},
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true
    }
}
```
Дополнительные сведения см. в статье о [соединителе SAP Business Warehouse ](data-factory-sap-business-warehouse-connector.md#dataset-properties). 

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из SAP Business Warehouse задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:


| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query | Указывает запрос многомерных выражений для чтения данных из экземпляра SAP Business Warehouse. | Запрос многомерных выражений. | Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "CopySapBwToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "<MDX query for SAP BW>"
                },
                "sink": {
                    "type": "BlobSink",
                    "writeBatchSize": 0,
                    "writeBatchTimeout": "00:00:00"
                }
            },
            "inputs": [{
                "name": "SapBwDataset"
            }],
            "outputs": [{
                "name": "AzureBlobDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "SapBwToBlob"
        }],
        "start": "2017-03-01T18:00:00",
        "end": "2017-03-01T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе SAP Business Warehouse ](data-factory-sap-business-warehouse-connector.md#copy-activity-properties). 

## <a name="sap-hana"></a>SAP HANA

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы SAP HANA задайте **SapHana** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно
-------- | ----------- | -------------- | --------
server | Имя сервера, на котором размещен экземпляр SAP HANA. Если ваш сервер использует настроенный порт, укажите `server:port`. | строка | Yes
authenticationType | Тип проверки подлинности. | string. Basic или Windows. | Yes 
Имя пользователя | Имя пользователя, имеющего доступ к серверу SAP. | строка | Yes
password | Пароль для пользователя | строка | Yes
gatewayName | Имя шлюза, который следует использовать службе фабрики данных для подключения к локальному экземпляру SAP HANA. | строка | Yes
encryptedCredential | Строка зашифрованных учетных данных. | строка | Нет 

#### <a name="example"></a>Пример

```json
{
    "name": "SapHanaLinkedService",
    "properties": {
        "type": "SapHana",
        "typeProperties": {
            "server": "<server name>",
            "authenticationType": "<Basic, or Windows>",
            "username": "<SAP user>",
            "password": "<Password for SAP user>",
            "gatewayName": "<gateway name>"
        }
    }
}

```
Дополнительные сведения см. в статье о [соединителе SAP HANA](data-factory-sap-hana-connector.md#linked-service-properties).
 
### <a name="dataset"></a>Выборка
Для определения набора данных SAP HANA задайте **RelationalTable** в качестве **типа** набора данных. Для набора данных SAP HANA типа **RelationalTable** не поддерживаются какие-либо свойства типа. 

#### <a name="example"></a>Пример

```json
{
    "name": "SapHanaDataset",
    "properties": {
        "type": "RelationalTable",
        "linkedServiceName": "SapHanaLinkedService",
        "typeProperties": {},
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true
    }
}
```
Дополнительные сведения см. в статье о [соединителе SAP HANA](data-factory-sap-hana-connector.md#dataset-properties). 

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из хранилища данных SAP HANA задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query | Указывает SQL-запрос для чтения данных из экземпляра SAP HANA. | SQL-запрос. | Yes |


#### <a name="example"></a>Пример


```json
{
    "name": "CopySapHanaToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "<SQL Query for HANA>"
                },
                "sink": {
                    "type": "BlobSink",
                    "writeBatchSize": 0,
                    "writeBatchTimeout": "00:00:00"
                }
            },
            "inputs": [{
                "name": "SapHanaDataset"
            }],
            "outputs": [{
                "name": "AzureBlobDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "SapHanaToBlob"
        }],
        "start": "2017-03-01T18:00:00",
        "end": "2017-03-01T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе SAP HANA](data-factory-sap-hana-connector.md#copy-activity-properties).


## <a name="sql-server"></a>SQL Server;

### <a name="linked-service"></a>Связанные службы
Создайте связанную службу типа **OnPremisesSqlServer**, чтобы связать локальную базу данных SQL Server с фабрикой данных. В следующей таблице содержится описание элементов JSON, которые относятся к локальной связанной службе SQL Server.

В следующей таблице содержится описание элементов JSON, которые относятся к связанной службе SQL Server.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| Тип |Свойству type необходимо присвоить значение **OnPremisesSqlServer**. |Yes |
| connectionString |Укажите сведения о параметре connectionString, необходимые для подключения к локальной базе данных SQL Server с помощью проверки подлинности SQL или проверки подлинности Windows. |Yes |
| gatewayName |Имя шлюза, который службе фабрики данных следует использовать для подключения к локальной базе данных SQL Server. |Yes |
| Имя пользователя |При использовании проверки подлинности Windows укажите имя пользователя. Например, **domainname\\username**. |Нет  |
| password |Введите пароль для учетной записи пользователя, указанной для выбранного имени пользователя. |Нет  |

Вы можете зашифровать учетные данные с помощью командлета **New-AzureRmDataFactoryEncryptValue** и использовать их в строке подключения, как показано в следующем примере (свойство **EncryptedCredential**):  

```json
"connectionString": "Data Source=<servername>;Initial Catalog=<databasename>;Integrated Security=True;EncryptedCredential=<encrypted credential>",
```


#### <a name="example-json-for-using-sql-authentication"></a>Пример: JSON для использования проверки подлинности SQL

```json
{
    "name": "MyOnPremisesSQLDB",
    "properties": {
        "type": "OnPremisesSqlServer",
        "typeProperties": {
            "connectionString": "Data Source=<servername>;Initial Catalog=MarketingCampaigns;Integrated Security=False;User ID=<username>;Password=<password>;",
            "gatewayName": "<gateway name>"
        }
    }
}
```
#### <a name="example-json-for-using-windows-authentication"></a>Пример: JSON для использования проверки подлинности Windows

Если указаны имя пользователя и пароль, то шлюз использует их, чтобы действовать от имени соответствующей учетной записи пользователя для подключения к локальной базе данных SQL Server. В противном случае шлюз подключается к SQL Server напрямую с помощью контекста безопасности шлюза (его стартовая учетная запись).

```json
{
    "Name": " MyOnPremisesSQLDB",
    "Properties": {
        "type": "OnPremisesSqlServer",
        "typeProperties": {
            "ConnectionString": "Data Source=<servername>;Initial Catalog=MarketingCampaigns;Integrated Security=True;",
            "username": "<domain\\username>",
            "password": "<password>",
            "gatewayName": "<gateway name>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе SQL Server](data-factory-sqlserver-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных SQL Server задайте **SqlServerTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы или представления в экземпляре базы данных SQL Server, на который ссылается связанная служба. |Yes |

#### <a name="example"></a>Пример
```json
{
    "name": "SqlServerInput",
    "properties": {
        "type": "SqlServerTable",
        "linkedServiceName": "SqlServerLinkedService",
        "typeProperties": {
            "tableName": "MyTable"
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе SQL Server](data-factory-sqlserver-connector.md#dataset-properties). 

### <a name="sql-source-in-copy-activity"></a>Источник SQL в действии копирования
При копировании данных из базы данных SQL Server задайте **SqlSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:


| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| SqlReaderQuery |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например, `select * from MyTable`. Может ссылаться на несколько таблиц из базы данных, на которую ссылается входной набор данных. Если не указано другое, выполняется инструкция SQL select from MyTable. |Нет  |
| sqlReaderStoredProcedureName |Имя хранимой процедуры, которая считывает данные из исходной таблицы. |Имя хранимой процедуры. |Нет  |
| storedProcedureParameters |Параметры для хранимой процедуры. |Пары имен и значений. Имена и регистр параметров должны совпадать с именами и регистром параметров хранимой процедуры. |Нет  |

Если для SqlSource указано **sqlReaderQuery** , то действие копирования выполняет этот запрос для источника базы данных SQL Server с целью получения данных.

Кроме того, можно создать хранимую процедуру, указав **sqlReaderStoredProcedureName** и **storedProcedureParameters** (если хранимая процедура принимает параметры).

Если не указать sqlReaderQuery или sqlReaderStoredProcedureName, то для построения запроса select к базе данных SQL Server будут использованы столбцы, определенные в разделе структуры. Если у определения набора данных нет структуры, выбираются все столбцы из таблицы.

> [!NOTE]
> При использовании **sqlReaderStoredProcedureName** по-прежнему необходимо указать значение свойства **tableName** в наборе данных JSON. Хотя какие-либо проверки этой таблицы отсутствуют.


#### <a name="example"></a>Пример
```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline for copy activity",
        "activities": [{
            "name": "SqlServertoBlob",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": " SqlServerInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "SqlSource",
                    "SqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

В этом примере для свойства SqlSource указано **sqlReaderQuery** . Действие копирования выполняет этот запрос к источнику базы данных SQL Server с целью получения данных. Кроме того, можно создать хранимую процедуру, указав **sqlReaderStoredProcedureName** и **storedProcedureParameters** (если хранимая процедура принимает параметры). Свойство sqlReaderQuery может ссылаться на несколько таблиц из базы данных, на которую ссылается входной набор данных. Он не ограничивается таблицей, заданной свойством typeProperty в качестве параметра tableName набора данных.

Если не указать sqlReaderQuery или sqlReaderStoredProcedureName, то для построения запроса select к базе данных SQL Server будут использованы столбцы, определенные в разделе структуры. Если у определения набора данных нет структуры, выбираются все столбцы из таблицы.

Дополнительные сведения см. в статье о [соединителе SQL Server](data-factory-sqlserver-connector.md#copy-activity-properties). 

### <a name="sql-sink-in-copy-activity"></a>Приемник SQL в действии копирования
При копировании данных в базу данных SQL Server задайте **SqlSink** в качестве **типа приемника** для действия копирования и укажите в разделе **sink** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| writeBatchTimeout |Время ожидания до выполнения операции пакетной вставки, пока не завершится срок ее действия. |Интервал времени<br/><br/> Пример: 00:30:00 (30 минут). |Нет  |
| writeBatchSize |Вставляет данные в таблицу SQL, когда размер буфера достигает значения writeBatchSize. |Целое число (количество строк) |Нет (значение по умолчанию — 10 000). |
| sqlWriterCleanupScript |Укажите запрос на выполнение действия копирования, позволяющий убедиться в том, что данные конкретного среза очищены. Дополнительные сведения см. в разделе о [повторяемости](#repeatability-during-copy). |Инструкция запроса. |Нет  |
| sliceIdentifierColumnName |Укажите имя столбца, в которое действие копирования добавляет автоматически созданный идентификатор среза. Этот идентификатор используется для очистки данных конкретного среза при повторном запуске. Дополнительные сведения см. в разделе о [повторяемости](#repeatability-during-copy). |Имя столбца с типом данных binary(32). |Нет  |
| sqlWriterStoredProcedureName |Имя хранимой процедуры, обновляющей данные или вставляющей их в целевую таблицу. |Имя хранимой процедуры. |Нет  |
| storedProcedureParameters |Параметры для хранимой процедуры. |Пары имен и значений. Имена и регистр параметров должны совпадать с именами и регистром параметров хранимой процедуры. |Нет  |
| sqlWriterTableType |Укажите имя типа таблицы для использования в хранимой процедуре. Действие копирования делает перемещаемые данные доступными во временной таблице этого типа. Код хранимой процедуры затем можно использовать для объединения копируемых и существующих данных. |Имя типа таблицы. |Нет  |

#### <a name="example"></a>Пример
Конвейер содержит действие копирования, которое использует эти входной и выходной наборы данных и выполняется каждый час. В определении JSON конвейера для типа **source** установлено значение **BlobSource**, а для типа **sink** — значение **SqlSink**.

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "AzureBlobtoSQL",
            "description": "Copy Activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureBlobInput"
            }],
            "outputs": [{
                "name": " SqlServerOutput "
            }],
            "typeProperties": {
                "source": {
                    "type": "BlobSource",
                    "blobColumnSeparators": ","
                },
                "sink": {
                    "type": "SqlSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе SQL Server](data-factory-sqlserver-connector.md#copy-activity-properties). 

## <a name="sybase"></a>Sybase

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы Sybase задайте **OnPremisesSybase** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| server |Имя сервера Sybase. |Yes |
| database |Имя базы данных Sybase. |Yes |
| schema |Имя схемы в базе данных. |Нет  |
| authenticationType |Тип проверки подлинности, используемый для подключения к базе данных Sybase. Возможными значениями являются: анонимная, обычная и Windows. |Yes |
| Имя пользователя |При использовании обычной проверки подлинности или проверки подлинности Windows укажите имя пользователя. |Нет  |
| password |Введите пароль для учетной записи пользователя, указанной для выбранного имени пользователя. |Нет  |
| gatewayName |Имя шлюза, который следует использовать службе фабрики данных для подключения к локальной базе данных Sybase. |Yes |

#### <a name="example"></a>Пример
```json
{
    "name": "OnPremSybaseLinkedService",
    "properties": {
        "type": "OnPremisesSybase",
        "typeProperties": {
            "server": "<server>",
            "database": "<database>",
            "schema": "<schema>",
            "authenticationType": "<authentication type>",
            "username": "<username>",
            "password": "<password>",
            "gatewayName": "<gatewayName>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Sybase](data-factory-onprem-sybase-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных Sybase задайте **RelationalTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы в экземпляре базы данных Sybase, на которое ссылается связанная служба. |Нет (если для свойства **RelationalSource** задано значение **query**). |

#### <a name="example"></a>Пример

```json
{
    "name": "SybaseDataSet",
    "properties": {
        "type": "RelationalTable",
        "linkedServiceName": "OnPremSybaseLinkedService",
        "typeProperties": {},
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true,
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Sybase](data-factory-onprem-sybase-connector.md#dataset-properties). 

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из базы данных Sybase задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:


| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например, `select * from MyTable`. |Нет (если для свойства **tableName** задано значение **dataset**). |

#### <a name="example"></a>Пример

```json
{
    "name": "CopySybaseToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "select * from DBA.Orders"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "inputs": [{
                "name": "SybaseDataSet"
            }],
            "outputs": [{
                "name": "AzureBlobSybaseDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "SybaseToBlob"
        }],
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе Sybase](data-factory-onprem-sybase-connector.md#copy-activity-properties).

## <a name="teradata"></a>Teradata

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы Teradata задайте **OnPremisesTeradata** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| server |Имя сервера Teradata. |Yes |
| authenticationType |Тип проверки подлинности, используемый для подключения к базе данных Teradata. Возможными значениями являются: анонимная, обычная и Windows. |Yes |
| Имя пользователя |При использовании обычной проверки подлинности или проверки подлинности Windows укажите имя пользователя. |Нет  |
| password |Введите пароль для учетной записи пользователя, указанной для выбранного имени пользователя. |Нет  |
| gatewayName |Имя шлюза, который следует использовать службе фабрики данных для подключения к локальной базе данных Teradata. |Yes |

#### <a name="example"></a>Пример
```json
{
    "name": "OnPremTeradataLinkedService",
    "properties": {
        "type": "OnPremisesTeradata",
        "typeProperties": {
            "server": "<server>",
            "authenticationType": "<authentication type>",
            "username": "<username>",
            "password": "<password>",
            "gatewayName": "<gatewayName>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Teradata](data-factory-onprem-teradata-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных больших двоичных объектов Teradata задайте **RelationalTable** в качестве **типа** набора данных. Сейчас нет каких-либо свойств типа, поддерживаемых для набора данных Teradata. 

#### <a name="example"></a>Пример
```json
{
    "name": "TeradataDataSet",
    "properties": {
        "type": "RelationalTable",
        "linkedServiceName": "OnPremTeradataLinkedService",
        "typeProperties": {},
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true,
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Teradata](data-factory-onprem-teradata-connector.md#dataset-properties).

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из базы данных Teradata задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например, `select * from MyTable`. |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "CopyTeradataToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "$$Text.Format('select * from MyTable where timestamp >= \\'{0:yyyy-MM-ddTHH:mm:ss}\\' AND timestamp < \\'{1:yyyy-MM-ddTHH:mm:ss}\\'', SliceStart, SliceEnd)"
                },
                "sink": {
                    "type": "BlobSink",
                    "writeBatchSize": 0,
                    "writeBatchTimeout": "00:00:00"
                }
            },
            "inputs": [{
                "name": "TeradataDataSet"
            }],
            "outputs": [{
                "name": "AzureBlobTeradataDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "TeradataToBlob"
        }],
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "isPaused": false
    }
}
```

Дополнительные сведения см. в статье о [соединителе Teradata](data-factory-onprem-teradata-connector.md#copy-activity-properties).

## <a name="cassandra"></a>Cassandra


### <a name="linked-service"></a>Связанные службы
Для определения связанной службы Cassandra задайте **OnPremisesCassandra** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| host |Один или несколько IP-адресов или имен узлов серверов Cassandra.<br/><br/>Укажите через запятую список IP-адресов или имен узлов для одновременного подключения ко всем серверам. |Yes |
| порт |TCP-порт, используемый сервером Cassandra для прослушивания клиентских подключений |Нет. Значение по умолчанию — 9042 |
| authenticationType |Укажите тип Basic или Anonymous |Yes |
| Имя пользователя |Укажите имя пользователя для учетной записи пользователя |Да (если для свойства authenticationType задано значение Basic) |
| password |Укажите пароль для учетной записи пользователя. |Да (если для свойства authenticationType задано значение Basic) |
| gatewayName |Имя шлюза, который используется для подключения к локальной базе данных Cassandra. |Yes |
| encryptedCredential |Учетные данные, зашифрованные шлюзом |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "CassandraLinkedService",
    "properties": {
        "type": "OnPremisesCassandra",
        "typeProperties": {
            "authenticationType": "Basic",
            "host": "<cassandra server name or IP address>",
            "port": 9042,
            "username": "user",
            "password": "password",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Cassandra](data-factory-onprem-cassandra-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных Cassandra задайте **CassandraTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| keyspace |Имя пространства ключей или схемы в базе данных Cassandra |Да (если для **CassandraSource** не определено значение **query**). |
| tableName |Имя таблицы в базе данных Cassandra |Да (если для **CassandraSource** не определено значение **query**). |

#### <a name="example"></a>Пример

```json
{
    "name": "CassandraInput",
    "properties": {
        "linkedServiceName": "CassandraLinkedService",
        "type": "CassandraTable",
        "typeProperties": {
            "tableName": "mytable",
            "keySpace": "<key space>"
        },
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true,
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Cassandra](data-factory-onprem-cassandra-connector.md#dataset-properties). 

### <a name="cassandra-source-in-copy-activity"></a>Источник Cassandra в действии копирования
При копировании данных из Cassandra задайте **CassandraSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Запрос SQL-92 или CQL. Ознакомьтесь со [справочником по CQL](https://docs.datastax.com/en/cql/3.1/cql/cql_reference/cqlReferenceTOC.html). <br/><br/>Если используется SQL-запрос, то таблицу, к которой необходимо отправить запрос, укажите в формате **имя_пространства_ключей.имя_таблицы**. |Нет (если в наборе данных определены свойства tableName и keyspace) |
| consistencyLevel |Определяет количество реплик, которые должны ответить на запрос на чтение перед возвращением данных в клиентское приложение. Чтобы выполнить запрос на чтение, база данных Cassandra проверяет наличие указанного количества реплик для данных |ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE. Дополнительные сведения см. в статье [Configuring data consistency](http://docs.datastax.com/en//cassandra/2.0/cassandra/dml/dml_config_consistency_c.html) (Настройка согласованности данных). |Нет. Значение по умолчанию — ONE |

#### <a name="example"></a>Пример
  
```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "CassandraToAzureBlob",
            "description": "Copy from Cassandra to an Azure blob",
            "type": "Copy",
            "inputs": [{
                "name": "CassandraInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "CassandraSource",
                    "query": "select id, firstname, lastname from mykeyspace.mytable"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе Cassandra](data-factory-onprem-cassandra-connector.md#copy-activity-properties).

## <a name="mongodb"></a>MongoDB

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы MongoDB задайте **OnPremisesMongoDB** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| server |IP-адрес или имя узла сервера MongoDB |Yes |
| порт |TCP-порт, используемый сервером MongoDB для прослушивания клиентских подключений |Значение по умолчанию — 27017 (необязательно) |
| authenticationType |Укажите тип Basic или Anonymous |Yes |
| Имя пользователя |Учетная запись пользователя для доступа к MongoDB |Да (если используется обычная проверка подлинности) |
| password |Пароль для пользователя |Да (если используется обычная проверка подлинности) |
| authSource |Имя базы данных MongoDB, которое будет использоваться для проверки учетных данных при проверке подлинности |Необязательно (если используется обычная проверка подлинности). По умолчанию используется учетная запись администратора и база данных, указанная с помощью свойства databaseName |
| databaseName |Имя базы данных MongoDB, к которой необходимо получить доступ |Yes |
| gatewayName |Имя шлюза, который обращается к хранилищу данных |Yes |
| encryptedCredential |Учетные данные, зашифрованные шлюзом |Необязательно |

#### <a name="example"></a>Пример

```json
{
    "name": "OnPremisesMongoDbLinkedService",
    "properties": {
        "type": "OnPremisesMongoDb",
        "typeProperties": {
            "authenticationType": "<Basic or Anonymous>",
            "server": "< The IP address or host name of the MongoDB server >",
            "port": "<The number of the TCP port that the MongoDB server uses to listen for client connections.>",
            "username": "<username>",
            "password": "<password>",
            "authSource": "< The database that you want to use to check your credentials for authentication. >",
            "databaseName": "<database name>",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе MongoDB](data-factory-on-premises-mongodb-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных MongoDB задайте **Collection** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| collectionName |Имя коллекции в базе данных MongoDB |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "MongoDbInputDataset",
    "properties": {
        "type": "MongoDbCollection",
        "linkedServiceName": "OnPremisesMongoDbLinkedService",
        "typeProperties": {
            "collectionName": "<Collection name>"
        },
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true
    }
}
```

Дополнительные сведения см. в статье о [соединителе MongoDB](data-factory-on-premises-mongodb-connector.md#dataset-properties).

#### <a name="mongodb-source-in-copy-activity"></a>Источник MongoDB в действии копирования
При копировании данных из MongoDB задайте **MongoDbSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL-92. Например, `select * from MyTable`. |Нет (если для свойства **collectionName** задано значение **dataset**). |

#### <a name="example"></a>Пример

```json
{
    "name": "CopyMongoDBToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "MongoDbSource",
                    "query": "select * from MyTable"
                },
                "sink": {
                    "type": "BlobSink",
                    "writeBatchSize": 0,
                    "writeBatchTimeout": "00:00:00"
                }
            },
            "inputs": [{
                "name": "MongoDbInputDataset"
            }],
            "outputs": [{
                "name": "AzureBlobOutputDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "MongoDBToAzureBlob"
        }],
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе MongoDB](data-factory-on-premises-mongodb-connector.md#copy-activity-properties).

## <a name="amazon-s3"></a>Amazon S3


### <a name="linked-service"></a>Связанные службы
Для определения связанной службы S3 задайте **AwsAccessKey** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| accessKeyID |Идентификатор секретного ключа доступа. |строка |Yes |
| secretAccessKey |Сам секретный ключ доступа. |Зашифрованная строка секрета |Yes |

#### <a name="example"></a>Пример
```json
{
    "name": "AmazonS3LinkedService",
    "properties": {
        "type": "AwsAccessKey",
        "typeProperties": {
            "accessKeyId": "<access key id>",
            "secretAccessKey": "<secret access key>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Amazon S3](data-factory-amazon-simple-storage-service-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных Amazon S3 задайте **AmazonS3** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| bucketName |Имя контейнера S3. |Строка |Yes |
| key |Ключ объекта S3. |Строка |Нет  |
| prefix |Префикс для ключа объекта S3. Выбираются объекты, ключи которых начинаются с этого префикса. Применяется, только если ключ пустой. |Строка |Нет  |
| версия |Версия объекта S3, если включено управление версиями S3. |Строка |Нет  |
| свойства | Поддерживаются следующие типы формата: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat**, **ParquetFormat**. Свойству **type** в разделе format необходимо присвоить одно из этих значений. Дополнительные сведения см. в разделах о [текстовом формате](data-factory-supported-file-and-compression-formats.md#text-format), [формате Json](data-factory-supported-file-and-compression-formats.md#json-format), [формате Avro](data-factory-supported-file-and-compression-formats.md#avro-format), [формате Orc](data-factory-supported-file-and-compression-formats.md#orc-format) и [ формате Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format). <br><br> Если требуется скопировать файлы между файловыми хранилищами **как есть** (двоичное копирование), можно пропустить раздел форматирования в определениях входного и выходного наборов данных. |Нет  | |
| compression | Укажите тип и уровень сжатия данных. Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**. Поддерживаемые уровни: **Optimal** (Оптимальный) и **Fastest** (Самый быстрый). См. дополнительные сведения о [форматах файлов и сжатия данных в фабрике данных Azure](data-factory-supported-file-and-compression-formats.md#compression-support). |Нет  | |


> [!NOTE]
> Свойства bucketName и key указывают расположение объекта S3, где контейнер — это корневой контейнер для объектов S3, а ключ — это полный путь к объекту S3.

#### <a name="example-sample-dataset-with-prefix"></a>Пример: пример набора данных с префиксом

```json
{
    "name": "dataset-s3",
    "properties": {
        "type": "AmazonS3",
        "linkedServiceName": "link- testS3",
        "typeProperties": {
            "prefix": "testFolder/test",
            "bucketName": "<S3 bucket name>",
            "format": {
                "type": "OrcFormat"
            }
        },
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true
    }
}
```
#### <a name="example-sample-data-set-with-version"></a>Пример: пример набора данных (с версией)

```json
{
    "name": "dataset-s3",
    "properties": {
        "type": "AmazonS3",
        "linkedServiceName": "link- testS3",
        "typeProperties": {
            "key": "testFolder/test.orc",
            "bucketName": "<S3 bucket name>",
            "version": "XXXXXXXXXczm0CJajYkHf0_k6LhBmkcL",
            "format": {
                "type": "OrcFormat"
            }
        },
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true
    }
}
```

#### <a name="example-dynamic-paths-for-s3"></a>Пример: динамические пути для S3
В этом примере мы используем фиксированные значения для свойств key и bucketName в наборе данных Amazon S3.

```json
"key": "testFolder/test.orc",
"bucketName": "<S3 bucket name>",
```

Можно настроить фабрику данных для динамического вычисления значений свойств key и bucketName во время выполнения с помощью системных переменных, например SliceStart.

```json
"key": "$$Text.Format('{0:MM}/{0:dd}/test.orc', SliceStart)"
"bucketName": "$$Text.Format('{0:yyyy}', SliceStart)"
```

То же самое можно сделать и для свойства prefix набора данных Amazon S3. В статье [Фабрика данных Azure — функции и системные переменные](data-factory-functions-variables.md) приведен список поддерживаемых функций и переменных.

Дополнительные сведения см. в статье о [соединителе Amazon S3](data-factory-amazon-simple-storage-service-connector.md#dataset-properties).

### <a name="file-system-source-in-copy-activity"></a>Источник файловой системы в действии копирования
При копировании данных из Amazon S3 задайте **FileSystemSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:


| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| recursive |Указывает, следует ли отображать рекурсивный список объектов S3 в каталоге. |True или false |Нет  |


#### <a name="example"></a>Пример


```json
{
    "name": "CopyAmazonS3ToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "FileSystemSource",
                    "recursive": true
                },
                "sink": {
                    "type": "BlobSink",
                    "writeBatchSize": 0,
                    "writeBatchTimeout": "00:00:00"
                }
            },
            "inputs": [{
                "name": "AmazonS3InputDataset"
            }],
            "outputs": [{
                "name": "AzureBlobOutputDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "AmazonS3ToBlob"
        }],
        "start": "2016-08-08T18:00:00",
        "end": "2016-08-08T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе Amazon S3](data-factory-amazon-simple-storage-service-connector.md#copy-activity-properties).

## <a name="file-system"></a>Файловая система


### <a name="linked-service"></a>Связанные службы
Вы можете связать локальную файловую систему с фабрикой данных Azure при помощи связанной службы **локального файлового сервера**. В таблице ниже приведены описания элементов JSON, которые относятся к связанной службе локального файлового сервера.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| Тип |Убедитесь, что свойству type присвоено значение **OnPremisesFileServer**. |Yes |
| host |Задает корневой путь к папке, которую необходимо скопировать. Чтобы указать специальные знаки в строке, используйте escape-символ "\". Примеры приведены в разделе [Примеры определений связанной службы и набора данных](#sample-linked-service-and-dataset-definitions). |Yes |
| userid |Укажите идентификатор пользователя, имеющего доступ к серверу. |Нет (если выбрать encryptedcredential) |
| password |Укажите пароль для пользователя (userid). |Нет (если выбрать encryptedcredential) |
| encryptedCredential |Укажите зашифрованные учетные данные. Чтобы их получить, выполните командлет New-AzureRmDataFactoryEncryptValue. |Нет (если имя пользователя и пароль указываются в виде обычного текста) |
| gatewayName |Задает имя шлюза, который следует использовать фабрике данных для подключения к локальному файловому серверу. |Yes |

#### <a name="sample-folder-path-definitions"></a>Пример определений пути к папке 
| Сценарий | Размещение в определении связанной службы | Путь к файлу в определении набора данных |
| --- | --- | --- |
| Локальная папка на компьютере со шлюзом управления данными. <br/><br/>Примеры: "D:\\\\*" или "D:\папка\вложенная_папка\\\*" |"D:\\\\" (для шлюза управления данными 2.0 и более поздних версий) <br/><br/> localhost (для версий, предшествующих версии 2.0 шлюза управления данными) |.\\\\ или "папка\\\\вложенная_папка" (для шлюза управления данными 2.0 и более поздних версий) <br/><br/>"D:\\\\" или "D:\\\\папка\\\\вложенная_папка" (для шлюза версии ниже 2.0) |
| Удаленная общая папка: <br/><br/>Примеры: "\\\\сервер\\общая_папка\\\*" или "\\\\сервер\\общая_папка\\папка\\вложенная_папка\\*" |\\\\\\\\сервер\\\\общая_папка |.\\\\ или "папка\\\\вложенная_папка" |


#### <a name="example-using-username-and-password-in-plain-text"></a>Пример указания имени пользователя и пароля в виде обычного текста

```json
{
    "Name": "OnPremisesFileServerLinkedService",
    "properties": {
        "type": "OnPremisesFileServer",
        "typeProperties": {
            "host": "\\\\Contosogame-Asia",
            "userid": "Admin",
            "password": "123456",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

#### <a name="example-using-encryptedcredential"></a>Пример использования encryptedcredential

```json
{
    "Name": " OnPremisesFileServerLinkedService ",
    "properties": {
        "type": "OnPremisesFileServer",
        "typeProperties": {
            "host": "D:\\",
            "encryptedCredential": "WFuIGlzIGRpc3Rpbmd1aXNoZWQsIG5vdCBvbmx5IGJ5xxxxxxxxxxxxxxxxx",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе файловой системы](data-factory-onprem-file-system-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных файловой системы задайте **FileShare** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| folderPath |Указывает вложенный путь к папке. Чтобы указать специальные знаки в строке, используйте escape-символ "\". Примеры приведены в разделе [Примеры определений связанной службы и набора данных](#sample-linked-service-and-dataset-definitions).<br/><br/>Вы можете использовать это свойство вместе с параметром **partitionBy**, чтобы в путях к папкам учитывались дата и время начала и окончания среза. |Yes |
| fileName |Укажите имя файла в папке **folderPath** , если таблица должна ссылаться на определенный файл в папке. Если этому свойству не присвоить значение, таблица будет указывать на все файлы в папке.<br/><br/>Если свойство fileName не указано для выходного набора данных, то имя созданного файла будет иметь следующий формат: <br/><br/>`Data.<Guid>.txt` (пример: Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt) |Нет  |
| fileFilter |Укажите фильтр для выбора подмножества файлов из folderPath. Фильтр дает возможность выбирать только некоторые файлы, а не все. <br/><br/>Допустимые значения: `*` (несколько знаков) и `?` (один знак).<br/><br/>Пример 1: "fileFilter": "*.log"<br/>Пример 2: fileFilter: 2016-1-?.txt<br/><br/>Обратите внимание, что свойство fileFilter применяется к входному набору данных FileShare. |Нет  |
| partitionedBy |С помощью свойства partitionedBy можно указать динамические значения folderPath и fileName для данных временного ряда. Например, можно параметризовать значение folderPath для каждого часа получения данных. |Нет  |
| свойства | Поддерживаются следующие типы формата: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat**, **ParquetFormat**. Свойству **type** в разделе format необходимо присвоить одно из этих значений. Дополнительные сведения см. в разделах о [текстовом формате](data-factory-supported-file-and-compression-formats.md#text-format), [формате Json](data-factory-supported-file-and-compression-formats.md#json-format), [формате Avro](data-factory-supported-file-and-compression-formats.md#avro-format), [формате Orc](data-factory-supported-file-and-compression-formats.md#orc-format) и [ формате Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format). <br><br> Если требуется скопировать файлы между файловыми хранилищами **как есть** (двоичное копирование), можно пропустить раздел форматирования в определениях входного и выходного наборов данных. |Нет  |
| compression | Укажите тип и уровень сжатия данных. Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**. Поддерживаемые уровни: **Optimal** и **Fastest**. См. сведения о [форматах файлов и сжатия данных в фабрике данных Azure](data-factory-supported-file-and-compression-formats.md#compression-support). |Нет  |

> [!NOTE]
> Свойства filename и fileFilter невозможно использовать одновременно.

#### <a name="example"></a>Пример

```json
{
    "name": "OnpremisesFileSystemInput",
    "properties": {
        "type": " FileShare",
        "linkedServiceName": " OnPremisesFileServerLinkedService ",
        "typeProperties": {
            "folderPath": "mysharedfolder/yearno={Year}/monthno={Month}/dayno={Day}",
            "fileName": "{Hour}.csv",
            "partitionedBy": [{
                "name": "Year",
                "value": {
                    "type": "DateTime",
                    "date": "SliceStart",
                        "format": "yyyy"
                }
            }, {
                "name": "Month",
                "value": {
                    "type": "DateTime",
                    "date": "SliceStart",
                    "format": "MM"
                }
            }, {
                "name": "Day",
                "value": {
                    "type": "DateTime",
                    "date": "SliceStart",
                    "format": "dd"
                }
            }, {
                "name": "Hour",
                "value": {
                    "type": "DateTime",
                    "date": "SliceStart",
                    "format": "HH"
                }
            }]
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе файловой системы](data-factory-onprem-file-system-connector.md#dataset-properties).

### <a name="file-system-source-in-copy-activity"></a>Источник файловой системы в действии копирования
При копировании данных из файловой системы задайте **FileSystemSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| recursive |Указывает, следует ли читать данные рекурсивно из вложенных папок или только из указанной папки. |True, False (по умолчанию) |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2015-06-01T18:00:00",
        "end": "2015-06-01T19:00:00",
        "description": "Pipeline for copy activity",
        "activities": [{
            "name": "OnpremisesFileSystemtoBlob",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": "OnpremisesFileSystemInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "FileSystemSource"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
            "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```
Дополнительные сведения см. в статье о [соединителе файловой системы](data-factory-onprem-file-system-connector.md#copy-activity-properties).

### <a name="file-system-sink-in-copy-activity"></a>Приемник файловой системы в действии копирования
При копировании данных в файловую систему задайте **FileSystemSink** в качестве **типа приемника** для действия копирования и задайте в разделе **sink** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| copyBehavior |Это свойство определяет поведение функции копирования, когда в качестве источника используется BlobSource или FileSystem. |/ **PreserveHierarchy:** сохраняет иерархию файлов в целевой папке. То есть относительный путь исходного файла в исходной папке идентичен относительному пути целевого файла в целевой папке.<br/><br/>**FlattenHierarchy**: все файлы из исходной папки создаются на первом уровне в целевой папке. Целевые файлы создаются с автоматически сформированными именами.<br/><br/>**MergeFiles**: объединяет все файлы из исходной папки в один файл. Если указано имя большого двоичного объекта или имя файла, то оно присваивается объединенному файлу. В противном случае присваивается автоматически созданное имя файла. |Нет  |
auto-

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2015-06-01T18:00:00",
        "end": "2015-06-01T20:00:00",
        "description": "pipeline for copy activity",
        "activities": [{
            "name": "AzureSQLtoOnPremisesFile",
            "description": "copy activity",
            "type": "Copy",
            "inputs": [{
                "name": "AzureSQLInput"
            }],
            "outputs": [{
                "name": "OnpremisesFileSystemOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "SqlSource",
                    "SqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "FileSystemSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 3,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе файловой системы](data-factory-onprem-file-system-connector.md#copy-activity-properties).

## <a name="ftp"></a>FTP

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы FTP задайте **FtpServer** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно | значение по умолчанию |
| --- | --- | --- | --- |
| host |Имя или IP-адрес FTP-сервера |Yes |&nbsp; |
| authenticationType |Укажите тип аутентификации |Yes |Обычная, анонимная |
| Имя пользователя |Пользователь, имеющий доступ к FTP-серверу |Нет  |&nbsp; |
| password |Пароль для пользователя (имя пользователя) |Нет  |&nbsp; |
| encryptedCredential |Зашифрованные учетные данные для доступа к FTP-серверу |Нет  |&nbsp; |
| gatewayName |Имя шлюза управления данными для подключения к локальному FTP-серверу |Нет  |&nbsp; |
| порт |Порт, прослушиваемый FTP-сервером |Нет  |21 |
| enableSsl |Укажите, какой канал следует использовать (FTP через SSL или TLS) |Нет  |Да |
| enableServerCertificateValidation |Укажите, следует ли включать проверку SSL-сертификата на сервере при использовании канала FTP через SSL или TLS |Нет  |Да |

#### <a name="example-using-anonymous-authentication"></a>Пример: использование анонимной проверки подлинности

```json
{
    "name": "FTPLinkedService",
    "properties": {
        "type": "FtpServer",
            "typeProperties": {
            "authenticationType": "Anonymous",
            "host": "myftpserver.com"
        }
    }
}
```

#### <a name="example-using-username-and-password-in-plain-text-for-basic-authentication"></a>Пример: использование имени пользователя и пароля в виде обычного текста для обычной проверки подлинности

```json
{
    "name": "FTPLinkedService",
    "properties": {
        "type": "FtpServer",
        "typeProperties": {
            "host": "myftpserver.com",
            "authenticationType": "Basic",
            "username": "Admin",
            "password": "123456"
        }
    }
}
```

#### <a name="example-using-port-enablessl-enableservercertificatevalidation"></a>Пример: использование свойств port, enableSsl, enableServerCertificateValidation

```json
{
    "name": "FTPLinkedService",
    "properties": {
        "type": "FtpServer",
        "typeProperties": {
            "host": "myftpserver.com",
            "authenticationType": "Basic",    
            "username": "Admin",
            "password": "123456",
            "port": "21",
            "enableSsl": true,
            "enableServerCertificateValidation": true
        }
    }
}
```

#### <a name="example-using-encryptedcredential-for-authentication-and-gateway"></a>Пример: использование свойства encryptedCredential для проверки подлинности и шлюза

```json
{
    "name": "FTPLinkedService",
    "properties": {
        "type": "FtpServer",
        "typeProperties": {
            "host": "myftpserver.com",
            "authenticationType": "Basic",
            "encryptedCredential": "xxxxxxxxxxxxxxxxx",
            "gatewayName": "<onpremgateway>"
        }
      }
}
```

Дополнительные сведения см. в статье о [соединителе FTP](data-factory-ftp-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных FTP задайте **FileShare** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| folderPath |Подпуть к папке. Чтобы указать специальные знаки в строке, используйте escape-знак "\". Примеры приведены в разделе [Примеры определений связанной службы и набора данных](#sample-linked-service-and-dataset-definitions).<br/><br/>Вы можете использовать это свойство вместе с параметром **partitionBy**, чтобы в путях к папкам учитывались дата и время начала и окончания среза. |Yes 
| fileName |Укажите имя файла в папке **folderPath** , если таблица должна ссылаться на определенный файл в папке. Если этому свойству не присвоить значение, таблица будет указывать на все файлы в папке.<br/><br/>Если свойство fileName не указано для выходного набора данных, то имя созданного файла будет иметь следующий формат: <br/><br/>Data.<Guid>.txt (например, Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt |Нет  |
| fileFilter |Укажите фильтр для выбора подмножества файлов из folderPath. Фильтр дает возможность выбирать только некоторые файлы, а не все.<br/><br/>Допустимые значения: `*` (несколько знаков) и `?` (один знак).<br/><br/>Пример 1: `"fileFilter": "*.log"`<br/>Пример 2: `"fileFilter": 2016-1-?.txt"`<br/><br/> Свойство fileFilter применяется к входному набору данных FileShare. HDFS не поддерживает это свойство. |Нет  |
| partitionedBy |Чтобы указать для временного ряда данных динамические путь к папке и имя файла, используйте свойство partitionedBy. Например, путь к папке folderPath каждый час будет другим. |Нет  |
| свойства | Поддерживаются следующие типы формата: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat**, **ParquetFormat**. Свойству **type** в разделе format необходимо присвоить одно из этих значений. Дополнительные сведения см. в разделах о [текстовом формате](data-factory-supported-file-and-compression-formats.md#text-format), [формате Json](data-factory-supported-file-and-compression-formats.md#json-format), [формате Avro](data-factory-supported-file-and-compression-formats.md#avro-format), [формате Orc](data-factory-supported-file-and-compression-formats.md#orc-format) и [ формате Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format). <br><br> Если требуется скопировать файлы между файловыми хранилищами **как есть** (двоичное копирование), можно пропустить раздел форматирования в определениях входного и выходного наборов данных. |Нет  |
| compression | Укажите тип и уровень сжатия данных. Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**. Поддерживаемые уровни: **Optimal** и **Fastest**. Узнайте больше о [форматах файлов и сжатия данных в фабрике данных Azure](data-factory-supported-file-and-compression-formats.md#compression-support). |Нет  |
| useBinaryTransfer |Укажите, использовать ли режим передачи в двоичном формате. Значение true, если использовать двоичный режим, и false, если ASCII. Значение по умолчанию: True. Это свойство можно использовать, только когда тип связанной службы является FTP-сервер. |Нет  |

> [!NOTE]
> Свойства filename и fileFilter нельзя использовать одновременно.

#### <a name="example"></a>Пример

```json
{
    "name": "FTPFileInput",
    "properties": {
        "type": "FileShare",
        "linkedServiceName": "FTPLinkedService",
        "typeProperties": {
            "folderPath": "<path to shared folder>",
            "fileName": "test.csv",
            "useBinaryTransfer": true
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе FTP](data-factory-ftp-connector.md#dataset-properties).

### <a name="file-system-source-in-copy-activity"></a>Источник файловой системы в действии копирования
При копировании данных из FTP задайте **FileSystemSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| recursive |Указывает, следует ли читать данные рекурсивно из вложенных папок или только из указанной папки. |True, False (по умолчанию) |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "pipeline",
    "properties": {
        "activities": [{
            "name": "FTPToBlobCopy",
            "inputs": [{
                "name": "FtpFileInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "FileSystemSource"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "NewestFirst",
                "retry": 1,
                "timeout": "00:05:00"
            }
        }],
        "start": "2016-08-24T18:00:00",
        "end": "2016-08-24T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе FTP](data-factory-ftp-connector.md#copy-activity-properties).


## <a name="hdfs"></a>HDFS

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы HDFS задайте **Hdfs** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| Тип |Для свойства type необходимо задать значение **Hdfs** |Yes |
| URL-адрес |URL-адрес в HDFS |Yes |
| authenticationType |Anonymous или Windows. <br><br> Чтобы использовать **аутентификацию Kerberos** для соединителя HDFS, настройте локальную среду, как описано [здесь](#use-kerberos-authentication-for-hdfs-connector). |Yes |
| userName |Имя пользователя для проверки подлинности Windows. |Да (для проверки подлинности Windows) |
| password |Пароль для проверки подлинности Windows. |Да (для проверки подлинности Windows) |
| gatewayName |Имя шлюза, который следует использовать службе фабрики данных для подключения к локальной системе HDFS. |Yes |
| encryptedCredential |[New-AzureRMDataFactoryEncryptValue](https://msdn.microsoft.com/library/mt603802.aspx) выводит учетные данные доступа. |Нет  |

#### <a name="example-using-anonymous-authentication"></a>Пример: использование анонимной проверки подлинности

```json
{
    "name": "HDFSLinkedService",
    "properties": {
        "type": "Hdfs",
        "typeProperties": {
            "authenticationType": "Anonymous",
            "userName": "hadoop",
            "url": "http://<machine>:50070/webhdfs/v1/",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

#### <a name="example-using-windows-authentication"></a>Пример: использование проверки подлинности Windows

```json
{
    "name": "HDFSLinkedService",
    "properties": {
        "type": "Hdfs",
        "typeProperties": {
            "authenticationType": "Windows",
            "userName": "Administrator",
            "password": "password",
            "url": "http://<machine>:50070/webhdfs/v1/",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе HDFS](#data-factory-hdfs-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных HDFS задайте **FileShare** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| folderPath |Путь к папке, Пример: `myfolder`<br/><br/>Чтобы указать специальные знаки в строке, используйте escape-знак "\". Например, для "папка\вложенная_папка" укажите "папка\\\\вложенная_папка", а для "d:\пример_папки" укажите "d:\\\\пример_папки".<br/><br/>Вы можете использовать это свойство вместе с параметром **partitionBy**, чтобы в путях к папкам учитывались дата и время начала и окончания среза. |Yes |
| fileName |Укажите имя файла в папке **folderPath** , если таблица должна ссылаться на определенный файл в папке. Если этому свойству не присвоить значение, таблица будет указывать на все файлы в папке.<br/><br/>Если свойство fileName не указано для выходного набора данных, то имя созданного файла будет иметь следующий формат: <br/><br/>Data<Guid>.txt (например, Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.). |Нет  |
| partitionedBy |Чтобы указать для временного ряда данных динамические путь к папке и имя файла, используйте свойство partitionedBy. Например, путь к папке (folderPath) каждый час будет другим. |Нет  |
| свойства | Поддерживаются следующие типы формата: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat**, **ParquetFormat**. Свойству **type** в разделе format необходимо присвоить одно из этих значений. Дополнительные сведения см. в разделах о [текстовом формате](data-factory-supported-file-and-compression-formats.md#text-format), [формате Json](data-factory-supported-file-and-compression-formats.md#json-format), [формате Avro](data-factory-supported-file-and-compression-formats.md#avro-format), [формате Orc](data-factory-supported-file-and-compression-formats.md#orc-format) и [ формате Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format). <br><br> Если требуется скопировать файлы между файловыми хранилищами **как есть** (двоичное копирование), можно пропустить раздел форматирования в определениях входного и выходного наборов данных. |Нет  |
| compression | Укажите тип и уровень сжатия данных. Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**. Поддерживаемые уровни: **Optimal** и **Fastest**. Узнайте больше о [форматах файлов и сжатия данных в фабрике данных Azure](data-factory-supported-file-and-compression-formats.md#compression-support). |Нет  |

> [!NOTE]
> Свойства filename и fileFilter нельзя использовать одновременно.

#### <a name="example"></a>Пример

```json
{
    "name": "InputDataset",
    "properties": {
        "type": "FileShare",
        "linkedServiceName": "HDFSLinkedService",
        "typeProperties": {
            "folderPath": "DataTransfer/UnitTest/"
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе HDFS](#data-factory-hdfs-connector.md#dataset-properties). 

### <a name="file-system-source-in-copy-activity"></a>Источник файловой системы в действии копирования
При копировании данных из HDFS задайте **FileSystemSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

**FileSystemSource** поддерживает следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| recursive |Указывает, следует ли читать данные рекурсивно из вложенных папок или только из указанной папки. |True, False (по умолчанию) |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "pipeline",
    "properties": {
        "activities": [{
            "name": "HdfsToBlobCopy",
            "inputs": [{
                "name": "InputDataset"
            }],
            "outputs": [{
                "name": "OutputDataset"
            }],
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "FileSystemSource"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "NewestFirst",
                "retry": 1,
                "timeout": "00:05:00"
            }
        }],
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе HDFS](#data-factory-hdfs-connector.md#copy-activity-properties).

## <a name="sftp"></a>SFTP


### <a name="linked-service"></a>Связанные службы
Для определения связанной службы SFTP задайте **Sftp** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- | --- |
| host | Имя или IP-адрес SFTP-сервера. |Yes |
| порт |Порт, прослушиваемый SFTP-сервером. По умолчанию используется значение 21. |Нет  |
| authenticationType |Укажите тип аутентификации. Допустимые значения: **Basic**, **SshPublicKey**. <br><br> Описание свойств и примеры JSON для каждого типа см. ниже в разделах [использование обычной аутентификации](#using-basic-authentication) и [использование аутентификации с открытым ключом SSH](#using-ssh-public-key-authentication) соответственно. |Yes |
| skipHostKeyValidation | Указывает, нужно ли пропустить проверку ключа узла. | Нет. По умолчанию имеет значение False. |
| hostKeyFingerprint | Содержит отпечаток ключа узла. | Да, если для `skipHostKeyValidation` указано значение False.  |
| gatewayName |Имя шлюза управления данными для подключения к локальному SFTP-серверу. | Да, если копирование выполняется с локального SFTP-сервера. |
| encryptedCredential | Зашифрованные учетные данные для доступа к SFTP-серверу. Генерируются автоматически, когда вы выбираете обычную аутентификацию (имя пользователя и пароль) или аутентификацию SshPublicKey (имя пользователя и путь к закрытому ключу или содержимое ключа) в мастере копирования или во всплывающем диалоговом окне ClickOnce. | Нет. Применимо, только если копирование выполняется с локального SFTP-сервера. |

#### <a name="example-using-basic-authentication"></a>Пример: использование обычной проверки подлинности

Чтобы использовать обычную аутентификацию, задайте для `authenticationType` значение `Basic`, а также все следующие свойства в дополнение к универсальным свойствам соединителя SFTP, которые описаны в последнем разделе.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- | --- |
| Имя пользователя | Пользователь, имеющий доступ к SFTP-серверу. |Yes |
| password | Пароль пользователя, указанного в свойстве имя пользователя. | Yes |

```json
{
    "name": "SftpLinkedService",
    "properties": {
        "type": "Sftp",
        "typeProperties": {
            "host": "<SFTP server name or IP address>",
            "port": 22,
            "authenticationType": "Basic",
            "username": "xxx",
            "password": "xxx",
            "skipHostKeyValidation": false,
            "hostKeyFingerPrint": "ssh-rsa 2048 xx:00:00:00:xx:00:x0:0x:0x:0x:0x:00:00:x0:x0:00",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

#### <a name="example-basic-authentication-with-encrypted-credential"></a>Пример: обычная проверка подлинности с шифрованием учетных данных**

```json
{
    "name": "SftpLinkedService",
    "properties": {
        "type": "Sftp",
        "typeProperties": {
            "host": "<FTP server name or IP address>",
            "port": 22,
            "authenticationType": "Basic",
            "username": "xxx",
            "encryptedCredential": "xxxxxxxxxxxxxxxxx",
            "skipHostKeyValidation": false,
            "hostKeyFingerPrint": "ssh-rsa 2048 xx:00:00:00:xx:00:x0:0x:0x:0x:0x:00:00:x0:x0:00",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

#### <a name="using-ssh-public-key-authentication"></a>Пример: использование проверки подлинности с открытым ключом SSH**

Чтобы использовать обычную аутентификацию, задайте для `authenticationType` значение `SshPublicKey`, а также все следующие свойства в дополнение к универсальным свойствам соединителя SFTP, которые описаны в последнем разделе.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- | --- |
| Имя пользователя |Пользователь, имеющий доступ к SFTP-серверу. |Yes |
| privateKeyPath | Укажите доступный для шлюза абсолютный путь к файлу закрытого ключа. | Должно быть указано одно из свойств: `privateKeyPath` или `privateKeyContent`. <br><br> Применимо, только если копирование выполняется с локального SFTP-сервера. |
| privateKeyContent | Сериализованная строка с содержимым закрытого ключа. Мастер копирования может автоматически считать файл закрытого ключа и извлечь его содержимое. Если вы используете другие средства и (или) пакет SDK, лучше использовать свойство privateKeyPath. | Должно быть указано одно из свойств: `privateKeyPath` или `privateKeyContent`. |
| passPhrase | Укажите пароль или парольную фразу для расшифровки закрытого ключа, если они используются для защиты файлы ключа. | Да, если файл закрытого ключа защищен парольной фразой. |

```json
{
    "name": "SftpLinkedServiceWithPrivateKeyPath",
    "properties": {
        "type": "Sftp",
        "typeProperties": {
            "host": "<FTP server name or IP address>",
            "port": 22,
            "authenticationType": "SshPublicKey",
            "username": "xxx",
            "privateKeyPath": "D:\\privatekey_openssh",
            "passPhrase": "xxx",
            "skipHostKeyValidation": true,
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

#### <a name="example-sshpublickey-authentication-using-private-key-content"></a>Пример: проверка подлинности с закрытым ключом SSH, для которого указано содержимое**

```json
{
    "name": "SftpLinkedServiceWithPrivateKeyContent",
    "properties": {
        "type": "Sftp",
        "typeProperties": {
            "host": "mysftpserver.westus.cloudapp.azure.com",
            "port": 22,
            "authenticationType": "SshPublicKey",
            "username": "xxx",
            "privateKeyContent": "<base64 string of the private key content>",
            "passPhrase": "xxx",
            "skipHostKeyValidation": true
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе SFTP](data-factory-sftp-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных SFTP задайте **FileShare** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| folderPath |Подпуть к папке. Чтобы указать специальные знаки в строке, используйте escape-знак "\". Примеры приведены в разделе [Примеры определений связанной службы и набора данных](#sample-linked-service-and-dataset-definitions).<br/><br/>Вы можете использовать это свойство вместе с параметром **partitionBy**, чтобы в путях к папкам учитывались дата и время начала и окончания среза. |Yes |
| fileName |Укажите имя файла в папке **folderPath** , если таблица должна ссылаться на определенный файл в папке. Если этому свойству не присвоить значение, таблица будет указывать на все файлы в папке.<br/><br/>Если свойство fileName не указано для выходного набора данных, то имя созданного файла будет иметь следующий формат: <br/><br/>Data.<Guid>.txt (например, Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt |Нет  |
| fileFilter |Укажите фильтр для выбора подмножества файлов из folderPath. Фильтр дает возможность выбирать только некоторые файлы, а не все.<br/><br/>Допустимые значения: `*` (несколько знаков) и `?` (один знак).<br/><br/>Пример 1: `"fileFilter": "*.log"`<br/>Пример 2: `"fileFilter": 2016-1-?.txt"`<br/><br/> Свойство fileFilter применяется к входному набору данных FileShare. HDFS не поддерживает это свойство. |Нет  |
| partitionedBy |Чтобы указать для временного ряда данных динамические путь к папке и имя файла, используйте свойство partitionedBy. Например, путь к папке folderPath каждый час будет другим. |Нет  |
| свойства | Поддерживаются следующие типы формата: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat**, **ParquetFormat**. Свойству **type** в разделе format необходимо присвоить одно из этих значений. Дополнительные сведения см. в разделах о [текстовом формате](data-factory-supported-file-and-compression-formats.md#text-format), [формате Json](data-factory-supported-file-and-compression-formats.md#json-format), [формате Avro](data-factory-supported-file-and-compression-formats.md#avro-format), [формате Orc](data-factory-supported-file-and-compression-formats.md#orc-format) и [ формате Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format). <br><br> Если требуется скопировать файлы между файловыми хранилищами **как есть** (двоичное копирование), можно пропустить раздел форматирования в определениях входного и выходного наборов данных. |Нет  |
| compression | Укажите тип и уровень сжатия данных. Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**. Поддерживаемые уровни: **Optimal** и **Fastest**. Узнайте больше о [форматах файлов и сжатия данных в фабрике данных Azure](data-factory-supported-file-and-compression-formats.md#compression-support). |Нет  |
| useBinaryTransfer |Укажите, использовать ли режим передачи в двоичном формате. Значение true, если использовать двоичный режим, и false, если ASCII. Значение по умолчанию: True. Это свойство можно использовать, только когда тип связанной службы является FTP-сервер. |Нет  |

> [!NOTE]
> Свойства filename и fileFilter нельзя использовать одновременно.

#### <a name="example"></a>Пример

```json
{
    "name": "SFTPFileInput",
    "properties": {
        "type": "FileShare",
        "linkedServiceName": "SftpLinkedService",
        "typeProperties": {
            "folderPath": "<path to shared folder>",
            "fileName": "test.csv"
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе SFTP](data-factory-sftp-connector.md#dataset-properties). 

### <a name="file-system-source-in-copy-activity"></a>Источник файловой системы в действии копирования
При копировании данных из источника SFTP задайте **FileSystemSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| recursive |Указывает, следует ли читать данные рекурсивно из вложенных папок или только из указанной папки. |True, False (по умолчанию) |Нет  |



#### <a name="example"></a>Пример

```json
{
    "name": "pipeline",
    "properties": {
        "activities": [{
            "name": "SFTPToBlobCopy",
            "inputs": [{
                "name": "SFTPFileInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "FileSystemSource"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "NewestFirst",
                "retry": 1,
                "timeout": "00:05:00"
            }
        }],
        "start": "2017-02-20T18:00:00",
        "end": "2017-02-20T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе SFTP](data-factory-sftp-connector.md#copy-activity-properties).


## <a name="http"></a>HTTP

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы HTTP задайте **Http** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| URL-адрес | Базовый URL-адрес веб-сервера. | Yes |
| authenticationType | Указывает тип проверки подлинности. Допустимые значения: **Anonymous**, **Basic**, **Digest**, **Windows** и **ClientCertificate**. <br><br> В разделах ниже описываются дополнительные свойства и приведены примеры JSON для поддерживаемых типов проверки подлинности. | Yes |
| enableServerCertificateValidation | Укажите, следует ли включать проверку SSL-сертификата на сервере, если источником является веб-сервер HTTPS. | Нет. Значение по умолчанию — true. |
| gatewayName | Имя шлюза управления данными для подключения к локальному источнику HTTP. | Да, если копирование выполняется из локального источника HTTP. |
| encryptedCredential | Зашифрованные учетные данные для доступа к конечной точке HTTP. При настройке сведений для проверки подлинности в мастере копирования или всплывающем диалоговом окне ClickOnce создаются автоматически. | Нет. Применимо, только когда копирование данных выполняется с локального HTTP-сервера. |

#### <a name="example-using-basic-digest-or-windows-authentication"></a>Пример: использование типов проверки подлинности Basic, Digest или Windows
Задайте для свойства `authenticationType` значения `Basic`, `Digest` или `Windows` и укажите следующие свойства помимо универсальных свойств соединителя HTTP, приведенных выше.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| Имя пользователя | Имя пользователя для доступа к конечной точке HTTP. | Yes |
| password | Пароль пользователя, указанного в свойстве имя пользователя. | Yes |

```json
{
    "name": "HttpLinkedService",
    "properties": {
        "type": "Http",
        "typeProperties": {
            "authenticationType": "basic",
            "url": "https://en.wikipedia.org/wiki/",
            "userName": "user name",
            "password": "password"
        }
    }
}
```

#### <a name="example-using-clientcertificate-authentication"></a>Пример: использование типа проверки подлинности ClientCertificate

Чтобы использовать базовую проверку подлинности, задайте для `authenticationType` значение `ClientCertificate` и укажите приведенные ниже свойства помимо универсальных свойств соединителя HTTP, которые описаны выше.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| embeddedCertData | Содержимое двоичных данных файла обмена личной информацией (PFX-файла) с кодировкой Base64. | Должно быть указано одно из свойств: `embeddedCertData` или `certThumbprint`. |
| certThumbprint | Отпечаток сертификата, который был установлен в хранилище сертификатов на компьютере шлюза. Применимо, только когда копирование данных выполняется из локального источника HTTP. | Должно быть указано одно из свойств: `embeddedCertData` или `certThumbprint`. |
| password | Пароль, связанный с сертификатом. | Нет  |

Если вы используете свойство `certThumbprint` для проверки подлинности, а сертификат установлен в личном хранилище локального компьютера, вам необходимо предоставить службе шлюза разрешение на чтение.

1. Запустите консоль управления (MMC). Добавьте оснастку **Сертификаты**, которая связана с **локальным компьютером**.
2. Разверните **Сертификаты**, **Личные**, а затем щелкните **Сертификаты**.
3. Щелкните правой кнопкой мыши сертификат в личном хранилище, а затем выберите **Все задачи**->**Управление закрытыми ключами...**
3. На вкладке **Безопасность** добавьте учетную запись пользователя, под которой запущена служба узла шлюза управления данными с доступом на чтение к сертификату.  

**Пример: использование клиентского сертификата.**Эта связанная служба связывает фабрику данных с локальным веб-сервером HTTP. Она использует сертификат клиента, установленный на компьютере со шлюзом управления данными.

```json
{
    "name": "HttpLinkedService",
    "properties": {
        "type": "Http",
        "typeProperties": {
            "authenticationType": "ClientCertificate",
            "url": "https://en.wikipedia.org/wiki/",
            "certThumbprint": "thumbprint of certificate",
            "gatewayName": "gateway name"
        }
    }
}
```

#### <a name="example-using-client-certificate-in-a-file"></a>Пример: использование сертификата клиента в файле
Эта связанная служба связывает фабрику данных с локальным веб-сервером HTTP. Она использует файл сертификата клиента на компьютере, где установлен шлюз управления данными.

```json
{
    "name": "HttpLinkedService",
    "properties": {
        "type": "Http",
        "typeProperties": {
            "authenticationType": "ClientCertificate",
            "url": "https://en.wikipedia.org/wiki/",
            "embeddedCertData": "base64 encoded cert data",
            "password": "password of cert"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе HTTP](data-factory-http-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных HTTP задайте **Http** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| relativeUrl | Относительный URL-адрес ресурса, который содержит данные. Если путь не задан, используется только URL-адрес, указанный в определении связанной службы. <br><br> Для создания динамического URL-адреса можно использовать [функции фабрики данных и системные переменные](data-factory-functions-variables.md), например: `"relativeUrl": "$$Text.Format('/my/report?month={0:yyyy}-{0:MM}&fmt=csv', SliceStart)"`. | Нет  |
| requestMethod | Метод HTTP. Допустимые значения: **GET** или **POST**. | Нет. Значение по умолчанию — `GET`. |
| additionalHeaders | Дополнительные заголовки HTTP-запроса. | Нет  |
| requestBody | Текст HTTP-запроса. | Нет  |
| свойства | Если вы хотите просто **извлечь данные из конечной точки HTTP "как есть"** — без анализа, пропустите параметры форматирования. <br><br> Поддерживаемые форматы для выполнения анализа содержимого ответа HTTP в процессе выполнения операции копирования: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat** и **ParquetFormat**. Дополнительные сведения см. в разделах о [текстовом формате](data-factory-supported-file-and-compression-formats.md#text-format), [формате Json](data-factory-supported-file-and-compression-formats.md#json-format), [формате Avro](data-factory-supported-file-and-compression-formats.md#avro-format), [формате Orc](data-factory-supported-file-and-compression-formats.md#orc-format) и [ формате Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format). |Нет  |
| compression | Укажите тип и уровень сжатия данных. Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**. Поддерживаемые уровни: **Optimal** и **Fastest**. Узнайте больше о [форматах файлов и сжатия данных в фабрике данных Azure](data-factory-supported-file-and-compression-formats.md#compression-support). |Нет  |

#### <a name="example-using-the-get-default-method"></a>Пример: использование метода GET (по умолчанию)

```json
{
    "name": "HttpSourceDataInput",
    "properties": {
        "type": "Http",
        "linkedServiceName": "HttpLinkedService",
        "typeProperties": {
            "relativeUrl": "XXX/test.xml",
            "additionalHeaders": "Connection: keep-alive\nUser-Agent: Mozilla/5.0\n"
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        }
    }
}
```

#### <a name="example-using-the-post-method"></a>Пример: использование метода POST

```json
{
    "name": "HttpSourceDataInput",
    "properties": {
        "type": "Http",
        "linkedServiceName": "HttpLinkedService",
        "typeProperties": {
            "relativeUrl": "/XXX/test.xml",
            "requestMethod": "Post",
            "requestBody": "body for POST HTTP request"
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        }
    }
}
```
Дополнительные сведения см. в статье о [соединителе HTTP](data-factory-http-connector.md#dataset-properties).

### <a name="http-source-in-copy-activity"></a>Источник HTTP в действии копирования
При копировании данных из источника HTTP задайте **HttpSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Обязательно |
| -------- | ----------- | -------- |
| httpRequestTimeout | Время ожидания ответа для HTTP-запроса. Это интервал времени для получения ответа, а не считывания данных ответа. | Нет. Значение по умолчанию  — 00:01:40. |


#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "HttpSourceToAzureBlob",
            "description": "Copy from an HTTP source to an Azure blob",
            "type": "Copy",
            "inputs": [{
                "name": "HttpSourceDataInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "HttpSource"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе HTTP](data-factory-http-connector.md#copy-activity-properties).

## <a name="odata"></a>OData

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы OData задайте **OData** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| URL-адрес |URL-адрес службы OData. |Yes |
| authenticationType |Тип проверки подлинности, используемый для подключения к источнику OData. <br/><br/> Возможные значения для облачной службы OData: "Анонимно", "Обычная" и "OAuth" (обратите внимание, что сейчас фабрика данных Azure поддерживает только аутентификацию OAuth на основе Azure Active Directory). <br/><br/> Для локальной службы OData возможными значениями являются "Анонимная", "Обычная" и "Windows". |Yes |
| Имя пользователя |При использовании обычной проверки подлинности укажите имя пользователя. |Да (только при использовании обычной проверки подлинности) |
| password |Введите пароль для учетной записи пользователя, указанной для выбранного имени пользователя. |Да (только при использовании обычной проверки подлинности) |
| authorizedCredential |Если используется OAuth, в мастере копирования фабрики данных или редакторе нажмите кнопку **Авторизовать** и введите свои учетные данные, после чего значение этого свойства будет создано автоматически. |Да (только при использовании аутентификации OAuth) |
| gatewayName |Имя шлюза, который следует использовать службе фабрики данных для подключения к локальной службе OData. Его необходимо указывать только в том случае, если вы копируете данные из источника в локальной службе OData. |Нет  |

#### <a name="example---using-basic-authentication"></a>Пример: использование обычной проверки подлинности
```json
{
    "name": "inputLinkedService",
    "properties": {
        "type": "OData",
        "typeProperties": {
            "url": "http://services.odata.org/OData/OData.svc",
            "authenticationType": "Basic",
            "username": "username",
            "password": "password"
        }
    }
}
```

#### <a name="example---using-anonymous-authentication"></a>Пример: использование анонимной проверки подлинности

```json
{
    "name": "ODataLinkedService",
    "properties": {
        "type": "OData",
        "typeProperties": {
            "url": "http://services.odata.org/OData/OData.svc",
            "authenticationType": "Anonymous"
        }
    }
}
```

#### <a name="example---using-windows-authentication-accessing-on-premises-odata-source"></a>Пример: использование проверки подлинности Windows при получении доступа к локальному источнику OData

```json
{
    "name": "inputLinkedService",
    "properties": {
        "type": "OData",
        "typeProperties": {
            "url": "<endpoint of on-premises OData source, for example, Dynamics CRM>",
            "authenticationType": "Windows",
            "username": "domain\\user",
            "password": "password",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

#### <a name="example---using-oauth-authentication-accessing-cloud-odata-source"></a>Пример: использование проверки подлинности OAuth при получении доступа к облачному источнику OData
```json
{
    "name": "inputLinkedService",
    "properties":
    {
        "type": "OData",
            "typeProperties":
        {
            "url": "<endpoint of cloud OData source, for example, https://<tenant>.crm.dynamics.com/XRMServices/2011/OrganizationData.svc>",
            "authenticationType": "OAuth",
            "authorizedCredential": "<auto generated by clicking the Authorize button on UI>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе OData](data-factory-odata-connector.md#linked-service-properties).

### <a name="dataset"></a>Выборка
Для определения набора данных OData задайте **ODataResource** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| path |Путь к ресурсу OData |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "ODataDataset",
    "properties": {
        "type": "ODataResource",
        "typeProperties": {
            "path": "Products"
        },
        "linkedServiceName": "ODataLinkedService",
        "structure": [],
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true,
        "policy": {
            "retryInterval": "00:01:00",
            "retryTimeout": "00:10:00",
            "maximumRetry": 3
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе OData](data-factory-odata-connector.md#dataset-properties).

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из источника OData задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Пример | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |"?$select=Name, Description&$top=5" |Нет  |

#### <a name="example"></a>Пример

```json
{
    "name": "CopyODataToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "?$select=Name, Description&$top=5"
                },
                "sink": {
                    "type": "BlobSink",
                    "writeBatchSize": 0,
                    "writeBatchTimeout": "00:00:00"
                }
            },
            "inputs": [{
                "name": "ODataDataSet"
            }],
            "outputs": [{
                "name": "AzureBlobODataDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "ODataToBlob"
        }],
        "start": "2017-02-01T18:00:00",
        "end": "2017-02-03T19:00:00"
    }
}
```

Дополнительные сведения см. в статье о [соединителе OData](data-factory-odata-connector.md#copy-activity-properties).


## <a name="odbc"></a>ODBC


### <a name="linked-service"></a>Связанные службы
Для определения связанной службы ODBC задайте **OnPremisesOdbc** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| connectionString |Учетные данные в строке подключения, не используемые для получения доступа, а также дополнительные зашифрованные учетные данные. Примеры приведены в следующих разделах. |Yes |
| credential |Учетные данные в строке подключения, используемые для получения доступа и указанные в формате "драйвер-определенное свойство-значение". Пример: “Uid=<user ID>;Pwd=<password>;RefreshToken=<secret refresh token>;”. |Нет  |
| authenticationType |Тип проверки подлинности, используемый для подключения к хранилищу данных ODBC. Возможными значениями являются: "Анонимная" и "Обычная". |Yes |
| Имя пользователя |При использовании обычной проверки подлинности укажите имя пользователя. |Нет  |
| password |Введите пароль для учетной записи пользователя, указанной для выбранного имени пользователя. |Нет  |
| gatewayName |Имя шлюза, который следует использовать службе фабрики данных для подключения к хранилищу данных ODBC. |Yes |

#### <a name="example---using-basic-authentication"></a>Пример: использование обычной проверки подлинности

```json
{
    "name": "ODBCLinkedService",
    "properties": {
        "type": "OnPremisesOdbc",
        "typeProperties": {
            "authenticationType": "Basic",
            "connectionString": "Driver={SQL Server};Server=Server.database.windows.net; Database=TestDatabase;",
            "userName": "username",
            "password": "password",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```
#### <a name="example---using-basic-authentication-with-encrypted-credentials"></a>Пример: использование обычной проверки подлинности и шифрования учетных данных
Учетные данные можно зашифровать с помощью командлета [New-AzureRMDataFactoryEncryptValue](https://msdn.microsoft.com/library/mt603802.aspx) (Azure PowerShell 1.0) или [New-AzureDataFactoryEncryptValue](https://msdn.microsoft.com/library/dn834940.aspx) (Azure PowerShell 0.9 или более ранней версии).  

```json
{
    "name": "ODBCLinkedService",
    "properties": {
        "type": "OnPremisesOdbc",
        "typeProperties": {
            "authenticationType": "Basic",
            "connectionString": "Driver={SQL Server};Server=myserver.database.windows.net; Database=TestDatabase;;EncryptedCredential=eyJDb25uZWN0...........................",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

#### <a name="example-using-anonymous-authentication"></a>Пример: использование анонимной проверки подлинности

```json
{
    "name": "ODBCLinkedService",
    "properties": {
        "type": "OnPremisesOdbc",
        "typeProperties": {
            "authenticationType": "Anonymous",
            "connectionString": "Driver={SQL Server};Server={servername}.database.windows.net; Database=TestDatabase;",
            "credential": "UID={uid};PWD={pwd}",
            "gatewayName": "<onpremgateway>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе ODBC](data-factory-odbc-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных ODBC задайте **RelationalTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы в хранилище данных ODBC. |Yes |


#### <a name="example"></a>Пример

```json
{
    "name": "ODBCDataSet",
    "properties": {
        "type": "RelationalTable",
        "linkedServiceName": "ODBCLinkedService",
        "typeProperties": {},
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true,
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе ODBC](data-factory-odbc-connector.md#dataset-properties). 

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из хранилища данных ODBC задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Строка запроса SQL. Например, `select * from MyTable`. |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "CopyODBCToBlob",
    "properties": {
        "description": "pipeline for copy activity",
        "activities": [{
            "type": "Copy",
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "$$Text.Format('select * from MyTable where timestamp >= \\'{0:yyyy-MM-ddTHH:mm:ss}\\' AND timestamp < \\'{1:yyyy-MM-ddTHH:mm:ss}\\'', WindowStart, WindowEnd)"
                },
                "sink": {
                    "type": "BlobSink",
                    "writeBatchSize": 0,
                    "writeBatchTimeout": "00:00:00"
                }
            },
            "inputs": [{
                "name": "OdbcDataSet"
            }],
            "outputs": [{
                "name": "AzureBlobOdbcDataSet"
            }],
            "policy": {
                "timeout": "01:00:00",
                "concurrency": 1
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "name": "OdbcToBlob"
        }],
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00"
    }
}
``` 

Дополнительные сведения см. в статье о [соединителе ODBC](data-factory-odbc-connector.md#copy-activity-properties).

## <a name="salesforce"></a>Salesforce


### <a name="linked-service"></a>Связанные службы
Для определения связанной службы Salesforce задайте **Salesforce** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| environmentUrl | Укажите URL-адрес экземпляра Salesforce. <br><br> — URL-адрес по умолчанию — https://login.salesforce.com. <br> — Чтобы скопировать данные из песочницы, укажите URL-адрес https://test.salesforce.com. <br> — Чтобы скопировать данные из пользовательского домена, укажите URL-адрес, например https://[домен].my.salesforce.com. |Нет  |
| Имя пользователя |Укажите имя пользователя для учетной записи пользователя. |Yes |
| password |Укажите пароль для учетной записи пользователя. |Yes |
| securityToken |Укажите маркер безопасности для учетной записи пользователя. Инструкции по получению и сбросу маркера безопасности см. в статье [Get security token](https://help.salesforce.com/apex/HTViewHelpDoc?id=user_security_token.htm) (Получение маркера безопасности). Общие сведения о маркере безопасности см. в статье [Security and the API](https://developer.salesforce.com/docs/atlas.en-us.api.meta/api/sforce_api_concepts_security.htm) (Безопасность и API). |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "SalesforceLinkedService",
    "properties": {
        "type": "Salesforce",
        "typeProperties": {
            "username": "<user name>",
            "password": "<password>",
            "securityToken": "<security token>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Salesforce](data-factory-salesforce-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения набора данных Salesforce задайте **RelationalTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| tableName |Имя таблицы в Salesforce |Нет (если для свойства **RelationalSource** задано значение **query**). |

#### <a name="example"></a>Пример

```json
{
    "name": "SalesforceInput",
    "properties": {
        "linkedServiceName": "SalesforceLinkedService",
        "type": "RelationalTable",
        "typeProperties": {
            "tableName": "AllDataType__c"
        },
        "availability": {
            "frequency": "Hour",
            "interval": 1
        },
        "external": true,
        "policy": {
            "externalData": {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
            }
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе Salesforce](data-factory-salesforce-connector.md#dataset-properties). 

### <a name="relational-source-in-copy-activity"></a>Реляционный источник в действии копирования
При копировании данных из Salesforce задайте **RelationalSource** в качестве **типа источника** для действия копирования и укажите в разделе **source** следующие свойства:

| Свойство | ОПИСАНИЕ | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| query |Используйте пользовательский запрос для чтения данных. |Запрос SQL-92 или запрос, написанный на [объектно-ориентированном языке запросов Salesforce (SOQL)](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm) . Пример: `select * from MyTable__c`. |Нет (если для свойства **tableName** задано значение **dataset**). |

#### <a name="example"></a>Пример  



```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "SalesforceToAzureBlob",
            "description": "Copy from Salesforce to an Azure blob",
            "type": "Copy",
            "inputs": [{
                "name": "SalesforceInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "RelationalSource",
                    "query": "SELECT Id, Col_AutoNumber__c, Col_Checkbox__c, Col_Currency__c, Col_Date__c, Col_DateTime__c, Col_Email__c, Col_Number__c, Col_Percent__c, Col_Phone__c, Col_Picklist__c, Col_Picklist_MultiSelect__c, Col_Text__c, Col_Text_Area__c, Col_Text_AreaLong__c, Col_Text_AreaRich__c, Col_URL__c, Col_Text_Encrypt__c, Col_Lookup__c FROM AllDataType__c"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

> [!IMPORTANT]
> Имя API для любых настраиваемых объектов должно содержать приставку __c.

Дополнительные сведения см. в статье о [соединителе Salesforce](data-factory-salesforce-connector.md#copy-activity-properties). 

## <a name="web-data"></a>Веб-данные 

### <a name="linked-service"></a>Связанные службы
Для определения связанной веб-службы задайте **Web** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| URL-адрес |URL-адрес источника Web |Yes |
| authenticationType |Анонимная. |Yes |
 

#### <a name="example"></a>Пример


```json
{
    "name": "web",
    "properties": {
        "type": "Web",
        "typeProperties": {
            "authenticationType": "Anonymous",
            "url": "https://en.wikipedia.org/wiki/"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе веб-таблицы](data-factory-web-table-connector.md#linked-service-properties). 

### <a name="dataset"></a>Выборка
Для определения веб-набора данных задайте **WebTable** в качестве **типа** набора данных и укажите в разделе **typeProperties** следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| Тип |Тип набора данных. Необходимо задать значение **WebTable** |Yes |
| path |Относительный URL-адрес ресурса, который содержит таблицу. |Нет. Если путь не задан, используется только URL-адрес, указанный в определении связанной службы. |
| index |Индекс таблицы в ресурсе. Дополнительные сведения см. в разделе [Получение индекса таблицы на HTML-странице](#get-index-of-a-table-in-an-html-page). |Yes |

#### <a name="example"></a>Пример

```json
{
    "name": "WebTableInput",
    "properties": {
        "type": "WebTable",
        "linkedServiceName": "WebLinkedService",
        "typeProperties": {
            "index": 1,
            "path": "AFI's_100_Years...100_Movies"
        },
        "external": true,
        "availability": {
            "frequency": "Hour",
            "interval": 1
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе веб-таблицы](data-factory-web-table-connector.md#dataset-properties). 

### <a name="web-source-in-copy-activity"></a>Веб-источник в действии копирования
При копировании данных из веб-таблицы задайте **WebSource** в качестве **типа источника** для действия копирования. В настоящее время, если источник в действии копирования относится к типу **WebSource**, дополнительные свойства не поддерживается.

#### <a name="example"></a>Пример

```json
{
    "name": "SamplePipeline",
    "properties": {
        "start": "2016-06-01T18:00:00",
        "end": "2016-06-01T19:00:00",
        "description": "pipeline with copy activity",
        "activities": [{
            "name": "WebTableToAzureBlob",
            "description": "Copy from a Web table to an Azure blob",
            "type": "Copy",
            "inputs": [{
                "name": "WebTableInput"
            }],
            "outputs": [{
                "name": "AzureBlobOutput"
            }],
            "typeProperties": {
                "source": {
                    "type": "WebSource"
                },
                "sink": {
                    "type": "BlobSink"
                }
            },
            "scheduler": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "concurrency": 1,
                "executionPriorityOrder": "OldestFirst",
                "retry": 0,
                "timeout": "01:00:00"
            }
        }]
    }
}
```

Дополнительные сведения см. в статье о [соединителе веб-таблицы](data-factory-web-table-connector.md#copy-activity-properties). 

## <a name="compute-environments"></a>ВЫЧИСЛИТЕЛЬНЫЕ СРЕДЫ
Следующая таблица содержит список вычислительных сред, поддерживаемых фабрикой данных, и доступных в них действий преобразования. Щелкните ссылку для интересующего вас вычисления, чтобы просмотреть схемы JSON связанной службы и связать ее с фабрикой данных. 

| Вычислительная среда | Действия |
| --- | --- |
| [Кластер HDInsight по запросу](#on-demand-azure-hdinsight-cluster) или [собственный кластер HDInsight](#existing-azure-hdinsight-cluster) |[Настраиваемое действие .NET](#net-custom-activity), [действие Hive](#hdinsight-hive-activity), [действие Pig] (#hdinsight-pig-activity), [действие MapReduce](#hdinsight-mapreduce-activity), [действие потоковой передачи Hadoop](#hdinsight-streaming-activityd), [действие Spark](#hdinsight-spark-activity) |
| [Пакетная служба Azure](#azure-batch) |[Настраиваемое действие .NET](#net-custom-activity) |
| [машинное обучение Azure](#azure-machine-learning) | [Действие выполнения пакета в службе машинного обучения](#machine-learning-batch-execution-activity), [действие обновления ресурса в службе машинного обучения](#machine-learning-update-resource-activity) |
| [Аналитика озера данных Azure](#azure-data-lake-analytics) |[Аналитика озера данных U-SQL](#data-lake-analytics-u-sql-activity) |
| [База данных Azure SQL](#azure-sql-database-1), [хранилище данных Azure SQL](#azure-sql-data-warehouse-1), [SQL Server](#sql-server-1) |[Хранимая процедура](#stored-procedure-activity) |

## <a name="on-demand-azure-hdinsight-cluster"></a>Кластер Azure HDInsight по требованию
Для обработки данных служба фабрики данных Azure автоматически создает кластер HDInsight под управлением Windows/Linux по запросу. Кластер создается в том же регионе, что и учетная запись хранения (свойство linkedServiceName в JSON), связанная с кластером. В этой связанной службе можно выполнить следующие действия преобразования: [настраиваемое действие .NET](#net-custom-activity), [действие Hive](#hdinsight-hive-activity), [действие Pig] (#hdinsight-pig-activity), [действие MapReduce](#hdinsight-mapreduce-activity), [действие потоковой передачи Hadoop](#hdinsight-streaming-activityd), [действие Spark](#hdinsight-spark-activity). 

### <a name="linked-service"></a>Связанные службы 
Следующая таблица содержит описание свойств, используемых в определении Azure JSON связанной службы HDInsight по требованию.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| Тип |Свойству type необходимо присвоить значение **HDInsightOnDemand**. |Yes |
| clusterSize |Общее количество рабочих узлов и узлов данных в кластере. Кластер HDInsight создается с 2 головными узлами и количеством рабочих узлов, заданным в этом свойстве. Узлы имеют размер Standard_D3 с 4 ядрами, то есть кластер с 4 рабочими узлами использует 24 ядра (4\*4 = 16 для рабочих узлов + 2\*4 = 8 для головных узлов). Дополнительные сведения об уровне Standard_D3 см. в статье [Создание кластеров Hadoop под управлением Linux в HDInsight](../../hdinsight/hdinsight-hadoop-provision-linux-clusters.md). |Yes |
| timeToLive |Допустимое время простоя кластера HDInsight по запросу. Указывает, как долго кластер HDInsight по запросу остается активным после выполнения действия, если в кластере нет других активных заданий.<br/><br/>Например, если выполнение действия занимает 6 минут, а значение свойства timetolive равно 5 минутам, кластер остается активным в течение 5 минут по истечении 6-минутного выполнения действия. Если в течение этих 6 минут выполняется другое действие, оно обрабатывается в том же кластере.<br/><br/>Создание кластера HDInsight по запросу является ресурсоемкой операцией и может занять некоторое время. При необходимости используйте этот параметр для повышения производительности фабрики данных путем повторного использования кластера HDInsight по запросу.<br/><br/>Если значение timetolive равно 0, кластер удаляется сразу после выполнения действия. С другой стороны, если задать большое значение, кластер может простаивать без необходимости, что приведет к большим затратам. Поэтому необходимо установить соответствующее значение в соответствии со своими потребностями.<br/><br/>Если значение свойства timetolive задано правильно, один и тот же экземпляр кластера HDInsight по запросу могут совместно использовать несколько конвейеров. |Yes |
| версия |Версия кластера HDInsight. Подробные сведения см. в статье [Версии HDInsight, поддерживаемые в фабрике данных Azure](data-factory-compute-linked-services.md#supported-hdinsight-versions-in-azure-data-factory). |Нет  |
| linkedServiceName |Связанная служба хранилища Azure, которую кластер по запросу должен использовать для хранения и обработки данных. <p>В настоящее время недоступно создание кластера HDInsight по запросу, который использует в качестве хранилища Azure Data Lake Store. Чтобы сохранить данные результатов обработки HDInsight в Azure Data Lake Store, воспользуйтесь действием копирования и скопируйте данные из хранилища BLOB-объектов Azure в Azure Data Lake Store.</p>  | Yes |
| additionalLinkedServiceNames |Указывает дополнительные учетные записи хранения для связанной службы HDInsight, чтобы служба фабрики данных могла регистрировать их от вашего имени. |Нет  |
| osType |Тип операционной системы. Допустимые значения: Windows (по умолчанию) и Linux. |Нет  |
| hcatalogLinkedServiceName |Имя связанной службы SQL Azure, указывающие на базу данных HCatalog. При создании кластера HDInsight по запросу используется база данных SQL Azure в качестве хранилища метаданных. |Нет  |

### <a name="json-example"></a>Пример JSON-файла
Представленный ниже код JSON определяет связанную службу HDInsight по запросу под управлением Linux. При обработке среза данных служба фабрики данных автоматически создает кластер HDInsight **под управлением Linux** . 

```json
{
    "name": "HDInsightOnDemandLinkedService",
    "properties": {
        "type": "HDInsightOnDemand",
        "typeProperties": {
            "version": "3.5",
            "clusterSize": 1,
            "timeToLive": "00:05:00",
            "osType": "Linux",
            "linkedServiceName": "StorageLinkedService"
        }
    }
}
```

Дополнительные сведения см. в статье [Вычислительные среды, поддерживаемые фабрикой данных Azure](data-factory-compute-linked-services.md). 

## <a name="existing-azure-hdinsight-cluster"></a>Имеющийся кластер Azure HDInsight
Чтобы зарегистрировать собственный кластер HDInsight в фабрике данных, вы можете создать связанную службу Azure HDInsight. В этой связанной службе можно выполнить следующие действия преобразования: [настраиваемое действие .NET](#net-custom-activity), [действие Hive](#hdinsight-hive-activity), [действие Pig] (#hdinsight-pig-activity), [действие MapReduce](#hdinsight-mapreduce-activity), [действие потоковой передачи Hadoop](#hdinsight-streaming-activityd), [действие Spark](#hdinsight-spark-activity). 

### <a name="linked-service"></a>Связанные службы
Следующая таблица содержит описание свойств, используемых в определении Azure JSON связанной службы Azure HDInsight.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| Тип |Свойству type необходимо присвоить значение **HDInsight**. |Yes |
| clusterUri |Универсальный код ресурса (URI) кластера HDInsight. |Yes |
| Имя пользователя |Укажите имя пользователя, которое будет использоваться для подключения к существующему кластеру HDInsight. |Yes |
| password |Укажите пароль для учетной записи пользователя. |Yes |
| linkedServiceName | Имя связанной службы для службы хранилища Azure, которая обращается к хранилищу BLOB-объектов Azure, используемому кластером HDInsight. <p>В настоящее время для этого свойства невозможно указать связанную службу Azure Data Lake Store. Вы можете получать доступ к данным в Azure Data Lake Store из сценариев Hive или Pig, если кластер HDInsight имеет доступ к Data Lake Store. </p>  |Yes |

Список поддерживаемых версий кластеров HDInsight см. в статье [Поддерживаемые версии HDInsight](data-factory-compute-linked-services.md#supported-hdinsight-versions-in-azure-data-factory). 

#### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "HDInsightLinkedService",
    "properties": {
        "type": "HDInsight",
        "typeProperties": {
            "clusterUri": " https://<hdinsightclustername>.azurehdinsight.net/",
            "userName": "admin",
            "password": "<password>",
            "linkedServiceName": "MyHDInsightStoragelinkedService"
        }
    }
}
```

## <a name="azure-batch"></a>Пакетная служба Azure
Чтобы зарегистрировать пакетный пул виртуальных машин в фабрике данных, можно создать связанную пакетную службу Azure. Пользовательские действия .NET можно выполнять с помощью пакетной службы Azure или службы Azure HDInsight. В этой связанной службе можно запустить [настраиваемое действие .NET](#net-custom-activity). 

### <a name="linked-service"></a>Связанные службы
Следующая таблица содержит описание свойств, используемых в определении Azure JSON связанной пакетной службы Azure.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| Тип |Свойству type необходимо присвоить значение **AzureBatch**. |Yes |
| accountName |Имя учетной записи пакетной службы Azure |Yes |
| accessKey |Ключ доступа к учетной записи пакетной службы Azure. |Yes |
| poolName |Имя пула виртуальных машин. |Yes |
| linkedServiceName |Имя связанной службы хранилища Azure, которая ассоциируется с этой связанной пакетной службой Azure. Эта связанная служба используется для промежуточных файлов, необходимых для выполнения действий и хранения журналов выполнения действий. |Yes |


#### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "AzureBatchLinkedService",
    "properties": {
        "type": "AzureBatch",
        "typeProperties": {
            "accountName": "<Azure Batch account name>",
            "accessKey": "<Azure Batch account key>",
            "poolName": "<Azure Batch pool name>",
            "linkedServiceName": "<Specify associated storage linked service reference here>"
        }
    }
}
```

## <a name="azure-machine-learning"></a>Машинное обучение Azure
Создайте связанную службу Машинного обучения Azure, чтобы зарегистрировать конечную точку пакетной оценки показателей машинного обучения оценки в фабрике данных. В этой связанной службе можно выполнять два действия преобразования данных: [действие выполнения пакета в службе машинного обучения](#machine-learning-batch-execution-activity), [действие обновления ресурса службы машинного обучения](#machine-learning-update-resource-activity). 

### <a name="linked-service"></a>Связанные службы
Следующая таблица содержит описание свойств, используемых в определении Azure JSON связанной службы машинного обучения Azure.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| type |Свойству type необходимо присвоить значение **AzureML**. |Yes |
| mlEndpoint |URL-адрес пакетной оценки. |Yes |
| apiKey |API модели опубликованной рабочей области. |Yes |

#### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "AzureMLLinkedService",
    "properties": {
        "type": "AzureML",
        "typeProperties": {
            "mlEndpoint": "https://[batch scoring endpoint]/jobs",
            "apiKey": "<apikey>"
        }
    }
}
```

## <a name="azure-data-lake-analytics"></a>Аналитика озера данных Azure
Можно создать связанную службу **аналитики озера данных Azure** , чтобы связать службу вычислений аналитики озера данных Azure с фабрикой данных Azure, прежде чем использовать [действие U-SQL аналитики озера данных](data-factory-usql-activity.md) в конвейере.

### <a name="linked-service"></a>Связанные службы

Следующая таблица содержит описание свойств, используемых в определении JSON связанной службы Azure Data Lake Analytics. 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| type |Свойству type необходимо присвоить значение **AzureDataLakeAnalytics**. |Yes |
| accountName |Имя учетной записи аналитики озера данных Azure. |Yes |
| dataLakeAnalyticsUri |Универсальный код ресурса (URI) аналитики озера данных Azure. |Нет  |
| authorization |Код авторизации извлекается автоматически после нажатия кнопки **Авторизовать** в редакторе фабрики данных и выполнения входа с авторизацией OAuth. |Yes |
| subscriptionId |Идентификатор подписки Azure |Нет (если не указан, используется подписка фабрики данных). |
| имя_группы_ресурсов |Имя группы ресурсов Azure |Нет (если не указано, используется группа ресурсов фабрики данных). |
| sessionid |Идентификатор сеанса из сеанса авторизации OAuth. Каждый идентификатор сеанса является уникальным и используется только один раз. При использовании редактора фабрики данных этот идентификатор создается автоматически. |Yes |


#### <a name="json-example"></a>Пример JSON-файла
В следующем примере представлено определение JSON для связанной службы аналитики озера данных Azure.

```json
{
    "name": "AzureDataLakeAnalyticsLinkedService",
    "properties": {
        "type": "AzureDataLakeAnalytics",
        "typeProperties": {
            "accountName": "<account name>",
            "dataLakeAnalyticsUri": "datalakeanalyticscompute.net",
            "authorization": "<authcode>",
            "sessionId": "<session ID>",
            "subscriptionId": "<subscription id>",
            "resourceGroupName": "<resource group name>"
        }
    }
}
```

## <a name="azure-sql-database"></a>Базы данных SQL Azure
Связанная служба SQL Azure создается и применяется к [действию хранимой процедуры](#stored-procedure-activity) для вызова хранимой процедуры из конвейера фабрики данных. 

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы базы данных SQL Azure задайте **AzureSqlDatabase** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| connectionString |В свойстве connectionString указываются сведения, необходимые для подключения к экземпляру базы данных SQL Azure. |Yes |

#### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "AzureSqlLinkedService",
    "properties": {
        "type": "AzureSqlDatabase",
        "typeProperties": {
            "connectionString": "Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;User ID=<username>@<servername>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30"
        }
    }
}
```

Дополнительную информацию см. в статье о [связанной службе SQL Azure](data-factory-azure-sql-connector.md#linked-service-properties).

## <a name="azure-sql-data-warehouse"></a>Хранилище данных SQL Azure
Связанная служба хранилища данных SQL Azure создается и применяется к [действию хранимой процедуры](data-factory-stored-proc-activity.md) для вызова хранимой процедуры из конвейера фабрики данных. 

### <a name="linked-service"></a>Связанные службы
Для определения связанной службы хранилища данных SQL Azure задайте **AzureSqlDW** в качестве **типа** связанной службы и укажите в разделе **typeProperties** следующие свойства:  

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| connectionString |Укажите сведения, необходимые для подключения к экземпляру хранилища данных SQL Azure, для свойства connectionString. |Yes |

#### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "connectionString": "Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;User ID=<username>@<servername>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе хранилища данных SQL Azure](data-factory-azure-sql-data-warehouse-connector.md#linked-service-properties). 

## <a name="sql-server"></a>SQL Server; 
Связанная служба SQL Server создается и применяется к [действию хранимой процедуры](data-factory-stored-proc-activity.md) для вызова хранимой процедуры из конвейера фабрики данных. 

### <a name="linked-service"></a>Связанные службы
Создайте связанную службу типа **OnPremisesSqlServer**, чтобы связать локальную базу данных SQL Server с фабрикой данных. В следующей таблице содержится описание элементов JSON, которые относятся к локальной связанной службе SQL Server.

В следующей таблице содержится описание элементов JSON, которые относятся к связанной службе SQL Server.

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| Тип |Свойству type необходимо присвоить значение **OnPremisesSqlServer**. |Yes |
| connectionString |Укажите сведения о параметре connectionString, необходимые для подключения к локальной базе данных SQL Server с помощью проверки подлинности SQL или проверки подлинности Windows. |Yes |
| gatewayName |Имя шлюза, который службе фабрики данных следует использовать для подключения к локальной базе данных SQL Server. |Yes |
| Имя пользователя |При использовании проверки подлинности Windows укажите имя пользователя. Например, **domainname\\username**. |Нет  |
| password |Введите пароль для учетной записи пользователя, указанной для выбранного имени пользователя. |Нет  |

Вы можете зашифровать учетные данные с помощью командлета **New-AzureRmDataFactoryEncryptValue** и использовать их в строке подключения, как показано в следующем примере (свойство **EncryptedCredential**):  

```JSON
"connectionString": "Data Source=<servername>;Initial Catalog=<databasename>;Integrated Security=True;EncryptedCredential=<encrypted credential>",
```


#### <a name="example-json-for-using-sql-authentication"></a>Пример: JSON для использования проверки подлинности SQL

```json
{
    "name": "MyOnPremisesSQLDB",
    "properties": {
        "type": "OnPremisesSqlServer",
        "typeProperties": {
            "connectionString": "Data Source=<servername>;Initial Catalog=MarketingCampaigns;Integrated Security=False;User ID=<username>;Password=<password>;",
            "gatewayName": "<gateway name>"
        }
    }
}
```
#### <a name="example-json-for-using-windows-authentication"></a>Пример: JSON для использования проверки подлинности Windows

Если указаны имя пользователя и пароль, то шлюз использует их, чтобы действовать от имени соответствующей учетной записи пользователя для подключения к локальной базе данных SQL Server. В противном случае шлюз подключается к SQL Server напрямую с помощью контекста безопасности шлюза (его стартовая учетная запись).

```json
{
    "Name": " MyOnPremisesSQLDB",
    "Properties": {
        "type": "OnPremisesSqlServer",
        "typeProperties": {
            "ConnectionString": "Data Source=<servername>;Initial Catalog=MarketingCampaigns;Integrated Security=True;",
            "username": "<domain\\username>",
            "password": "<password>",
            "gatewayName": "<gateway name>"
        }
    }
}
```

Дополнительные сведения см. в статье о [соединителе SQL Server](data-factory-sqlserver-connector.md#linked-service-properties).

## <a name="data-transformation-activities"></a>Действия преобразования данных

Действие | ОПИСАНИЕ
-------- | -----------
[Действие Hive HDInsight](#hdinsight-hive-activity) | Действие Hive HDInsight в конвейере фабрики данных выполняет запросы Hive к вашему собственному кластеру HDInsight или кластеру HDInsight по запросу под управлением Windows или Linux. 
[Действие Pig HDInsight](#hdinsight-pig-activity) | Действие Pig HDInsight в конвейере фабрики данных выполняет запросы Pig к вашему собственному кластеру HDInsight или кластеру HDInsight по запросу под управлением Windows или Linux.
[Действие MapReduce HDInsight](#hdinsight-mapreduce-activity) | Действие MapReduce HDInsight в конвейере фабрики данных выполняет программы MapReduce для вашего собственного кластера HDInsight или кластера HDInsight по запросу под управлением Windows или Linux.
[Действие потоковой передачи HDInsight](#hdinsight-streaming-activity) | Действие потоковой передачи HDInsight в конвейере фабрики данных выполняет программы потоковой передачи Hadoop для вашего собственного кластера HDInsight или кластера HDInsight по запросу под управлением Windows или Linux.
[Действие HDInsight Spark](#hdinsight-spark-activity) | Действие HDInsight Spark в конвейере фабрики данных выполняет программы Spark в вашем кластере HDInsight. 
[Действие выполнения пакета машинного обучения](#machine-learning-batch-execution-activity) | Фабрика данных Azure позволяет легко создавать конвейеры, в которых для прогнозной аналитики используется опубликованная веб-служба Машинного обучения Azure. С помощью действия выполнения пакета в конвейере фабрики данных Azure можно вызывать веб-службу Машинного обучения Azure, чтобы создавать прогнозы по данным в пакете. 
[Действие "Обновить ресурс" в службе машинного обучения](#machine-learning-update-resource-activity) | Со временем прогнозные модели в оценивающих экспериментах машинного обучения потребуют повторного обучения с помощью новых входных наборов данных. Когда повторное обучение будет завершено, вам потребуется обновить веб-службу оценки на основании обновленной модели машинного обучения. Чтобы обновить веб-службу с помощью заново обученной модели, можно использовать действие обновления ресурса Машинного обучения Azure.
[Действие хранимой процедуры](#stored-procedure-activity) | C помощью действия хранимой процедуры в конвейере фабрики данных можно вызвать хранимую процедуру из одного из следующих хранилищ данных: база данных SQL Azure, хранилище данных SQL Azure, база данных SQL Server вашего предприятия или виртуальная машина Azure. 
[Действие U-SQL в Data Lake Analytics](#data-lake-analytics-u-sql-activity) | Действие U-SQL Data Lake Analytics запускает сценарий U-SQL для кластера Azure Data Lake Analytics.  
[Настраиваемое действие .NET](#net-custom-activity) | Если вам нужно преобразовать данные способом, который не поддерживается фабрикой данных Azure, то можно создать настраиваемое действие с собственной логикой обработки данных и использовать это действие в конвейере. Можно настроить запуск настраиваемого действия .NET с помощью пакетной службы Azure или кластера HDInsight. 

     
## <a name="hdinsight-hive-activity"></a>Действие Hive HDInsight
В определении JSON действия Hive можно указать следующие свойства. Свойству type присваивается значение **HDInsightHive**. Сначала нужно создать связанную службу HDInsight и указать ее имя в качестве значения свойства **linkedServiceName**. В разделе **typeProperties** при установке HDInsightHive в качестве типа действия поддерживаются следующие свойства:

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| script |Указывается встроенный сценарий Hive. |Нет  |
| script path |Путь к файлу сценария Hive в хранилище BLOB-объектов Azure. Можно использовать либо свойство script, либо свойство scriptPath, но не оба сразу. В имени файла учитывается регистр знаков. |Нет  |
| defines |Параметры в виде пары "ключ-значение", ссылки на которые указываются в сценарии Hive с помощью элемента hiveconf. |Нет  |

Эти свойства type относятся к действию Hive. Другие свойства (за пределами раздела typeProperties) поддерживаются для всех действий.   

### <a name="json-example"></a>Пример JSON-файла
Ниже приведен фрагмент JSON, определяющий действие HDInsight Hive в конвейере.  

```json
{
    "name": "Hive Activity",
    "description": "description",
    "type": "HDInsightHive",
    "inputs": [
      {
        "name": "input tables"
      }
    ],
    "outputs": [
      {
        "name": "output tables"
      }
    ],
    "linkedServiceName": "MyHDInsightLinkedService",
    "typeProperties": {
      "script": "Hive script",
      "scriptPath": "<pathtotheHivescriptfileinAzureblobstorage>",
      "defines": {
        "param1": "param1Value"
      }
    },
   "scheduler": {
      "frequency": "Day",
      "interval": 1
    }
}
```

Дополнительные сведения см. в статье о [действии Hive](data-factory-hive-activity.md). 

## <a name="hdinsight-pig-activity"></a>Действие Pig HDInsight
В определении JSON действия Pig можно указать следующие свойства. Свойству type присваивается значение **HDInsightPig**. Сначала нужно создать связанную службу HDInsight и указать ее имя в качестве значения свойства **linkedServiceName**. В разделе **typeProperties** при установке HDInsightPig в качестве типа действия поддерживаются следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| script |Указывается встроенный сценарий Pig. |Нет  |
| script path |Путь к файлу сценария Pig в хранилище BLOB-объектов Azure. Можно использовать либо свойство script, либо свойство scriptPath, но не оба сразу. В имени файла учитывается регистр знаков. |Нет  |
| defines |Параметры в виде пары "ключ — значение", ссылки на которые указываются в сценарии Pig. |Нет  |

Эти свойства type относятся к действию Pig. Другие свойства (за пределами раздела typeProperties) поддерживаются для всех действий.   

### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "HiveActivitySamplePipeline",
      "properties": {
    "activities": [
        {
            "name": "Pig Activity",
            "description": "description",
            "type": "HDInsightPig",
            "inputs": [
                  {
                    "name": "input tables"
                  }
            ],
            "outputs": [
                  {
                    "name": "output tables"
                  }
            ],
            "linkedServiceName": "MyHDInsightLinkedService",
            "typeProperties": {
                  "script": "Pig script",
                  "scriptPath": "<pathtothePigscriptfileinAzureblobstorage>",
                  "defines": {
                    "param1": "param1Value"
                  }
            },
               "scheduler": {
                  "frequency": "Day",
                  "interval": 1
            }
          }
    ]
  }
}
```

Дополнительные сведения см. в статье о [действии Pig](#data-factory-pig-activity.md). 

## <a name="hdinsight-mapreduce-activity"></a>Действие MapReduce HDInsight
В определении JSON действия MapReduce можно указать следующие свойства. Свойству type присваивается значение **HDInsightMapReduce**. Сначала нужно создать связанную службу HDInsight и указать ее имя в качестве значения свойства **linkedServiceName**. В разделе **typeProperties** при установке HDInsightMapReduce в качестве типа действия поддерживаются следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| jarLinkedService | Имя связанной службы для службы хранилища Azure, содержащей JAR-файл. | Yes |
| jarFilePath | Путь к JAR-файлу в службе хранилища Azure. | Yes | 
| className | Имя основного класса в JAR-файле. | Yes | 
| arguments | Список разделенных запятыми аргументов для программы MapReduce. Во время выполнения вы увидите несколько дополнительных аргументов (например, mapreduce.job.tags) платформы MapReduce. Чтобы отличать свои аргументы от аргументов MapReduce, вы можете использовать параметр и значение в качестве аргументов, как показано в следующем примере (-s, --input, --output и т. д. — параметры, за которыми сразу следуют их значения). | Нет  | 

### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "MahoutMapReduceSamplePipeline",
    "properties": {
        "description": "Sample Pipeline to Run a Mahout Custom Map Reduce Jar. This job calculates an Item Similarity Matrix to determine the similarity between two items",
        "activities": [
            {
                "type": "HDInsightMapReduce",
                "typeProperties": {
                    "className": "org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob",
                    "jarFilePath": "adfsamples/Mahout/jars/mahout-examples-0.9.0.2.2.7.1-34.jar",
                    "jarLinkedService": "StorageLinkedService",
                    "arguments": ["-s", "SIMILARITY_LOGLIKELIHOOD", "--input", "wasb://adfsamples@spestore.blob.core.windows.net/Mahout/input", "--output", "wasb://adfsamples@spestore.blob.core.windows.net/Mahout/output/", "--maxSimilaritiesPerItem", "500", "--tempDir", "wasb://adfsamples@spestore.blob.core.windows.net/Mahout/temp/mahout"]
                },
                "inputs": [
                    {
                        "name": "MahoutInput"
                    }
                ],
                "outputs": [
                    {
                        "name": "MahoutOutput"
                    }
                ],
                "policy": {
                    "timeout": "01:00:00",
                    "concurrency": 1,
                    "retry": 3
                },
                "scheduler": {
                    "frequency": "Hour",
                    "interval": 1
                },
                "name": "MahoutActivity",
                "description": "Custom Map Reduce to generate Mahout result",
                "linkedServiceName": "HDInsightLinkedService"
            }
        ],
        "start": "2017-01-03T00:00:00",
        "end": "2017-01-04T00:00:00"
    }
}
```

Дополнительные сведения см. в статье о [действии MapReduce](data-factory-map-reduce.md). 

## <a name="hdinsight-streaming-activity"></a>Действие потоковой передачи HDInsight
В определении JSON действия потоковой передачи Hadoop можно указать следующие свойства. Свойству type присваивается значение **HDInsightStreaming**. Сначала нужно создать связанную службу HDInsight и указать ее имя в качестве значения свойства **linkedServiceName**. В разделе **typeProperties** при установке HDInsightStreaming в качестве типа действия поддерживаются следующие свойства: 

| Свойство | ОПИСАНИЕ | 
| --- | --- |
| mapper | Имя исполняемого файла средства сопоставления. В этом примере таким файлом является cat.exe.| 
| reducer | Имя исполняемого файла средства приведения. В этом примере таким файлом является wc.exe. | 
| input | Входной файл (включая расположение) для средства сопоставления. В примере "wasb://adfsample@<account name>.blob.core.windows.net/example/data/gutenberg/davinci.txt" adfsample — это контейнер BLOB-объектов, example/data/Gutenberg — это папка, а davinci.txt — это двоичный большой объект. |
| output | Выходной файл (включая расположение) для средства приведения. Результат задания потоковой передачи Hadoop записывается в расположение, заданное для этого свойства. |
| filePaths | Пути к исполняемым файлам средства сопоставления и приведения. В примере adfsample/example/apps/wc.exe adfsample — это контейнер больших двоичных объектов, example/apps — это папка, а wc.exe — это исполняемый файл. | 
| fileLinkedService | Связанная служба хранилища Azure, представляющая хранилище Azure, где хранятся указанные в разделе filePaths файлы. | 
| arguments | Список разделенных запятыми аргументов для программы MapReduce. Во время выполнения вы увидите несколько дополнительных аргументов (например, mapreduce.job.tags) платформы MapReduce. Чтобы отличать свои аргументы от аргументов MapReduce, вы можете использовать параметр и значение в качестве аргументов, как показано в следующем примере (-s, --input, --output и т. д. — параметры, за которыми сразу следуют их значения). | 
| getDebugInfo | Необязательный элемент. Если для него установлено значение Failure, журналы скачиваются только при сбое. Если для него установлено значение All, журналы скачиваются всегда, независимо от состояния выполнения. | 

> [!NOTE]
> Вам нужно указать выходной набор данных для действия потоковой передачи Hadoop для свойства **outputs**. Это может быть просто фиктивный набор данных, необходимый для соблюдения расписания конвейера (ежечасно, ежедневно и т. д.). Если действие не принимает входные данные, можно не указывать входной набор данных для действия для свойства **inputs**.  

## <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "HadoopStreamingPipeline",
    "properties": {
        "description": "Hadoop Streaming Demo",
        "activities": [
            {
                "type": "HDInsightStreaming",
                "typeProperties": {
                    "mapper": "cat.exe",
                    "reducer": "wc.exe",
                    "input": "wasb://<nameofthecluster>@spestore.blob.core.windows.net/example/data/gutenberg/davinci.txt",
                    "output": "wasb://<nameofthecluster>@spestore.blob.core.windows.net/example/data/StreamingOutput/wc.txt",
                    "filePaths": ["<nameofthecluster>/example/apps/wc.exe","<nameofthecluster>/example/apps/cat.exe"],
                    "fileLinkedService": "StorageLinkedService",
                    "getDebugInfo": "Failure"
                },
                "outputs": [
                    {
                        "name": "StreamingOutputDataset"
                    }
                ],
                "policy": {
                    "timeout": "01:00:00",
                    "concurrency": 1,
                    "executionPriorityOrder": "NewestFirst",
                    "retry": 1
                },
                "scheduler": {
                    "frequency": "Day",
                    "interval": 1
                },
                "name": "RunHadoopStreamingJob",
                "description": "Run a Hadoop streaming job",
                "linkedServiceName": "HDInsightLinkedService"
            }
        ],
        "start": "2014-01-04T00:00:00",
        "end": "2014-01-05T00:00:00"
    }
}
```

Дополнительные сведения см. в статье о [действии потоковой передачи Hadoop](data-factory-hadoop-streaming-activity.md). 

## <a name="hdinsight-spark-activity"></a>Действие HDInsight Spark
В определении JSON действия Spark можно указать следующие свойства. Свойству type присваивается значение **HDInsightSpark**. Сначала нужно создать связанную службу HDInsight и указать ее имя в качестве значения свойства **linkedServiceName**. В разделе **typeProperties** при установке HDInsightSpark в качестве типа действия поддерживаются следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
| -------- | ----------- | -------- |
| rootPath | Контейнер BLOB-объектов Azure и папка, содержащая файл Spark. В имени файла учитывается регистр знаков. | Yes |
| entryFilePath | Относительный путь к корневой папке пакета и кода Spark. | Yes |
| className | Основной класс Java или Spark приложения. | Нет  | 
| arguments | Список аргументов командной строки для программы Spark. | Нет  | 
| proxyUser | Учетная запись пользователя для олицетворения, используемая для выполнения программы Spark. | Нет  | 
| sparkConfig | Свойства конфигурации Spark. | Нет  | 
| getDebugInfo | Указывает, когда файлы журнала Spark копируются в службу хранилища Azure, используемое кластером HDInsight или определенное sparkJobLinkedService. Допустимые значения: None, Always или Failure. Значение по умолчанию: None. | Нет  | 
| sparkJobLinkedService | Связанная служба службы хранилища Azure, в которой хранятся файл задания Spark, зависимости и журналы.  Если значение этого свойства не указано, используется хранилище, связанное с кластером HDInsight. | Нет  |

### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "SparkPipeline",
    "properties": {
        "activities": [
            {
                "type": "HDInsightSpark",
                "typeProperties": {
                    "rootPath": "adfspark\\pyFiles",
                    "entryFilePath": "test.py",
                    "getDebugInfo": "Always"
                },
                "outputs": [
                    {
                        "name": "OutputDataset"
                    }
                ],
                "name": "MySparkActivity",
                "linkedServiceName": "HDInsightLinkedService"
            }
        ],
        "start": "2017-02-05T00:00:00",
        "end": "2017-02-06T00:00:00"
    }
}
```
Обратите внимание на следующие моменты. 

- Свойству **type** присваивается значение **HDInsightSpark**.
- Свойству **rootPath** присваивается значение **adfspark\\pyFiles**, где adfspark — контейнер больших двоичных объектов Azure, а pyFiles — папка с файлами в этом контейнере. В этом примере хранилище BLOB-объектов Azure связано с кластером Spark. Файл можно отправить в другое хранилище Azure. Чтобы сделать это, создайте связанную службу хранилища Azure, которая свяжет эту учетную запись хранения с фабрикой данных. Затем укажите имя связанной службы в качестве значения свойства **sparkJobLinkedService**. Сведения об этом свойстве и других свойствах, поддерживаемых действием Spark, см. в [этом разделе](#spark-activity-properties).
- Свойству **entryFilePath** присваивается значение **test.py**, которое является файлом Python. 
- Свойству **getDebugInfo** присваивается значение **Always**. Так файлы журналов будут создаваться постоянно (успешные и неудачные события).  

    > [!IMPORTANT]
    > Если вы не устраняете неполадки, мы советуем не устанавливать для этого свойства значение Always в рабочей среде. 
- В разделе **outputs** содержится один выходной набор данных. Вы должны определить выходной набор данных, даже если программа Spark не выдает выходные данные. На основе этого набора настраивается расписание конвейера (ежечасно, ежедневно и т. д.).

Дополнительные сведения о действии Spark см. в [этой статье](data-factory-spark.md).  

## <a name="machine-learning-batch-execution-activity"></a>Действие выполнения пакета в службе машинного обучения
В определении JSON действия выполнения пакета в Машинном обучении Azure можно указать следующие свойства. Свойству type действия присваивается значение **AzureMLBatchExecution**. Сначала нужно создать связанную службу машинного обучения Azure и указать ее имя в качестве значения свойства **linkedServiceName**. В разделе **typeProperties** при установке AzureMLBatchExecution в качестве типа действия поддерживаются следующие свойства:

Свойство | ОПИСАНИЕ | Обязательно 
-------- | ----------- | --------
webServiceInput | Набор данных, передаваемый в качестве входных данных для веб-службы Машинного обучения Azure. Его также следует включить в качестве входных данных для действия. |Используйте webServiceInput или webServiceInputs. | 
webServiceInputs | Укажите наборы данных, передаваемый в качестве входных данных для веб-службы Машинного обучения Azure. Если веб-служба принимает несколько входных наборов данных, используйте свойство webServiceInputs вместо webServiceInput. Наборы данных, на которые ссылается свойство **webServiceInputs**, также должны быть включены в раздел **inputs** действия. | Используйте webServiceInput или webServiceInputs. | 
webServiceOutputs | Наборы данных, присваиваемые в качестве выходных данных для веб-службы Машинного обучения Azure. Веб-служба возвращает выходные данные в этом наборе данных. | Yes | 
globalParameters | Укажите значения параметров веб-службы в этом разделе. | Нет  | 

### <a name="json-example"></a>Пример JSON-файла
В этом примере у действия имеется входной (**MLSqlInput**) и выходной наборы данных (**MLSqlOutput**). Набор **MLSqlInput** передается в веб-службу в качестве входных данных с помощью свойства JSON **webServiceInput**, а **MLSqlOutput** — в качестве выходных данных с помощью свойства JSON **webServiceOutputs**. 

```json
{
   "name": "MLWithSqlReaderSqlWriter",
   "properties": {
      "description": "Azure ML model with sql azure reader/writer",
      "activities": [{
         "name": "MLSqlReaderSqlWriterActivity",
         "type": "AzureMLBatchExecution",
         "description": "test",
         "inputs": [ { "name": "MLSqlInput" }],
         "outputs": [ { "name": "MLSqlOutput" } ],
         "linkedServiceName": "MLSqlReaderSqlWriterDecisionTreeModel",
         "typeProperties":
         {
            "webServiceInput": "MLSqlInput",
            "webServiceOutputs": {
               "output1": "MLSqlOutput"
            },
            "globalParameters": {
               "Database server name": "<myserver>.database.windows.net",
               "Database name": "<database>",
               "Server user account name": "<user name>",
               "Server user account password": "<password>"
            }              
         },
         "policy": {
            "concurrency": 1,
            "executionPriorityOrder": "NewestFirst",
            "retry": 1,
            "timeout": "02:00:00"
         }
      }],
      "start": "2016-02-13T00:00:00",
       "end": "2016-02-14T00:00:00"
   }
}
```

В примере JSON развернутая веб-служба Машинного обучения Azure использует модуль чтения и модуль записи для чтения данных из базы данных SQL Azure и записи их в нее. Эта веб-служба предоставляет следующие параметры: Database server name, Database name, Server user account name и Server user account password.

> [!NOTE]
> Только входные и выходные данные действия AzureMLBatchExecution могут передаваться как параметры веб-службы. Например, в приведенном выше фрагменте JSON MLSqlInput —входные данные действия AzureMLBatchExecution, передаваемые в качестве входных данных в веб-службу через параметр webServiceInput.

## <a name="machine-learning-update-resource-activity"></a>Действие обновления ресурса в службе машинного обучения
В определении JSON действия обновления ресурса в Машинном обучении Azure можно указать следующие свойства. Свойству type действия присваивается значение **AzureMLUpdateResource**. Сначала нужно создать связанную службу машинного обучения Azure и указать ее имя в качестве значения свойства **linkedServiceName**. В разделе **typeProperties** при установке AzureMLUpdateResource в качестве типа действия поддерживаются следующие свойства:

Свойство | ОПИСАНИЕ | Обязательно 
-------- | ----------- | --------
trainedModelName | Имя переобученной модели. | Yes |  
trainedModelDatasetName | Набор данных, указывающий на файл iLearner, возвращенный операцией повторного обучения. | Yes | 

### <a name="json-example"></a>Пример JSON-файла
Конвейер содержит два действия: **AzureMLBatchExecution** и **AzureMLUpdateResource**. Действие "Выполнение пакета" машинного обучения Azure принимает входные данные для обучения и создает выходной файл iLearner. Это действие обращается к веб-службе обучения (обучающему эксперименту, опубликованному в виде веб-службы), передает ей данные для обучения и получает файл ilearner. Набор данных placeholderBlob является фиктивным набором данных, который необходим фабрике данных Azure для запуска конвейера.


```json
{
    "name": "pipeline",
    "properties": {
        "activities": [
            {
                "name": "retraining",
                "type": "AzureMLBatchExecution",
                "inputs": [
                    {
                        "name": "trainingData"
                    }
                ],
                "outputs": [
                    {
                        "name": "trainedModelBlob"
                    }
                ],
                "typeProperties": {
                    "webServiceInput": "trainingData",
                    "webServiceOutputs": {
                        "output1": "trainedModelBlob"
                    }              
                 },
                "linkedServiceName": "trainingEndpoint",
                "policy": {
                    "concurrency": 1,
                    "executionPriorityOrder": "NewestFirst",
                    "retry": 1,
                    "timeout": "02:00:00"
                }
            },
            {
                "type": "AzureMLUpdateResource",
                "typeProperties": {
                    "trainedModelName": "trained model",
                    "trainedModelDatasetName" :  "trainedModelBlob"
                },
                "inputs": [{ "name": "trainedModelBlob" }],
                "outputs": [{ "name": "placeholderBlob" }],
                "policy": {
                    "timeout": "01:00:00",
                    "concurrency": 1,
                    "retry": 3
                },
                "name": "AzureML Update Resource",
                "linkedServiceName": "updatableScoringEndpoint2"
            }
        ],
        "start": "2016-02-13T00:00:00",
        "end": "2016-02-14T00:00:00"
    }
}
```

## <a name="data-lake-analytics-u-sql-activity"></a>Действие U-SQL в аналитике озера данных
В определении JSON действия U-SQL можно указать следующие свойства. Свойству type действия присваивается значение **DataLakeAnalyticsU-SQL**. Вам нужно создать связанную службу Azure Data Lake Analytics и указать ее имя в качестве значения свойства **linkedServiceName**. В разделе **typeProperties** при установке DataLakeAnalyticsU-SQL в качестве типа действия поддерживаются следующие свойства: 

| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| scriptPath |Путь к папке, содержащей скрипт U-SQL В имени файла учитывается регистр. |Нет (если используется скрипт) |
| scriptLinkedService |Связанная служба, которая связывает хранилище, содержащее скрипт, с фабрикой данных |Нет (если используется скрипт) |
| script |Указание сценария непосредственно в строке вместо использования scriptPath и scriptLinkedService. Например: "script" : "CREATE DATABASE test". |Нет (при использовании scriptPath и scriptLinkedService) |
| degreeOfParallelism |Максимальное количество узлов, используемых одновременно для выполнения задания. |Нет  |
| priority |Определяет, какие задания из всех в очереди должны запускаться в первую очередь. Чем меньше число, тем выше приоритет. |Нет  |
| parameters |Параметры скрипта U-SQL |Нет  |

### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "ComputeEventsByRegionPipeline",
    "properties": {
        "description": "This pipeline computes events for en-gb locale and date less than Feb 19, 2012.",
        "activities": 
        [
            {
                "type": "DataLakeAnalyticsU-SQL",
                "typeProperties": {
                    "scriptPath": "scripts\\kona\\SearchLogProcessing.txt",
                    "scriptLinkedService": "StorageLinkedService",
                    "degreeOfParallelism": 3,
                    "priority": 100,
                    "parameters": {
                        "in": "/datalake/input/SearchLog.tsv",
                        "out": "/datalake/output/Result.tsv"
                    }
                },
                "inputs": [
                    {
                        "name": "DataLakeTable"
                    }
                ],
                "outputs": 
                [
                    {
                        "name": "EventsByRegionTable"
                    }
                ],
                "policy": {
                    "timeout": "06:00:00",
                    "concurrency": 1,
                    "executionPriorityOrder": "NewestFirst",
                    "retry": 1
                },
                "scheduler": {
                    "frequency": "Day",
                    "interval": 1
                },
                "name": "EventsByRegion",
                "linkedServiceName": "AzureDataLakeAnalyticsLinkedService"
            }
        ],
        "start": "2015-08-08T00:00:00",
        "end": "2015-08-08T01:00:00",
        "isPaused": false
    }
}
```

Дополнительные сведения см. в статье о [действии U-SQL Data Lake Analytics](data-factory-usql-activity.md). 

## <a name="stored-procedure-activity"></a>Действие хранимой процедуры
В определении JSON действия хранимой процедуры можно указать следующие свойства. Свойству type действия присваивается значение **SqlServerStoredProcedure**. Создайте одну из следующих связанных служб и укажите ее имя в качестве значения свойства **linkedServiceName**:

- SQL Server; 
- Базы данных SQL Azure
- Хранилище данных SQL Azure

В разделе **typeProperties** при установке SqlServerStoredProcedure в качестве типа действия поддерживаются следующие свойства:

| Свойство | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| storedProcedureName |Укажите имя хранимой процедуры в базе данных SQL Azure или хранилище данных SQL Azure, представленной связанной службой, которую использует выходная таблица. |Yes |
| storedProcedureParameters |Указываемые значения для параметров хранимой процедуры. Если для параметра необходимо передать значение null, используйте синтаксис param1: null (все символы в нижнем регистре). Чтобы научиться использовать это свойство, см. пример ниже. |Нет  |

При указании входного набора данных он должен быть доступен (в состоянии "Готово") для выполнения действия хранимой процедуры. Входной набор данных не может потребляться в хранимой процедуре в качестве параметра. Он используется только для проверки зависимости перед запуском действия хранимой процедуры. Для действия хранимой процедуры необходимо указать выходной набор данных. 

Выходной набор данных указывает **расписание** для действия хранимой процедуры (ежечасно, еженедельно, ежемесячно и т. д.). Выходной набор данных должен использовать **связанную службу**, ссылающуюся на Базу данных SQL Azure, хранилище данных SQL Azure или базу данных SQL Server, в которых следует запускать хранимую процедуру. Выходной набор данных может использоваться для передачи результатов хранимой процедуры для дальнейшей обработки с помощью другого действия ([цепочки действий](data-factory-scheduling-and-execution.md##multiple-activities-in-a-pipeline)) в конвейере. Тем не менее фабрика данных не записывает выходные данные хранимой процедуры в этот набор данных автоматически. Выходные данные записывает сама хранимая процедура в таблицу SQL, на которую указывает выходной набор данных. В некоторых случаях выходной набор данных может быть **фиктивным набором данных**. Он используется, только чтобы задать расписание выполнения действия хранимой процедуры.  

### <a name="json-example"></a>Пример JSON-файла

```json
{
    "name": "SprocActivitySamplePipeline",
    "properties": {
        "activities": [
            {
                "type": "SqlServerStoredProcedure",
                "typeProperties": {
                    "storedProcedureName": "sp_sample",
                    "storedProcedureParameters": {
                        "DateTime": "$$Text.Format('{0:yyyy-MM-dd HH:mm:ss}', SliceStart)"
                    }
                },
                "outputs": [{ "name": "sprocsampleout" }],
                "name": "SprocActivitySample"
            }
        ],
         "start": "2016-08-02T00:00:00",
         "end": "2016-08-02T05:00:00",
        "isPaused": false
    }
}
```

Дополнительные сведения см. в статье о [действии хранимой процедуры](data-factory-stored-proc-activity.md). 

## <a name="net-custom-activity"></a>Настраиваемое действие .NET
В определении JSON настраиваемого действия .NET можно указать следующие свойства. Свойству type присваивается значение **DotNetActivity**. Создайте связанную службу Azure HDInsight или связанную пакетную службу Azure и укажите ее имя в качестве значения свойства **linkedServiceName**. В разделе **typeProperties** при установке DotNetActivity в качестве типа действия поддерживаются следующие свойства:
 
| Свойство | ОПИСАНИЕ | Обязательно |
|:--- |:--- |:--- |
| AssemblyName | Имя сборки. В этом примере это **MyDotnetActivity.dll**. | Yes |
| EntryPoint |Имя класса, реализующего интерфейс IDotNetActivity. В примере это **MyDotNetActivityNS.MyDotNetActivity**, где MyDotNetActivityNS — это пространство имен, а MyDotNetActivity — класс.  | Yes | 
| PackageLinkedService | Имя связанной службы хранилища Azure, указывающей на хранилище BLOB-объектов с ZIP-файлом настраиваемого действия. В этом примере это **AzureStorageLinkedService**.| Yes |
| PackageFile | Имя ZIP-файла. В этом примере это **customactivitycontainer/MyDotNetActivity.zip**. | Yes |
| extendedProperties | Расширенные свойства, которые можно определить и передать в код .NET. В этом примере переменной **SliceStart** присваивается значение на основе системной переменной SliceStart. | Нет  | 

### <a name="json-example"></a>Пример JSON-файла

```json
{
  "name": "ADFTutorialPipelineCustom",
  "properties": {
    "description": "Use custom activity",
    "activities": [
      {
        "Name": "MyDotNetActivity",
        "Type": "DotNetActivity",
        "Inputs": [
          {
            "Name": "InputDataset"
          }
        ],
        "Outputs": [
          {
            "Name": "OutputDataset"
          }
        ],
        "LinkedServiceName": "AzureBatchLinkedService",
        "typeProperties": {
          "AssemblyName": "MyDotNetActivity.dll",
          "EntryPoint": "MyDotNetActivityNS.MyDotNetActivity",
          "PackageLinkedService": "AzureStorageLinkedService",
          "PackageFile": "customactivitycontainer/MyDotNetActivity.zip",
          "extendedProperties": {
            "SliceStart": "$$Text.Format('{0:yyyyMMddHH-mm}', Time.AddMinutes(SliceStart, 0))"
          }
        },
        "Policy": {
          "Concurrency": 2,
          "ExecutionPriorityOrder": "OldestFirst",
          "Retry": 3,
          "Timeout": "00:30:00",
          "Delay": "00:00:00"
        }
      }
    ],
    "start": "2016-11-16T00:00:00",
    "end": "2016-11-16T05:00:00",
    "isPaused": false
  }
}
```

Подробные сведения см. в статье об [использовании настраиваемых действий в фабрике данных](data-factory-use-custom-activities.md). 

## <a name="next-steps"></a>Дальнейшие действия
Ознакомьтесь со следующими руководствами: 

- [Руководство. Создание конвейера с действием копирования с помощью портала Azure](data-factory-copy-activity-tutorial-using-azure-portal.md)
- [Руководство. Создание первой фабрики данных Azure с помощью портала Azure](data-factory-build-your-first-pipeline-using-editor.md)
