---
title: "Копирование данных в хранилище BLOB-объектов Azure и из него | Документация Майкрософт"
description: "Как копировать данные BLOB-объектов в фабрике данных Azure. Используйте наш пример: как копировать данные в базу данных SQL Azure и хранилище BLOB-объектов Azure и обратно."
services: data-factory
documentationcenter: 
author: linda33wj
manager: jhubbard
editor: monicar
ms.assetid: bec8160f-5e07-47e4-8ee1-ebb14cfb805d
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 01/05/2018
ms.author: jingwang
robots: noindex
ms.openlocfilehash: f66ddecd6b999400b05a4b00aa781ffef3f7887d
ms.sourcegitcommit: 1d423a8954731b0f318240f2fa0262934ff04bd9
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/05/2018
---
# <a name="copy-data-to-or-from-azure-blob-storage-using-azure-data-factory"></a>Копирование данных в хранилище BLOB-объектов Azure и обратно с помощью фабрики данных Azure
> [!div class="op_single_selector" title1="Select the version of Data Factory service you are using:"]
> * [Версия 1 — общедоступная](data-factory-azure-blob-connector.md)
> * [Версия 2 — предварительная](../connector-azure-blob-storage.md)

> [!NOTE]
> Статья относится к версии 1 фабрики данных, которая является общедоступной версией. Если вы используете версию 2 службы фабрики данных, которая находится на этапе предварительной версии, см. статью [Копирование данных из хранилища BLOB-объектов Azure и обратно с помощью фабрики данных Azure](../connector-azure-blob-storage.md).


В этой статье объясняется, как с помощью действия копирования в фабрике данных Azure копировать данные в хранилище BLOB-объектов Azure и обратно. Этот документ является продолжением статьи о [действиях перемещения данных](data-factory-data-movement-activities.md), в которой приведены общие сведения о перемещении данных с помощью действия копирования.

## <a name="overview"></a>Обзор
Данные можно скопировать из любого поддерживаемого в качестве источника хранилища данных в хранилище BLOB-объектов Azure или из хранилища BLOB-объектов Azure в любое поддерживаемое в качестве приемника хранилище данных. В следующей таблице приведен список хранилищ данных, которые поддерживаются в качестве источников и приемников для действия копирования. Например, вы можете переместить данные **из** базы данных SQL Server или SQL Azure **в** хранилище BLOB-объектов Azure. Вы также можете копировать данные **из** хранилища BLOB-объектов Azure **в** хранилище данных SQL Azure или в коллекцию Azure Cosmos DB. 

## <a name="supported-scenarios"></a>Поддерживаемые сценарии использования.
Данные можно скопировать **из хранилища BLOB-объектов Azure** в следующие хранилища данных:

[!INCLUDE [data-factory-supported-sink](../../../includes/data-factory-supported-sinks.md)]

Данные можно скопировать **в хранилище BLOB-объектов Azure** из следующих хранилищ данных:

[!INCLUDE [data-factory-supported-sources](../../../includes/data-factory-supported-sources.md)]
 
> [!IMPORTANT]
> Действие копирования поддерживает копирование данных как в учетные записи хранения Azure общего назначения и в хранилище горячих и холодных BLOB-объектов, так и из них. Это действие поддерживает **чтение данных из блочных, добавочных или страничных BLOB-объектов**, однако **запись поддерживается только в блочные BLOB-объекты**. Хранилище Azure класса Premium нельзя использовать в качестве приемника, так как оно поддерживается страничными BLOB-объектами.
> 
> Действие копирования не удаляет данные из источника после их успешного копирования в место назначения. Если необходимо удалить исходные данные после успешного копирования, создайте [настраиваемое действие](data-factory-use-custom-activities.md) для удаления данных и используйте это действие в конвейере. Пример см. в [образце действия удаления большого двоичного объекта или папки на GitHub](https://github.com/Azure/Azure-DataFactory/tree/master/Samples/DeleteBlobFileFolderCustomActivity). 

## <a name="get-started"></a>Начало работы
Можно создать конвейер с действием копирования, которое перемещает данные из хранилища BLOB-объектов Azure или в него с помощью различных инструментов и интерфейсов API.

Проще всего создать конвейер с помощью **мастера копирования**. Эта статья содержит [пошаговое руководство](#walkthrough-use-copy-wizard-to-copy-data-tofrom-blob-storage) для создания конвейера, который копирует данные из расположения хранилища BLOB-объектов Azure в другое расположение хранилища BLOB-объектов Azure. Инструкции по созданию конвейера для копирования данных из хранилища BLOB-объектов Azure в базу данных SQL Azure см. в статье [Руководство. Создание конвейера с действием копирования с помощью мастера копирования фабрики данных](data-factory-copy-data-wizard-tutorial.md).

Также для создания конвейера можно использовать следующие инструменты: **портал Azure**, **Visual Studio**, **Azure PowerShell**, **шаблон Azure Resource Manager**, **API .NET** и **REST API**. Пошаговые инструкции по созданию конвейера с действием копирования см. в [руководстве по действию копирования](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).

Независимо от используемого средства или API-интерфейса, для создания конвейера, который перемещает данные из источника данных в приемник, выполняются следующие шаги:

1. Создание **фабрики данных**. Фабрика данных может содержать один или несколько конвейеров. 
2. Создайте **связанные службы**, чтобы связать входные и выходные данные с фабрикой данных. Например, при копировании данных из хранилища BLOB-объектов Azure в базу данных SQL Azure создайте две связанные службы, чтобы связать учетную запись хранения Azure и базу данных SQL Azure с фабрикой данных. Сведения о свойствах связанной службы, относящихся к хранилищу BLOB-объектов Azure, см. в разделе [Свойства связанной службы](#linked-service-properties). 
2. Создайте **наборы данных**, которые представляют входные и выходные данные для операции копирования. В примере, упомянутом на последнем этапе, создайте набор данных, чтобы указать контейнер BLOB-объектов и папку, содержащую входные данные. Также создайте другой набор данных, чтобы указать таблицу SQL в базе данных SQL Azure, содержащую данные, скопированные из хранилища BLOB-объектов. Сведения о свойствах набора данных, относящихся к хранилищу BLOB-объектов Azure, см. в разделе [Свойства набора данных](#dataset-properties).
3. Создайте **конвейер** с действием копирования, который принимает входной набор данных и возвращает выходной набор данных. В примере выше BlobSource используется как источник, а SqlSink — как приемник для действия копирования. Точно так же при копировании из базы данных SQL Azure в хранилище BLOB-объектов Azure в действии копирования используются SqlSource и BlobSink. Сведения о свойствах действия копирования, относящихся к хранилищу BLOB-объектов Azure, см. в разделе [Свойства действия копирования](#copy-activity-properties). Для получения сведений о том, как использовать хранилище данных в качестве источника или приемника, щелкните ссылку в предыдущем разделе об источнике данных.  

Если вы используете мастер, то он автоматически создает определения JSON для сущностей фабрики данных (связанных служб, наборов данных и конвейера). При использовании инструментов и интерфейсов API (за исключением API .NET) вы самостоятельно определяете эти сущности фабрики данных в формате JSON.  Примеры с определениями JSON для сущностей фабрики данных, которые используются для копирования данных из хранилища BLOB-объектов Azure и обратно, см. в разделе [Примеры определений JSON](#json-examples-for-copying-data-to-and-from-blob-storage  ) этой статьи.

Следующие разделы содержат сведения о свойствах JSON, которые используются для определения сущностей фабрики данных, характерных для хранилища BLOB-объектов Azure.

## <a name="linked-service-properties"></a>Свойства связанной службы
Для связи хранилища Azure с фабрикой данных Azure можно использовать два типа связанных служб: **AzureStorage** и **AzureStorageSas**. Связанная служба хранилища Azure предоставляет фабрике данных глобальный доступ к хранилищу Azure, а связанная служба SAS хранилища Azure — ограниченный или привязанный ко времени доступ к хранилищу Azure. Других различий между этими связанными службами нет. Выберите связанную службу, соответствующую вашим задачам. Более подробно эти службы описаны в следующих разделах.

[!INCLUDE [data-factory-azure-storage-linked-services](../../../includes/data-factory-azure-storage-linked-services.md)]

## <a name="dataset-properties"></a>Свойства набора данных
Чтобы указать набор данных, представляющий входные или выходные данные в хранилище BLOB-объектов Azure, укажите для свойства типа набора данных значение **AzureBlob**. Для свойства **linkedServiceName** набора данных задайте имя связанной службы хранилища Azure или SAS службы хранилища Azure.  В свойствах типа набора данных указывается **контейнер больших двоичных объектов** и **папка** в хранилище BLOB-объектов.

Полный список разделов и свойств JSON, используемых для определения наборов данных, см. в статье [Создание наборов данных](data-factory-create-datasets.md). Разделы структуры, доступности и политики JSON набора данных одинаковы для всех типов наборов данных (SQL Azure, большие двоичные объекты Azure, таблицы Azure и т. д.).

Фабрика данных поддерживает следующие CLS-совместимые значения типов на основе .NET, когда нужно указывать сведения о типах в разделе structure для схемы по считываемым источникам данных, таким как большие двоичные объекты Azure: Int16, Int32, Int64, Single, Double, Decimal, Byte[], Bool, String, Guid, Datetime, Datetimeoffset, Timespan. Фабрика данных автоматически преобразует типы при переносе данных из источника в приемник.

Раздел **typeProperties** отличается для разных типов наборов данных. Он содержит сведения о расположении данных в хранилище данных, формате данных и т. д. Раздел typeProperties набора данных типа **AzureBlob** содержит следующие свойства.

| Свойство | ОПИСАНИЕ | Требуется |
| --- | --- | --- |
| folderPath |Путь контейнеру и папке в хранилище BLOB-объектов. Пример: myblobcontainer\myblobfolder\ |Yes |
| fileName |Имя большого двоичного объекта. Свойство fileName является необязательным и в нем учитывается регистр знаков.<br/><br/>Если указать имя файла, то действие (включая копирование) работает с определенным большим двоичным объектом.<br/><br/>Если значение fileName не указано, то копируются все большие двоичные объекты в folderPath для входного набора данных.<br/><br/>Если свойство **fileName** не указано для выходного набора данных, а свойство **preserveHierarchy** не указано в приемнике действия, имя созданного файла будет иметь следующий формат: Data.<Guid>.txt (например, Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt). |Нет  |
| partitionedBy |Необязательное свойство. Его можно использовать, чтобы указать динамические путь к папке и имя файла для временного ряда данных. Например, путь к папке (значение folderPath) каждый час может быть другим. Дополнительные сведения и примеры см. ниже в разделе [Использование свойства partitionedBy](#using-partitionedBy-property). |Нет  |
| свойства | Поддерживаются следующие типы формата: **TextFormat**, **JsonFormat**, **AvroFormat**, **OrcFormat**, **ParquetFormat**. Свойству **type** в разделе format необходимо присвоить одно из этих значений. Дополнительные сведения см. в разделах о [текстовом формате](data-factory-supported-file-and-compression-formats.md#text-format), [формате Json](data-factory-supported-file-and-compression-formats.md#json-format), [формате Avro](data-factory-supported-file-and-compression-formats.md#avro-format), [формате Orc](data-factory-supported-file-and-compression-formats.md#orc-format) и [ формате Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format). <br><br> Если требуется скопировать файлы между файловыми хранилищами **как есть** (двоичное копирование), можно пропустить раздел форматирования в определениях входного и выходного наборов данных. |Нет  |
| compression | Укажите тип и уровень сжатия данных. Поддерживаемые типы: **GZip**, **Deflate**, **BZip2** и **ZipDeflate**. Поддерживаемые уровни: **Optimal** и **Fastest**. Узнайте больше о [форматах файлов и сжатия данных в фабрике данных Azure](data-factory-supported-file-and-compression-formats.md#compression-support). |Нет  |

### <a name="using-partitionedby-property"></a>Использование свойства partitionedBy
Как сказано выше, для данных временных рядов путь к папке (folderPath) и имя файла (fileName) можно указывать динамически. Это делается с помощью свойства **partitionedBy**, [функций фабрики данных и системных переменных](data-factory-functions-variables.md).

Дополнительные сведения о наборах данных временных рядов, планировании и срезах см. в статьях [Наборы данных в фабрике данных Azure](data-factory-create-datasets.md) и [Планирование и исполнение с использованием фабрики данных](data-factory-scheduling-and-execution.md).

#### <a name="sample-1"></a>Пример 1

```json
"folderPath": "wikidatagateway/wikisampledataout/{Slice}",
"partitionedBy":
[
    { "name": "Slice", "value": { "type": "DateTime", "date": "SliceStart", "format": "yyyyMMddHH" } },
],
```

В этом примере {Slice} заменяется значением SliceStart (системная переменная фабрики данных) в формате ГГГГММДДЧЧ. SliceStart указывает время начала среза. Значение folderPath отличается для каждого среза. Например: wikidatagateway/wikisampledataout/2014100103 или wikidatagateway/wikisampledataout/2014100104.

#### <a name="sample-2"></a>Пример 2

```json
"folderPath": "wikidatagateway/wikisampledataout/{Year}/{Month}/{Day}",
"fileName": "{Hour}.csv",
"partitionedBy":
 [
    { "name": "Year", "value": { "type": "DateTime", "date": "SliceStart", "format": "yyyy" } },
    { "name": "Month", "value": { "type": "DateTime", "date": "SliceStart", "format": "MM" } },
    { "name": "Day", "value": { "type": "DateTime", "date": "SliceStart", "format": "dd" } },
    { "name": "Hour", "value": { "type": "DateTime", "date": "SliceStart", "format": "hh" } }
],
```

В этом примере год, месяц, день и время SliceStart извлекаются в отдельные переменные, используемые в свойствах folderPath и fileName.

## <a name="copy-activity-properties"></a>Свойства действия копирования
Полный список разделов и свойств, используемых для определения действий, см. в статье [Создание конвейеров](data-factory-create-pipelines.md). Свойства, например имя, описание, входные и выходные таблицы, политики и т. д., доступны для всех типов действий. В свою очередь свойства, доступные в разделе **typeProperties** действия, зависят от конкретного типа действия. Для действия копирования они различаются в зависимости от типов источников и приемников. При перемещении данных из хранилища BLOB-объектов Azure в действии копирования задается тип источника **BlobSource**. Аналогично, при перемещении данных в хранилище BLOB-объектов Azure в действии копирования задается тип приемника **BlobSink**. Этот раздел содержит список свойств, поддерживаемых типами BlobSource и BlobSink.

Для **BlobSource** в разделе **typeProperties** могут быть указаны следующие свойства.

| Свойство | ОПИСАНИЕ | Допустимые значения | Требуется |
| --- | --- | --- | --- |
| recursive |Указывает, следует ли читать данные рекурсивно из вложенных папок или только из указанной папки. |True (значение по умолчанию), False |Нет  |

Для **BlobSink** в разделе **typeProperties** могут быть указаны следующие свойства.

| Свойство | ОПИСАНИЕ | Допустимые значения | Требуется |
| --- | --- | --- | --- |
| copyBehavior |Это свойство определяет поведение функции копирования, когда в качестве источника используется BlobSource или FileSystem. |<b>PreserveHierarchy:</b> сохраняет иерархию файлов в целевой папке. Относительный путь исходного файла в исходной папке идентичен относительному пути целевого файла в целевой папке.<br/><br/><b>FlattenHierarchy</b>: все файлы из исходной папки размещаются на первом уровне в целевой папке. Целевые файлы имеют автоматически сформированное имя. <br/><br/><b>MergeFiles</b>: объединяет все файлы из исходной папки в один файл. Если указано имя файла или большого двоичного объекта, именем объединенного файла будет указанное имя; в противном случае имя файла будет автоматически сформировано. |Нет  |

**BlobSource** также поддерживает эти два свойства для обеспечения обратной совместимости.

* **treatEmptyAsNull**указывает, следует ли интерпретировать null или пустую строку как значение null.
* **skipHeaderLineCount** указывает, сколько строк необходимо пропустить. Применяется, только когда для входного набора данных используется TextFormat.

Аналогичным образом **BlobSink** поддерживает следующее свойство для обеспечения обратной совместимости.

* **blobWriterAddHeader**указывает, следует ли добавлять заголовок определений столбцов при записи в выходной набор данных.

Наборы данных поддерживают теперь такие свойства, которые реализуют те же функции: **treatEmptyAsNull**, **skipLineCount**, **firstRowAsHeader**.

В таблице ниже приведены инструкции по использованию новых свойств набора данных вместо указанных свойств источника и приемника больших двоичных объектов.

| Свойство "Действие копирования" | Свойство "Набор данных" |
|:--- |:--- |
| skipHeaderLineCount в BlobSource |skipLineCount и firstRowAsHeader. Сначала строки пропускаются, а затем первая строка считывается как заголовок. |
| treatEmptyAsNull в BlobSource |treatEmptyAsNull во входном наборе данных |
| blobWriterAddHeader в BlobSink |firstRowAsHeader в выходном наборе данных |

Подробную информацию об этих свойствах см. в разделе [Определение TextFormat](data-factory-supported-file-and-compression-formats.md#text-format).    

### <a name="recursive-and-copybehavior-examples"></a>Примеры recursive и copyBehavior
В данном разделе описываются результаты выполнения операции копирования при использовании различных сочетаний значений recursive и copyBehavior.

| recursive | copyBehavior | Результаты выполнения операции |
| --- | --- | --- |
| Да |preserveHierarchy |Если у исходной папки "Папка1" такая структура:  <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5<br/><br/>Целевая папка "Папка1" создается с такой же структурой, как и исходная папка.<br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5 |
| Да |flattenHierarchy |Если у исходной папки "Папка1" такая структура:  <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5<br/><br/>Целевая папка "Папка1" создается со следующей структурой: <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Автоматически созданное имя для "Файл1"<br/>&nbsp;&nbsp;&nbsp;&nbsp;Автоматически созданное имя для "Файл2"<br/>&nbsp;&nbsp;&nbsp;&nbsp;Автоматически созданное имя для "Файл3"<br/>&nbsp;&nbsp;&nbsp;&nbsp;Автоматически созданное имя для "Файл4"<br/>&nbsp;&nbsp;&nbsp;&nbsp;Автоматически созданное имя для "Файл5" |
| Да |mergeFiles |Если у исходной папки "Папка1" такая структура:  <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5<br/><br/>Целевая папка "Папка1" создается со следующей структурой: <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Содержимое файлов "Файл1", "Файл2", "Файл3", "Файл4" и "Файл5" объединяется в один файл с автоматически созданным именем. |
| false |preserveHierarchy |Если у исходной папки "Папка1" такая структура:  <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5<br/><br/>Целевая папка "Папка1" создается со следующей структурой:<br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/><br/><br/>Папка "Вложенная_папка1" с файлами "Файл3", "Файл4" и "Файл5" не будет включена в эту папку. |
| false |flattenHierarchy |Если у исходной папки "Папка1" такая структура: <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5<br/><br/>Целевая папка "Папка1" создается со следующей структурой:<br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Автоматически созданное имя для "Файл1"<br/>&nbsp;&nbsp;&nbsp;&nbsp;Автоматически созданное имя для "Файл2"<br/><br/><br/>Папка "Вложенная_папка1" с файлами "Файл3", "Файл4" и "Файл5" не будет включена в эту папку. |
| false |mergeFiles |Если у исходной папки "Папка1" такая структура: <br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Файл2<br/>&nbsp;&nbsp;&nbsp;&nbsp;Вложенная_папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл3<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл4<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Файл5<br/><br/>Целевая папка "Папка1" создается со следующей структурой:<br/><br/>Папка1<br/>&nbsp;&nbsp;&nbsp;&nbsp;Содержимое файлов "Файл1" и "Файл2" объединяется в один файл с автоматически созданным именем. Автоматически созданное имя для "Файл1"<br/><br/>Папка "Вложенная_папка1" с файлами "Файл3", "Файл4" и "Файл5" не будет включена в эту папку. |

## <a name="walkthrough-use-copy-wizard-to-copy-data-tofrom-blob-storage"></a>Пошаговое руководство. Копирование данных из хранилища больших двоичных объектов и в него с помощью мастера копирования
Давайте посмотрим, как быстро скопировать данные из хранилища BLOB-объектов Azure. В этом пошаговом руководстве используются исходное и целевое хранилища данных типа хранилище BLOB-объектов Azure. Конвейер в этом пошаговом руководстве копирует данные из одной папки в другую в пределах одного контейнера больших двоичных объектов. Это пошаговое руководстве упрощено, чтобы показать параметры или свойства при использовании хранилища больших двоичных объектов в качестве источника или приемника. 

### <a name="prerequisites"></a>Технические условия
1. Создайте **учетную запись хранения Azure** общего назначения, если у вас ее нет. В этом пошаговом руководстве хранилище больших объектов используется как **исходное** и **целевое** хранилище данных. в статье [Об учетных записях хранения Azure](../../storage/common/storage-create-storage-account.md#create-a-storage-account) .
2. Создайте контейнер больших двоичных объектов **adfblobconnector** в учетной записи хранения. 
4. Создайте папку **input** в контейнере **adfblobconnector**.
5. Создайте файл **emp.txt** со следующим содержимым и передайте его в папку **input**, используя такие средства, как [обозреватель хранилищ Azure](https://azurestorageexplorer.codeplex.com/).
    ```json
    John, Doe
    Jane, Doe
    ```
### <a name="create-the-data-factory"></a>Создание фабрики данных
1. Войдите на [портале Azure](https://portal.azure.com).
2. Нажмите кнопку **+ Создать** в верхнем левом углу, выберите **Аналитика** и щелкните **Фабрика данных**.
3. В колонке **Создать фабрику данных** выполните следующие действия.   
    1. Введите **ADFBlobConnectorDF** в качестве **имени**. Имя фабрики данных Azure должно быть глобально уникальным. При возникновении ошибки `*Data factory name “ADFBlobConnectorDF” is not available` измените имя фабрики данных (например, на ваше_имя_ADFBlobConnectorDF) и попробуйте создать фабрику данных снова. Ознакомьтесь с разделом [Фабрика данных — правила именования](data-factory-naming-rules.md) , чтобы узнать о правилах именования артефактов фабрики данных.
    2. Выберите свою **подписку Azure**.
    3. Для группы ресурсов выберите **Использовать существующий**, чтобы выбрать существующую группу ресурсов, или **Создать** для ввода имени группы ресурсов.
    4. Укажите **расположение** фабрики данных.
    5. Установите флажок **Закрепить на панели мониторинга** в нижней части колонки.
    6. Нажмите кнопку **Создать**.
3. После создания вы увидите колонку **Фабрика данных**, как показано на рисунке ниже: ![Домашняя страница фабрики данных](./media/data-factory-azure-blob-connector/data-factory-home-page.png)

### <a name="copy-wizard"></a>Мастер копирования
1. Чтобы запустить **мастер копирования данных** в отдельной вкладке, на домашней странице фабрики данных щелкните **Copy data [preview]** (Копирование данных [предварительная версия]).    
    
    > [!NOTE]
    >    Если веб-браузер завис на действии "Авторизация...", отключите параметр или снимите флажок **Block third party cookies and site data** (Блокировать сторонние файлы cookie и данные сайта). Либо оставьте флажок и создайте исключение для адреса **login.microsoftonline.com**, а затем попробуйте запустить мастер еще раз.
2. Вот что нужно сделать на странице **Свойства** :
    1. Введите **CopyPipeline** в качестве **имени задачи**. Имя задачи — это имя конвейера в фабрике данных.
    2. (Необязательно.) Введите **описание** задачи.
    3. В разделе **Task cadence or Task schedule** (Периодичность или расписание задач) оставьте переключатель **Run regularly on schedule** (Регулярное выполнение по расписанию). Если вы хотите, чтобы эта задача выполнилась только один раз, а не выполнялась постоянно по расписанию, выберите **Run once now** (Выполнить один раз). Если выбрать переключатель **Run once now** (Выполнить один раз), будет создан [одноразовый конвейер](data-factory-create-pipelines.md#onetime-pipeline). 
    4. Сохраните параметры для **шаблона повторения**. Эта задача выполняется ежедневно в соответствии со значениями времени начала и окончания, которые будут указаны на следующем шаге.
    5. Измените **время и дату начала** на **21.04.2017**. 
    6. Измените **время и дату окончания** на **25.04.2017**. Вы можете ввести дату, а не искать ее в календаре.     
    8. Нажмите кнопку **Далее**.
      ![Средство копирования — страница "Свойства"](./media/data-factory-azure-blob-connector/copy-tool-properties-page.png) 
3. На странице **Source data store** (Хранилище данных источников) щелкните плитку **Хранилище BLOB-объектов Azure**. На этой странице можно указать хранилище данных источников для задачи копирования. Вы можете использовать существующую связанную службу хранилища данных или указать новое хранилище данных. Чтобы использовать имеющуюся связанную службу, щелкните **From existing linked services** (Из существующих связанных служб) и выберите нужную службу. 
    ![Средство копирования — страница "Исходное хранилище данных"](./media/data-factory-azure-blob-connector/copy-tool-source-data-store-page.png)
4. Вот что нужно сделать на странице **Specify the Azure Blob storage account** (Указание учетной записи хранилища BLOB-объектов Azure).
   1. Оставьте автоматически созданное имя в поле **Имя подключения**. Имя подключения — это имя связанной службы типа служба хранилища Azure. 
   2. Убедитесь, что в качестве **метода выбора учетной записи** выбран вариант **From Azure subscriptions** (Из подписок Azure).
   3. В поле **Подписка Azure** выберите свою подписку Azure или оставьте значение **Select all** (Выбрать все).   
   4. Выберите **учетную запись хранения Azure** из списка учетных записей хранения Azure, доступных в выбранной вами подписке. Кроме того, вы можете вручную настроить параметры учетной записи хранения. Для этого в качестве **метода выбора учетной записи** выберите вариант **Ввести вручную**.
   5. Нажмите кнопку **Далее**. 
      ![Средство копирования — указание учетной записи хранилища BLOB-объектов Azure](./media/data-factory-azure-blob-connector/copy-tool-specify-azure-blob-storage-account.png)
5. Вот что нужно сделать на странице **Choose the input file or folder** (Выбор входного файла или папки).
   1. Дважды щелкните **adfblobcontainer**.
   2. Выберите **input** и нажмите кнопку **Выбрать**. В этом пошаговом руководстве вы выберете папку input. Кроме того, можно выбрать файл emp.txt в папке. 
      ![Средство копирования — выбор входного файла или папки](./media/data-factory-azure-blob-connector/copy-tool-choose-input-file-or-folder.png)
6. Вот что нужно сделать на странице **Choose the input file or folder** (Выбор входного файла или папки).
    1. Убедитесь, что в качестве **файла или папки** выбрано **adfblobconnector или input**. Если файлы находятся во вложенных папках, например 2017/04/01, 2017/04/02 и т. д., введите adfblobconnector/input/{год}/{месяц}/{день} в качестве файла или папки. При нажатии клавиши TAB за пределами текстового поля отображаются три раскрывающихся списка для выбора формата года (гггг), месяца (мм) и дня (дд). 
    2. Не устанавливайте флажок **Copy file recursively** (Копировать файл рекурсивно). Выберите этот параметр, чтобы рекурсивно переключаться между папками для файлов, копируемых в место назначения. 
    3. Не устанавливайте флажок **Binary copy** (Двоичное копирование). Выберите этот параметр, чтобы выполнить двоичное копирование исходного файла в место назначения. Не устанавливайте его для этого пошагового руководства, чтобы на следующих страницах отобразились дополнительные параметры. 
    4. Убедитесь, что для параметра **Compression type** (Тип сжатия) задано значение **Нет**. Выберите значение для этого параметра, если исходные файлы сжимаются с использованием одного из поддерживаемых форматов. 
    5. Нажмите кнопку **Далее**.
    ![Средство копирования — выбор входного файла или папки](./media/data-factory-azure-blob-connector/chose-input-file-folder.png) 
7. На странице **File format settings** (Параметры формата файла) отображаются разделители и схема, которые мастер определяет автоматически при анализе файла. 
    1. Проверьте следующие параметры: a. Убедитесь, что для **формата файла** задано значение **Текстовый формат**. Все поддерживаемые форматы находятся в раскрывающемся списке. Например, JSON, Avro, ORC, Parquet.
        Б. Убедитесь, что в качестве **разделителя столбцов** используется `Comma (,)`. Другие разделители столбцов, поддерживаемые фабрикой данных, находятся в раскрывающемся списке. Вы также можете указать пользовательский разделитель.
        c. Убедитесь, что в качестве **разделителя строк** используется `Carriage Return + Line feed (\r\n)`. Другие разделители строк, поддерживаемые фабрикой данных, находятся в раскрывающемся списке. Вы также можете указать пользовательский разделитель.
        d. Убедитесь, что для **пропуска указанного количества строк** задано значение **0**. Если требуется пропустить несколько строк в верхней части файла, введите их количество здесь.
        д.  Убедитесь, что флажок **The first data row contains column names** (Первая строка данных содержит имена столбцов) не установлен. Если исходные файлы содержат имена столбцов в первой строке, выберите этот параметр.
        f. Убедитесь, что установлен флажок **Treat empty column value as null** (Значение пустого столбца рассматривается как null).
    2. Разверните раздел **Дополнительные параметры** для просмотра доступных дополнительных параметров.
    3. В нижней части страницы просмотрите **предварительную версию** данных из файла emp.txt.
    4. Щелкните вкладку **Схема** внизу, чтобы увидеть схему, выведенную мастером копирования путем просмотра данных в исходном файле.
    5. Проверив разделители и просмотрев предварительные данные, нажмите кнопку **Далее**.
    ![Средство копирования — параметры формата файла](./media/data-factory-azure-blob-connector/copy-tool-file-format-settings.png)  
8. На странице **целевого хранилища данных** выберите **Хранилище BLOB-объектов Azure** и нажмите кнопку **Далее**. В этом пошаговом руководстве хранилище BLOB-объектов Azure используется как исходное и целевое хранилище данных.    
    ![Мастер копирования — выбор целевого хранилища данных](media/data-factory-azure-blob-connector/select-destination-data-store.png)
9. Вот что нужно сделать на странице **Specify the Azure Blob storage account** (Указание учетной записи хранилища BLOB-объектов Azure).
   1. В поле **Имя подключения** введите **AzureStorageLinkedService**.
   2. Убедитесь, что в качестве **метода выбора учетной записи** выбран вариант **From Azure subscriptions** (Из подписок Azure).
   3. Выберите свою **подписку Azure**.  
   4. Выберите свою учетную запись хранения Azure. 
   5. Нажмите кнопку **Далее**.     
10. Вот что нужно сделать на странице **Choose the output file or folder** (Выбор выходного файла или папки). 
    6. В поле **Путь к папке** укажите **adfblobconnector/output/{год}/{месяц}/{день}**. Нажмите клавишу **TAB**.
    7. В качестве **года** выберите **yyyy**.
    8. Убедитесь, что для **месяца** задано значение **ММ**.
    9. Убедитесь, что для **дня** задано значение **Д**.
    10. Убедитесь, что для параметра **Compression type** (Тип сжатия) задано значение **Нет**.
    11. Убедитесь, что для параметра **Copy behavior** (Поведение копирования) задано значение **Merge files** (Объединение файлов). Если выходной файл с тем же именем уже существует, новое содержимое добавляется в тот же файл в конце.
    12. Нажмите кнопку **Далее**.
    ![Средство копирования — выбор выходного файла или папки](media/data-factory-azure-blob-connector/choose-the-output-file-or-folder.png)
11. На странице **File format settings** (Параметры формата файла) проверьте параметры и нажмите кнопку **Далее**. Кроме того, здесь вы можете добавить заголовок в выходной файл. Если выбрать этот параметр, добавляется строка заголовка с именами столбцов из схемы источника. При просмотре схемы для источника вы можете изменить имена столбцов по умолчанию. Например, можно изменить имя первого столбца на "Имя", а второго — на "Фамилия". Затем создается выходной файл с заголовком и с этими именами в качестве имен столбцов. 
    ![Средство копирования — параметры формата файла для назначения](media/data-factory-azure-blob-connector/file-format-destination.png)
12. На странице **Performance settings** (Параметры производительности) убедитесь, что для параметров **Cloud units** (Единицы облака) и **Parallel copies** (Параллельные копии) задано значение **Автоматически**, и нажмите кнопку "Далее". Дополнительные сведения об этих параметрах см. в [руководстве по производительности и настройке действия копирования](data-factory-copy-activity-performance.md#parallel-copy).
    ![Средство копирования — параметры производительности](media/data-factory-azure-blob-connector/copy-performance-settings.png) 
14. На странице **Сводка** просмотрите все параметры (свойства задачи, параметры источника, назначения и копии) и нажмите кнопку **Далее**.
    ![Средство копирования — страница "Сводка"](media/data-factory-azure-blob-connector/copy-tool-summary-page.png)
15. Просмотрите сведения на странице **Сводка** и нажмите кнопку **Готово**. Мастер создаст две связанные службы, два набора данных (входной и выходной) и один конвейер в фабрике данных (из которой вы запустили мастер копирования).
    ![Средство копирования — страница "Развертывание"](media/data-factory-azure-blob-connector/copy-tool-deployment-page.png)

### <a name="monitor-the-pipeline-copy-task"></a>Мониторинг конвейера (задача копирования)

1. На странице **развертывания** щелкните ссылку `Click here to monitor copy pipeline`. 
2. **Приложение для мониторинга и управления** должно появиться на отдельной вкладке.  ![Приложение для мониторинга и управления](media/data-factory-azure-blob-connector/monitor-manage-app.png)
3. Измените время **начала** в верху на `04/19/2017` и время **окончания** на `04/27/2017` и нажмите кнопку **Применить**. 
4. В списке **Activity windows** (Окна действий) появится пять окон действий. Значение **Window Start** (Начало окна) должно охватывать все дни от начала до конца выполнения конвейера. 
5. Нажмите кнопку **обновления** в списке **Activity windows** (Окна действий) несколько раз, пока состояние всех окон действия не изменится на "Готово". 
6. Убедитесь, что выходные файлы создаются в выходной папке контейнера adfblobconnector. Структура папок в выходной папке будет следующей: 
    ```
    2017/04/21
    2017/04/22
    2017/04/23
    2017/04/24
    2017/04/25    
    ```
Подробные сведения о мониторинге фабрик данных и управлении ими см. в статье [Мониторинг конвейеров фабрики данных Azure и управление ими с помощью приложения для мониторинга и управления](data-factory-monitor-manage-app.md). 
 
### <a name="data-factory-entities"></a>Сущности фабрики данных
Вернитесь на вкладку с домашней страницей фабрики данных. Обратите внимание, что в фабрике данных теперь есть две связанные службы, два набора данных и один конвейер. 

![Домашняя страница фабрики данных с сущностями](media/data-factory-azure-blob-connector/data-factory-home-page-with-numbers.png)

Щелкните **Создать и развернуть** для запуска редактора фабрики данных. 

![Редактор фабрики данных](media/data-factory-azure-blob-connector/data-factory-editor.png)

В вашей фабрике данных должны находиться следующие сущности фабрики данных 

 - Две связанные службы: одна — для источника, а другая — для назначения. Обе службы должны ссылаться на одну и ту же учетную запись Azure в этом пошаговом руководстве. 
 - Два набора данных: входной и выходной. В этом пошаговом руководстве оба набора используют один контейнер больших двоичных объектов, но ссылаются на разные папки (входная и выходная).
 - Один конвейер. Конвейер содержит действие копирования, использующее источник и приемник больших двоичных объектов для копирования данных из одного расположения больших двоичных объектов Azure в другое. 

Дополнительные сведения об этих сущностях приведены в следующих разделах. 

#### <a name="linked-services"></a>Связанные службы
Вы должны видеть две связанные службы: одна — для источника, а другая — для назначения. В этом пошаговом руководстве оба определения выглядят одинаково, за исключением имен. В качестве **типа** связанной службы задано значение **AzureStorage**. Самое важное свойство определения связанной службы — **connectionString**, которое фабрика данных использует для подключения к учетной записи хранения Azure во время выполнения. Игнорируйте свойство hubName в определении. 

##### <a name="source-blob-storage-linked-service"></a>Связанная служба исходного хранилища больших двоичных объектов
```json
{
    "name": "Source-BlobStorage-z4y",
    "properties": {
        "type": "AzureStorage",
        "typeProperties": {
            "connectionString": "DefaultEndpointsProtocol=https;AccountName=mystorageaccount;AccountKey=**********"
        }
    }
}
```

##### <a name="destination-blob-storage-linked-service"></a>Связанная служба выходного хранилища больших двоичных объектов

```json
{
    "name": "Destination-BlobStorage-z4y",
    "properties": {
        "type": "AzureStorage",
        "typeProperties": {
            "connectionString": "DefaultEndpointsProtocol=https;AccountName=mystorageaccount;AccountKey=**********"
        }
    }
}
```

Дополнительные сведения о связанной службе хранилища Azure см. в разделе [Свойства связанной службы](#linked-service-properties). 

#### <a name="datasets"></a>Наборы данных
Есть два набора данных: входной и выходной. В качестве типа для обоих наборов данных задано значение **AzureBlob**. 

Входной набор данных указывает на папку **input** контейнера больших двоичных объектов **adfblobconnector**. Для свойства **external** этого набора данных задано значение **true**, так как данные не создаются конвейером при выполнении действия копирования, принимающего этот набор данных в качестве входных данных. 

Выходной набор данных указывает на папку **output** того же контейнера больших двоичных объектов. Выходной набор данных также использует значение года, месяца и дня системной переменной **SliceStart** для динамического вычисления пути для выходного файла. Список функций и системных переменных, поддерживаемых фабрикой данных Azure, см. в [этой статье](data-factory-functions-variables.md). Для свойства **external** задано значение **false** (значение по умолчанию), так как этот набор данных создает конвейер. 

Дополнительные сведения о свойствах, поддерживаемых набором данных большого двоичного объекта Azure, см. в разделе [свойств набора данных](#dataset-properties).

##### <a name="input-dataset"></a>Входной набор данных

```json
{
    "name": "InputDataset-z4y",
    "properties": {
        "structure": [
            { "name": "Prop_0", "type": "String" },
            { "name": "Prop_1", "type": "String" }
        ],
        "type": "AzureBlob",
        "linkedServiceName": "Source-BlobStorage-z4y",
        "typeProperties": {
            "folderPath": "adfblobconnector/input/",
            "format": {
                "type": "TextFormat",
                "columnDelimiter": ","
            }
        },
        "availability": {
            "frequency": "Day",
            "interval": 1
        },
        "external": true,
        "policy": {}
    }
}
```

##### <a name="output-dataset"></a>Выходной набор данных

```json
{
    "name": "OutputDataset-z4y",
    "properties": {
        "structure": [
            { "name": "Prop_0", "type": "String" },
            { "name": "Prop_1", "type": "String" }
        ],
        "type": "AzureBlob",
        "linkedServiceName": "Destination-BlobStorage-z4y",
        "typeProperties": {
            "folderPath": "adfblobconnector/output/{year}/{month}/{day}",
            "format": {
                "type": "TextFormat",
                "columnDelimiter": ","
            },
            "partitionedBy": [
                { "name": "year", "value": { "type": "DateTime", "date": "SliceStart", "format": "yyyy" } },
                { "name": "month", "value": { "type": "DateTime", "date": "SliceStart", "format": "MM" } },
                { "name": "day", "value": { "type": "DateTime", "date": "SliceStart", "format": "dd" } }
            ]
        },
        "availability": {
            "frequency": "Day",
            "interval": 1
        },
        "external": false,
        "policy": {}
    }
}
```

#### <a name="pipeline"></a>Конвейер
В конвейере содержится всего одно действие. В качестве **типа** действия задано значение **Copy**.  В свойствах типа действия есть два раздела: один — для источника, а другой — для приемника. В качестве типа источника задано значение **BlobSource**, так как действие копирует данные из хранилища больших двоичных объектов. В качестве типа приемника задано значение **BlobSink**, так как действие копирует данные в хранилище больших двоичных объектов. Действие копирования принимает InputDataset-z4y в качестве входных данных и OutputDataset-z4y в качестве выходных. 

Дополнительные сведения о свойствах, поддерживаемых BlobSource и BlobSink, см. в разделе [свойств действия копирования](#copy-activity-properties). 

```json
{
    "name": "CopyPipeline",
    "properties": {
        "activities": [
            {
                "type": "Copy",
                "typeProperties": {
                    "source": {
                        "type": "BlobSource",
                        "recursive": false
                    },
                    "sink": {
                        "type": "BlobSink",
                        "copyBehavior": "MergeFiles",
                        "writeBatchSize": 0,
                        "writeBatchTimeout": "00:00:00"
                    }
                },
                "inputs": [
                    {
                        "name": "InputDataset-z4y"
                    }
                ],
                "outputs": [
                    {
                        "name": "OutputDataset-z4y"
                    }
                ],
                "policy": {
                    "timeout": "1.00:00:00",
                    "concurrency": 1,
                    "executionPriorityOrder": "NewestFirst",
                    "style": "StartOfInterval",
                    "retry": 3,
                    "longRetry": 0,
                    "longRetryInterval": "00:00:00"
                },
                "scheduler": {
                    "frequency": "Day",
                    "interval": 1
                },
                "name": "Activity-0-Blob path_ adfblobconnector_input_->OutputDataset-z4y"
            }
        ],
        "start": "2017-04-21T22:34:00Z",
        "end": "2017-04-25T05:00:00Z",
        "isPaused": false,
        "pipelineMode": "Scheduled"
    }
}
```

## <a name="json-examples-for-copying-data-to-and-from-blob-storage"></a>Примеры JSON для копирования данных в хранилище BLOB-объектов и обратно  
Ниже приведены примеры с определениями JSON, которые можно использовать для создания конвейера с помощью [портала Azure](data-factory-copy-activity-tutorial-using-azure-portal.md), [Visual Studio](data-factory-copy-activity-tutorial-using-visual-studio.md) или [Azure PowerShell](data-factory-copy-activity-tutorial-using-powershell.md). В них показано, как копировать данные в базу данных SQL Azure и хранилище BLOB-объектов Azure и обратно. Тем не менее данные можно копировать **непосредственно** из любых источников в любой указанный [здесь](data-factory-data-movement-activities.md#supported-data-stores-and-formats) приемник. Это делается с помощью действия копирования в фабрике данных Azure.

### <a name="json-example-copy-data-from-blob-storage-to-sql-database"></a>Пример JSON. Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL
В примере ниже используется следующее:

1. Связанная служба типа [AzureSqlDatabase](data-factory-azure-sql-connector.md#linked-service-properties).
2. Связанная служба типа [AzureStorage](#linked-service-properties).
3. Входной [набор данных](data-factory-create-datasets.md) типа [AzureBlob](#dataset-properties).
4. Выходной [набор данных](data-factory-create-datasets.md) типа [AzureSqlTable](data-factory-azure-sql-connector.md#dataset-properties).
5. [Конвейер](data-factory-create-pipelines.md) с действием копирования, в котором используются [BlobSource](#copy-activity-properties) и [SqlSink](data-factory-azure-sql-connector.md#copy-activity-properties).

В этом примере данные временного ряда каждый час копируются из большого двоичного объекта Azure в таблицу SQL Azure. Используемые в этих примерах свойства JSON описаны в разделах, следующих за примерами.

**Связанная служба SQL Azure**

```json
{
  "name": "AzureSqlLinkedService",
  "properties": {
    "type": "AzureSqlDatabase",
    "typeProperties": {
      "connectionString": "Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;User ID=<username>@<servername>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30"
    }
  }
}
```
**Связанная служба хранилища Azure**

```json
{
  "name": "StorageLinkedService",
  "properties": {
    "type": "AzureStorage",
    "typeProperties": {
      "connectionString": "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"
    }
  }
}
```
Фабрика данных Azure поддерживает два типа связанных служб хранилища Azure: **AzureStorage** и **AzureStorageSas**. Для первой необходимо указать строку подключения, содержащую ключ учетной записи, а для второй — URI подписанного URL-адреса. Дополнительные сведения см. в разделе [Связанные службы](#linked-service-properties).  

**Входной набор данных BLOB-объекта Azure**

Данные берутся из нового BLOB-объекта каждый час (frequency: hour, interval: 1). Путь к папке с BLOB-объектом и имя файла вычисляются динамически на основе времени начала обрабатываемого среза. В пути к папке используется год, месяц и день начала, а в имени файла — час начала. Когда для параметра external задано значение true, фабрика данных считает эту таблицу внешней по отношению к себе и не созданной в результате какого-либо действия в фабрике данных.

```json
{
  "name": "AzureBlobInput",
  "properties": {
    "type": "AzureBlob",
    "linkedServiceName": "StorageLinkedService",
    "typeProperties": {
      "folderPath": "mycontainer/myfolder/yearno={Year}/monthno={Month}/dayno={Day}/",
      "fileName": "{Hour}.csv",
      "partitionedBy": [
        { "name": "Year", "value": { "type": "DateTime", "date": "SliceStart", "format": "yyyy" } },
        { "name": "Month", "value": { "type": "DateTime", "date": "SliceStart", "format": "MM" } },
        { "name": "Day", "value": { "type": "DateTime", "date": "SliceStart", "format": "dd" } },
        { "name": "Hour", "value": { "type": "DateTime", "date": "SliceStart", "format": "HH" } }
      ],
      "format": {
        "type": "TextFormat",
        "columnDelimiter": ",",
        "rowDelimiter": "\n"
      }
    },
    "external": true,
    "availability": {
      "frequency": "Hour",
      "interval": 1
    },
    "policy": {
      "externalData": {
        "retryInterval": "00:01:00",
        "retryTimeout": "00:10:00",
        "maximumRetry": 3
      }
    }
  }
}
```
**Выходной набор данных SQL Azure**

В этом примере данные копируются в таблицу с именем MyOutputTable в базе данных SQL Azure. Создайте таблицу в базе данных SQL Azure с тем количеством столбцов, которое должно быть в CSV-файле большого двоичного объекта. Новые строки добавляются в таблицу каждый час.

```json
{
  "name": "AzureSqlOutput",
  "properties": {
    "type": "AzureSqlTable",
    "linkedServiceName": "AzureSqlLinkedService",
    "typeProperties": {
      "tableName": "MyOutputTable"
    },
    "availability": {
      "frequency": "Hour",
      "interval": 1
    }
  }
}
```
**Действие копирования в конвейере с BLOB-объектом в качестве источника и базой данных SQL в качестве приемника:**

Конвейер содержит действие копирования, которое использует входной и выходной наборы данных и выполняется каждый час. В определении JSON конвейера для типа **source** установлено значение **BlobSource**, а для типа **sink** — значение **SqlSink**.

```json
{  
    "name":"SamplePipeline",
    "properties":{  
    "start":"2014-06-01T18:00:00",
    "end":"2014-06-01T19:00:00",
    "description":"pipeline with copy activity",
    "activities":[  
      {
        "name": "AzureBlobtoSQL",
        "description": "Copy Activity",
        "type": "Copy",
        "inputs": [
          {
            "name": "AzureBlobInput"
          }
        ],
        "outputs": [
          {
            "name": "AzureSqlOutput"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "BlobSource"
          },
          "sink": {
            "type": "SqlSink"
          }
        },
       "scheduler": {
          "frequency": "Hour",
          "interval": 1
        },
        "policy": {
          "concurrency": 1,
          "executionPriorityOrder": "OldestFirst",
          "retry": 0,
          "timeout": "01:00:00"
        }
      }
      ]
   }
}
```
### <a name="json-example-copy-data-from-azure-sql-to-azure-blob"></a>Пример JSON. Копирование данных из базы данных SQL Azure в большой двоичный объект Azure
В примере ниже используется следующее:

1. Связанная служба типа [AzureSqlDatabase](data-factory-azure-sql-connector.md#linked-service-properties).
2. Связанная служба типа [AzureStorage](#linked-service-properties).
3. Входной [набор данных](data-factory-create-datasets.md) типа [AzureSqlTable](data-factory-azure-sql-connector.md#dataset-properties).
4. Выходной [набор данных](data-factory-create-datasets.md) типа [AzureBlob](#dataset-properties).
5. [Конвейер](data-factory-create-pipelines.md) с действием копирования, в котором используются [SqlSource](data-factory-azure-sql-connector.md#copy-activity-properties) и [BlobSink](#copy-activity-properties).

В этом примере данные временного ряда каждый час копируются из таблицы SQL Azure в большой двоичный объект Azure. Используемые в этих примерах свойства JSON описаны в разделах, следующих за примерами.

**Связанная служба SQL Azure**

```json
{
  "name": "AzureSqlLinkedService",
  "properties": {
    "type": "AzureSqlDatabase",
    "typeProperties": {
      "connectionString": "Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;User ID=<username>@<servername>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30"
    }
  }
}
```
**Связанная служба хранилища Azure**

```json
{
  "name": "StorageLinkedService",
  "properties": {
    "type": "AzureStorage",
    "typeProperties": {
      "connectionString": "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"
    }
  }
}
```
Фабрика данных Azure поддерживает два типа связанных служб хранилища Azure: **AzureStorage** и **AzureStorageSas**. Для первой необходимо указать строку подключения, содержащую ключ учетной записи, а для второй — URI подписанного URL-адреса. Дополнительные сведения см. в разделе [Связанные службы](#linked-service-properties).  

**Входной набор данных SQL Azure**

В примере предполагается, что таблица MyTable уже создана в SQL Azure и содержит столбец с именем timestampcolumn для данных временных рядов.

Если для параметра external задать значение true, то фабрика данных воспримет эту таблицу как внешнюю, которая создана не в результате какого-либо действия в этой службе.

```json
{
  "name": "AzureSqlInput",
  "properties": {
    "type": "AzureSqlTable",
    "linkedServiceName": "AzureSqlLinkedService",
    "typeProperties": {
      "tableName": "MyTable"
    },
    "external": true,
    "availability": {
      "frequency": "Hour",
      "interval": 1
    },
    "policy": {
      "externalData": {
        "retryInterval": "00:01:00",
        "retryTimeout": "00:10:00",
        "maximumRetry": 3
      }
    }
  }
}
```

**Выходной набор данных BLOB-объекта Azure**

Данные записываются в новый BLOB-объект каждый час (frequency: hour, interval: 1). Путь к папке BLOB-объекта вычисляется динамически на основе времени начала обрабатываемого среза. В пути к папке используется год, месяц, день и час времени начала.

```json
{
  "name": "AzureBlobOutput",
  "properties": {
    "type": "AzureBlob",
    "linkedServiceName": "StorageLinkedService",
    "typeProperties": {
      "folderPath": "mycontainer/myfolder/yearno={Year}/monthno={Month}/dayno={Day}/hourno={Hour}/",
      "partitionedBy": [
        {
          "name": "Year",
          "value": { "type": "DateTime",  "date": "SliceStart", "format": "yyyy" } },
        { "name": "Month", "value": { "type": "DateTime", "date": "SliceStart", "format": "MM" } },
        { "name": "Day", "value": { "type": "DateTime", "date": "SliceStart", "format": "dd" } },
        { "name": "Hour", "value": { "type": "DateTime", "date": "SliceStart", "format": "HH" } }
      ],
      "format": {
        "type": "TextFormat",
        "columnDelimiter": "\t",
        "rowDelimiter": "\n"
      }
    },
    "availability": {
      "frequency": "Hour",
      "interval": 1
    }
  }
}
```

**Действие копирования в конвейере с базой данных SQL в качестве источника и BLOB-объектом в качестве приемника:**

Конвейер содержит действие копирования, которое использует входной и выходной наборы данных и выполняется каждый час. В определении JSON конвейера для типа **source** установлено значение **SqlSource**, а для типа **sink** — значение **BlobSink**. SQL-запрос, указанный для свойства **SqlReaderQuery** , выбирает для копирования данные за последний час.

```json
{  
    "name":"SamplePipeline",
    "properties":{  
        "start":"2014-06-01T18:00:00",
        "end":"2014-06-01T19:00:00",
        "description":"pipeline for copy activity",
        "activities":[  
              {
                "name": "AzureSQLtoBlob",
                "description": "copy activity",
                "type": "Copy",
                "inputs": [
                  {
                    "name": "AzureSQLInput"
                  }
                ],
                "outputs": [
                  {
                    "name": "AzureBlobOutput"
                  }
                ],
                "typeProperties": {
                    "source": {
                        "type": "SqlSource",
                        "SqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
                      },
                      "sink": {
                        "type": "BlobSink"
                      }
                },
                   "scheduler": {
                      "frequency": "Hour",
                      "interval": 1
                },
                "policy": {
                      "concurrency": 1,
                      "executionPriorityOrder": "OldestFirst",
                      "retry": 0,
                      "timeout": "01:00:00"
                }
              }
         ]
    }
}
```

> [!NOTE]
> Сведения о сопоставлении столбцов в наборе данных, используемом в качестве источника, со столбцами в приемнике см. в [этой статье](data-factory-map-columns.md).

## <a name="performance-and-tuning"></a>Производительность и настройка
Ознакомьтесь со статьей [Руководство по настройке производительности действия копирования](data-factory-copy-activity-performance.md), в которой описываются ключевые факторы, влияющие на производительность перемещения данных (действие копирования) в фабрике данных Azure, и различные способы оптимизации этого процесса.
