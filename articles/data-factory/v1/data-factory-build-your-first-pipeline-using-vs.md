---
title: "Создание первой фабрики данных (Visual Studio) | Документация Майкрософт"
description: "В этом руководстве вы создадите образец конвейера фабрики данных Azure с помощью Visual Studio."
services: data-factory
documentationcenter: 
author: spelluru
manager: jhubbard
editor: monicar
ms.assetid: 7398c0c9-7a03-4628-94b3-f2aaef4a72c5
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: hero-article
ms.date: 01/22/2018
ms.author: spelluru
robots: noindex
ms.openlocfilehash: 49fb249b6ff1169527829c77a6539926ec99b912
ms.sourcegitcommit: 9cc3d9b9c36e4c973dd9c9028361af1ec5d29910
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/23/2018
---
# <a name="tutorial-create-a-data-factory-by-using-visual-studio"></a>Руководство: создание фабрики данных с помощью Visual Studio
> [!div class="op_single_selector" title="Tools/SDKs"]
> * [Обзор и предварительные требования](data-factory-build-your-first-pipeline.md)
> * [портал Azure](data-factory-build-your-first-pipeline-using-editor.md)
> * [Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)
> * [PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)
> * [Шаблон Resource Manager](data-factory-build-your-first-pipeline-using-arm.md)
> * [REST API](data-factory-build-your-first-pipeline-using-rest-api.md)


> [!NOTE]
> Статья относится к версии 1 фабрики данных, которая является общедоступной версией. Если вы используете версию 2 службы фабрики данных, которая находится на этапе предварительной версии, ознакомьтесь с [кратким руководством по созданию фабрики данных с помощью фабрики данных Azure версии 2](../quickstart-create-data-factory-dot-net.md).

В этом руководстве рассматривается создание фабрики данных Azure с помощью Visual Studio. Вы сможете создать проект Visual Studio с помощью шаблона проекта фабрики данных, определить сущности фабрики данных (связанные службы, наборы данных и конвейеры) в формате JSON, а затем опубликовать или развернуть эти сущности в облаке. 

В этом руководстве конвейеру доступно одно действие — **действие Hive HDInsight**. Это действие запускает сценарий Hive в кластере HDInsight Azure, который преобразует входные данные в выходные. Конвейер запускается раз в месяц по расписанию. Время начала и окончания запуска также указаны. 

> [!NOTE]
> Копирование данных с помощью фабрики данных Azure в этом руководстве не рассматривается. Инструкции по копированию данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных Azure см. в [этой статье](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).
> 
> Конвейер может содержать сразу несколько действий. Два действия можно объединить в цепочку (выполнить одно действие вслед за другим), настроив выходной набор данных одного действия как входной набор данных другого действия. Дополнительные сведения см. в разделе [Несколько действий в конвейере](data-factory-scheduling-and-execution.md#multiple-activities-in-a-pipeline).


## <a name="walkthrough-create-and-publish-data-factory-entities"></a>Пошаговое руководство: создание и публикация сущностей фабрики данных
В процессе работы с этим руководством вам потребуется выполнить приведенные ниже действия.

1. Создать две связанные службы — **AzureStorageLinkedService1** и **HDInsightOnDemandLinkedService1**. 
   
    Входные и выходные данные для действия Hive, предусмотренные в этом руководстве, находятся в одном хранилище BLOB-объектов Azure. Чтобы преобразовать существующие входные данные в выходные, вам понадобится использовать кластер HDInsight по запросу. Кластер HDInsight по запросу создается автоматически фабрикой данных Azure во время выполнения, когда входные данные готовы к обработке. Вам потребуется связать хранилища данных или вычисления с фабрикой данных, чтобы служба фабрики данных могла подключиться к ним во время выполнения. Таким образом, вы свяжете свою учетную запись хранения Azure с фабрикой данных с помощью службы AzureStorageLinkedService1, а кластер HDInsight по запросу — с помощью службы HDInsightOnDemandLinkedService1. При публикации вам нужно будет указать имя фабрики данных, которая будет создана, или имя существующей фабрики данных.  
2. Создать два набора данных — **InputDataset** и **OutputDataset** для входных и выходных данных, сохраненных в хранилище BLOB-объектов Azure. 
   
    Эти определения наборов данных представляют созданную на предыдущем шаге связанную службу хранения Azure. Для набора данных InputDataset вам потребуется указать контейнер больших двоичных объектов (adfgetstarted) и папку (inptutdata), в которой содержится большой двоичный объект с входными данными. Для набора данных OutputDataset вам потребуется указать контейнер больших двоичных объектов (adfgetstarted) и папку (partitioneddata), в которой содержатся выходные данные. Нужно указать и другие свойства, такие как структура, доступность данных и политика.
3. Создать конвейер с именем **MyFirstPipeline**. 
  
    В этом руководстве конвейеру доступно одно действие — **действие Hive HDInsight**. Это действие запускает сценарий Hive в кластере HDInsight по запросу, который преобразует входные данные в выходные. Дополнительные сведения о действии Hive см. в статье о [преобразовании данных с помощью действия Hive в фабрике данных Azure](data-factory-hive-activity.md). 
4. Создать фабрику данных с именем **DataFactoryUsingVS**. Развертывание фабрики данных и всех сущностей фабрики данных, таких как связанные службы, таблицы и конвейеры.
5. После публикации для мониторинга конвейера вы можете использовать колонки портала Azure и приложение мониторинга и управления. 
  
### <a name="prerequisites"></a>предварительным требованиям
1. Прочтите [обзорную статью](data-factory-build-your-first-pipeline.md) и выполните **предварительные требования** . Чтобы перейти к статье, вы также можете выбрать в раскрывающемся списке в верхней области параметр **Обзор и предварительные требования**. После выполнения предварительных требований вам необходимо снова вернуться к этой статье. Для этого выберите в раскрывающемся списке параметр **Visual Studio**.
2. Создавать экземпляры фабрики данных может пользователь с ролью [Участник фабрики данных](../../active-directory/role-based-access-built-in-roles.md#data-factory-contributor) на уровне подписки или группы ресурсов.  
3. На вашем компьютере должны быть установлены следующие компоненты:
   * Visual Studio 2013 или Visual Studio 2015.
   * Загрузите пакет SDK Azure для Visual Studio 2013 или Visual Studio 2015. Перейдите на [cтраницу загрузки Azure](https://azure.microsoft.com/downloads/) и щелкните **VS 2013** или **VS2015** в разделе **.NET**.
   * Скачайте последнюю версию подключаемого модуля фабрики данных Azure для Visual Studio: [VS 2013](https://visualstudiogallery.msdn.microsoft.com/754d998c-8f92-4aa7-835b-e89c8c954aa5) или [VS 2015](https://visualstudiogallery.msdn.microsoft.com/371a4cf9-0093-40fa-b7dd-be3c74f49005). Вы также можете обновить подключаемый модуль, выполнив следующие действия. В меню выберите **Сервис** -> **Расширения и обновления** -> **В сети** -> **Галерея Visual Studio** -> **Microsoft Azure Data Factory Tools for Visual Studio**(Средства фабрики данных Microsoft Azure для Visual Studio) -> **Обновить**.

Теперь давайте создадим фабрику данных Azure с помощью Visual Studio.

### <a name="create-visual-studio-project"></a>Создание проекта Visual Studio
1. Запустите **Visual Studio 2013** или **Visual Studio 2015**. Щелкните **Файл**, наведите указатель мыши на пункт **Создать** и щелкните **Проект**. Откроется диалоговое окно **Новый проект** .  
2. В диалоговом окне **Новый проект** выберите шаблон **DataFactory** и щелкните **Empty Data Factory Project** (Пустой проект фабрики данных).   

    ![Диалоговое окно "Новый проект"](./media/data-factory-build-your-first-pipeline-using-vs/new-project-dialog.png)
3. Введите **имя** проекта, **расположение** и имя **решения**, а затем нажмите кнопку **ОК**.

    ![обозревателе решений](./media/data-factory-build-your-first-pipeline-using-vs/solution-explorer.png)

### <a name="create-linked-services"></a>Создание связанных служб
На этом шаге вы создадите две связанные службы — **службу хранилища Azure** и **связанную службу кластера HDInsight по запросу**. 

Связанная служба хранилища Azure позволит связать учетную запись хранения Azure с фабрикой данных, предоставляя сведения о подключении. Служба фабрики данных использует строку подключения из параметра связанной службы для подключения к хранилищу Azure во время выполнения. Это хранилище содержит входные и выходные данные для конвейера, а также файл скрипта Hive, используемый действием Hive. 

Если используется связанная служба HDInsight по запросу, кластер HDInsight автоматически создается во время выполнения, когда входные данные готовы к обработке. После завершения обработки и простоя в течение указанного времени кластер удаляется. 

> [!NOTE]
> Фабрика данных будет создана, когда вы укажете имя и параметры для нее во время публикации ее решения.

#### <a name="create-azure-storage-linked-service"></a>Создание связанной службы хранения Azure
1. Щелкните правой кнопкой мыши **Связанные службы** в обозревателе решений, наведите указатель мыши на команду **Добавить** и выберите **Новый элемент**.      
2. В диалоговом окне **Добавление нового элемента** выберите в списке пункт **Azure Storage Linked Service** (Связанная служба хранилища Azure) и нажмите кнопку **Добавить**.
    ![Связанная служба хранения Azure](./media/data-factory-build-your-first-pipeline-using-vs/new-azure-storage-linked-service.png)
3. Замените `<accountname>` и `<accountkey>` именем и ключом учетной записи хранения Azure. Сведения о получении, просмотре, копировании и повторном создании ключей доступа к хранилищу см. в разделе [Управление учетной записью хранения](../../storage/common/storage-create-storage-account.md#manage-your-storage-account).
    ![Связанная служба хранения Azure](./media/data-factory-build-your-first-pipeline-using-vs/azure-storage-linked-service.png)
4. Сохраните файл **AzureStorageLinkedService1.json** .

#### <a name="create-azure-hdinsight-linked-service"></a>Создание связанной службы Azure HDInsight
1. В **обозревателе решений** щелкните правой кнопкой мыши **Связанные службы**, наведите указатель на пункт **Добавить** и выберите **Новый элемент**.
2. Выберите **Связанная служба HDInsight по запросу**, а затем щелкните **Добавить**.
3. Замените фрагмент **JSON** на приведенный ниже код JSON.

     ```json
    {
        "name": "HDInsightOnDemandLinkedService",
        "properties": {
        "type": "HDInsightOnDemand",
            "typeProperties": {
                "version": "3.5",
                "clusterSize": 1,
                "timeToLive": "00:05:00",
                "osType": "Linux",
                "linkedServiceName": "AzureStorageLinkedService1"
            }
        }
    }
    ```

    В следующей таблице приведены описания свойств JSON, используемых в этом фрагменте кода.

    Свойство | ОПИСАНИЕ
    -------- | ----------- 
    ClusterSize (размер кластера) | Указывает размер кластера HDInsight Hadoop.
    TimeToLive | Указывает, сколько времени может простаивать кластер HDInsight, прежде чем он будет удален.
    linkedServiceName | Указывает имя учетной записи хранения, в которой будут храниться журналы, создаваемые кластером HDInsight Hadoop. 

    > [!IMPORTANT]
    > Кластер HDInsight создает **контейнер по умолчанию** в хранилище BLOB-объектов, указанном в коде JSON (linkedServiceName). При удалении кластера HDInsight этот контейнер не удаляется. В этом весь замысел. Если используется связанная служба HDInsight по запросу, кластер HDInsight создается при каждой обработке среза данных (если не используется динамический кластер (timeToLive)). После завершения обработки кластер автоматически удаляется.
    > 
    > По мере обработки новых срезов количество контейнеров в хранилище BLOB-объектов будет увеличиваться. Если эти контейнеры не используются для устранения неполадок с заданиями, удалите их — это позволит сократить расходы на хранение. Имена этих контейнеров указаны по шаблону `adf<yourdatafactoryname>-<linkedservicename>-datetimestamp`. Для удаления контейнеров в хранилище BLOB-объектов Azure используйте такие инструменты, как [Microsoft Storage Explorer](http://storageexplorer.com/) .

    Дополнительные сведения о свойствах JSON см. в разделе [Связанная служба Azure HDInsight по запросу](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service). 
4. Сохраните файл **HDInsightOnDemandLinkedService1.json** .

### <a name="create-datasets"></a>Создание наборов данных
На этом шаге вы создадите наборы данных, которые представляют входные и выходные данные для обработки Hive. Эти наборы данных ссылаются на службу **AzureStorageLinkedService1** , созданную ранее в ходе работы с этим руководством. Точки связанной службы указывают на учетную запись хранения Azure, а наборы данных указывают контейнер, папку и имя файла в хранилище, в котором содержатся входные и выходные данные.   

#### <a name="create-input-dataset"></a>Создание входного набора данных
1. В **обозревателе решений** щелкните правой кнопкой мыши элемент **Таблицы**, наведите указатель на пункт **Добавить** и выберите **Новый элемент**.
2. Выберите в списке **BLOB-объект Azure**, измените имя файла на **InputDataSet.json** и нажмите кнопку **Добавить**.
3. Замените фрагмент **JSON** в редакторе на приведенный ниже фрагмент кода JSON.

    ```json
    {
        "name": "AzureBlobInput",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService1",
            "typeProperties": {
                "fileName": "input.log",
                "folderPath": "adfgetstarted/inputdata",
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "availability": {
                "frequency": "Month",
                "interval": 1
            },
            "external": true,
            "policy": {}
        }
    }
    ```
    Этот фрагмент кода JSON определяет набор данных с именем **AzureBlobInput**, который представляет входные данные для действия Hive в конвейере. Вам необходимо указать, что входные данные размещаются в контейнере больших двоичных объектов с именем `adfgetstarted` и в папке с именем `inputdata`.

    В следующей таблице приведены описания свойств JSON, используемых в этом фрагменте кода.

    Свойство | ОПИСАНИЕ |
    -------- | ----------- |
    Тип |Так как данные размещаются в хранилище BLOB-объектов Azure, для свойства типа задается значение **AzureBlob**.
    linkedServiceName | Указывает созданную ранее службу AzureStorageLinkedService1.
    fileName |Это необязательное свойство. Если это свойство не указано, выбираются все файлы из папки folderPath. В этом случае обрабатывается только файл input.log.
    Тип | Файлы журнала представлены в текстовом формате, поэтому мы используем значение TextFormat. |
    columnDelimiter | Столбцы в файлах журнала разделяются запятыми (`,`).
    frequency и interval | Для свойства frequency задано значение Month, а для свойства interval — значение 1. Это означает, что срезы входных данных доступны ежемесячно.
    external | Это свойство имеет значение true, если входные данные действия не создаются конвейером. Это свойство указывается только для входных наборов данных. Для входного набора данных первого действия всегда задавайте значение true.
4. Сохраните файл **InputDataset.json** .

#### <a name="create-output-dataset"></a>Создание выходного набора данных
Теперь вы можете создать выходной набор данных, чтобы представить выходные данные, сохраненные в хранилище BLOB-объектов Azure.

1. В **обозревателе решений** щелкните правой кнопкой мыши элемент **Таблицы**, наведите указатель на пункт **Добавить** и выберите **Новый элемент**.
2. Выберите в списке **BLOB-объект Azure**, измените имя файла на **OutputDataset.json** и нажмите кнопку **Добавить**.
3. Замените фрагмент **JSON** в редакторе на приведенный ниже код JSON.
    
    ```json
    {
        "name": "AzureBlobOutput",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService1",
            "typeProperties": {
                "folderPath": "adfgetstarted/partitioneddata",
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "availability": {
                "frequency": "Month",
                "interval": 1
            }
        }
    }
    ```
    Этот фрагмент кода JSON определяет набор данных с именем **AzureBlobOutput**, который представляет выходные данные для действия Hive в конвейере. Вам необходимо указать, что выходные данные, созданные действием Hive, размещаются в контейнере больших двоичных объектов с именем `adfgetstarted` и в папке с именем `partitioneddata`. 
    
    В разделе **availability** указывается частота, с которой будет создаваться выходной набор данных (ежемесячно). С помощью выходного набора данных настраивается расписание конвейера. Конвейер запускается ежемесячно в соответствии с точным указанием времени начала и окончания запуска. 

    Описание этих свойств можно найти в разделе **Создание входного набора данных** . Так как выходной набор данных создается конвейером, нет необходимости задавать ему свойство external.
4. Сохраните файл **OutputDataset.json** .

### <a name="create-pipeline"></a>Создание конвейера
Пока что вы создали связанную службу хранения Azure, а также входной и выходной наборы данных. Теперь вы можете создать конвейер с помощью действия **HDInsightHive**. Параметру **input** для действия Hive присвоено значение **AzureBlobInput**, а параметру **output** — значение **AzureBlobOutput**. Срезы входного и выходного наборов данных создаются ежемесячно (частота: месяц, интервал: 1). 

1. В **обозревателе решений** щелкните правой кнопкой мыши **Конвейеры**, наведите указатель на команду **Добавить** и выберите **Новый элемент.**
2. Выберите в списке пункт **Конвейер преобразования Hive** и нажмите кнопку **Добавить**.
3. Замените фрагмент **JSON** на приведенный ниже код.

    > [!IMPORTANT]
    > Замените `<storageaccountname>` именем своей учетной записи хранения.

    ```json
    {
        "name": "MyFirstPipeline",
        "properties": {
            "description": "My first Azure Data Factory pipeline",
            "activities": [
                {
                    "type": "HDInsightHive",
                    "typeProperties": {
                        "scriptPath": "adfgetstarted/script/partitionweblogs.hql",
                        "scriptLinkedService": "AzureStorageLinkedService1",
                        "defines": {
                            "inputtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/inputdata",
                            "partitionedtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/partitioneddata"
                        }
                    },
                    "inputs": [
                        {
                            "name": "AzureBlobInput"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "AzureBlobOutput"
                        }
                    ],
                    "policy": {
                        "concurrency": 1,
                        "retry": 3
                    },
                    "scheduler": {
                        "frequency": "Month",
                        "interval": 1
                    },
                    "name": "RunSampleHiveActivity",
                    "linkedServiceName": "HDInsightOnDemandLinkedService"
                }
            ],
            "start": "2016-04-01T00:00:00Z",
            "end": "2016-04-02T00:00:00Z",
            "isPaused": false
        }
    }
    ```

    > [!IMPORTANT]
    > Замените `<storageaccountname>` именем своей учетной записи хранения.

    Этот фрагмент кода JSON определяет конвейер, состоящий из одного действия (действия Hive). Это действие запускает сценарий Hive в кластере HDInsight по запросу для преобразования входных данных в выходные. В разделе действий конвейера JSON вы увидите только одно действие в массиве. Типу действия задано значение **HDInsightHive**. 

    В свойствах типа, заданных действию Hive HDInsight, вам необходимо указать, у какой связанной службы хранения Azure есть файл скрипта Hive, путь к файлу скрипта, а также параметры для этого файла. 

    Файл скрипта Hive **partitionweblogs.hql** хранится в учетной записи хранения Azure (указанной scriptLinkedService), а также в папке `script` в контейнере `adfgetstarted`.

    Раздел `defines` используется, чтобы указать параметры среды выполнения, передаваемые скрипту Hive в качестве значений конфигурации Hive (например, `${hiveconf:inputtable}`, `${hiveconf:partitionedtable})`).

    Активный период конвейера задается с помощью свойств **start** и **end**. Так как вы настроили ежемесячное создание набора данных, конвейер создает только один срез (с учетом того, что даты начала и окончания охватывают один месяц).

    В действии JSON укажите, что скрипт Hive будет выполняться в среде вычислений, указанной в свойстве **linkedServiceName**, — **HDInsightOnDemandLinkedService**.
4. Сохраните файл **HiveActivity1.json** .

### <a name="add-partitionweblogshql-and-inputlog-as-a-dependency"></a>Добавление файлов partitionweblogs.hql и input.log в качестве зависимости
1. В **окне обозревателя решений** щелкните правой кнопкой мыши **Зависимости**, наведите указатель на команду **Добавить** и щелкните **Существующий элемент**.  
2. Перейдите в папку **C:\ADFGettingStarted**, выберите файлы **partitionweblogs.hql** и **input.log**, а затем щелкните **Добавить**. Эти два файла, которые вы создали, являются необходимыми компонентами, описанными в [обзоре руководства](data-factory-build-your-first-pipeline.md).

При публикации решения в рамках следующего шага файл **partitionweblogs.hql** отправляется в папку **скриптов**, расположенную в контейнере больших двоичных объектов `adfgetstarted`.   

### <a name="publishdeploy-data-factory-entities"></a>Публикация и развертывание сущностей фабрики данных
На этом этапе вы сможете опубликовать сущности фабрики данных (связанные службы, наборы данных и конвейер) своего проекта в службе фабрики данных Azure. В процессе публикации вам понадобится указать имя фабрики данных. 

1. В обозревателе решений щелкните проект правой кнопкой мыши и выберите **Опубликовать**.
2. Когда отобразится диалоговое окно **Вход в учетную запись Майкрософт**, введите данные учетной записи с подпиской Azure и нажмите кнопку **Войти**.
3. Вы должны увидеть следующее диалоговое окно:

   ![Диалоговое окно "Опубликовать"](./media/data-factory-build-your-first-pipeline-using-vs/publish.png)
4. На странице **Configure data factory** (Настройка фабрики данных) выполните следующие действия:

    ![Публикация. Новые параметры фабрики данных](media/data-factory-build-your-first-pipeline-using-vs/publish-new-data-factory.png)

   1. Выберите **Создать новую фабрику данных** .
   2. Введите уникальное **имя** фабрики данных. Например: **DataFactoryUsingVS09152016**. Оно должно быть глобально уникальным.
   3. Выберите соответствующую подписку в поле **Подписка** . 
        > [!IMPORTANT]
        > Если подписки не отображаются, проверьте, выполнен ли вход с использованием учетной записи администратора или соадминистратора подписки.
   4. Выберите **группу ресурсов** для создаваемой фабрики данных.
   5. Выберите **регион** для фабрики данных.
   6. Нажмите кнопку **Далее**, чтобы перейти на страницу **Publish Items** (Публикация элементов). (Нажмите клавишу **TAB**, чтобы выйти из поля с именем, если кнопка **Далее** недоступна).

    > [!IMPORTANT]
    > Если при публикации появится сообщение об ошибке **Имя фабрики данных DataFactoryUsingVS недоступно**, измените имя (например, на ваше_имя_DataFactoryUsingVS). Ознакомьтесь с разделом [Фабрика данных — правила именования](data-factory-naming-rules.md) , чтобы узнать о правилах именования артефактов фабрики данных.   
1. На странице **Publish Items** (Публикация элементов) выберите все сущности фабрик данных и нажмите кнопку **Далее**, чтобы перейти на страницу **Сводка**.

    ![Страница Publish items (Публикация элементов)](media/data-factory-build-your-first-pipeline-using-vs/publish-items-page.png)     
2. Просмотрите сводку и нажмите кнопку **Далее** для запуска процесса развертывания и просмотра **состояния развертывания**.

    ![Страница "Сводка"](media/data-factory-build-your-first-pipeline-using-vs/summary-page.png)
3. На странице **Состояние развертывания** вы увидите состояние процесса развертывания. После завершения развертывания нажмите кнопку "Готово".

Необходимо учитывать следующие важные замечания.

- Если появится сообщение об ошибке **Подписка не зарегистрирована для использования пространства имен Microsoft.DataFactory**, выполните одно из следующих действий и повторите попытку публикации.
    - В Azure PowerShell выполните следующую команду, чтобы зарегистрировать поставщик фабрики данных Azure:
        ```PowerShell   
        Register-AzureRmResourceProvider -ProviderNamespace Microsoft.DataFactory
        ```
        Чтобы убедиться, что поставщик фабрики данных зарегистрирован, выполните следующую команду:

        ```PowerShell
        Get-AzureRmResourceProvider
        ```
    - Войдите на [портал Azure](https://portal.azure.com) с использованием подписки Azure и откройте колонку фабрики данных или создайте на портале фабрику данных. Поставщик будет зарегистрирован автоматически.
- В будущем имя фабрики данных может быть зарегистрировано в качестве DNS-имени и, следовательно, стать отображаемым.
- Чтобы создать экземпляры фабрики данных, вы должны быть администратором или соадминистратором подписки Azure.

### <a name="monitor-pipeline"></a>Отслеживание конвейера
На этом шаге вы узнаете, как отслеживать конвейер с помощью представления схемы фабрики данных. 

#### <a name="monitor-pipeline-using-diagram-view"></a>Мониторинг конвейера с использованием представления схемы
1. Войдите на [портал Azure](https://portal.azure.com/) и сделайте следующее:
   1. Щелкните **Другие службы** и выберите **Фабрики данных**.
       
        ![Обзор фабрик данных](./media/data-factory-build-your-first-pipeline-using-vs/browse-datafactories.png)
   2. В списке фабрик данных выберите имя своей фабрики данных (например, **DataFactoryUsingVS09152016**).
   
       ![Выбор фабрики данных](./media/data-factory-build-your-first-pipeline-using-vs/select-first-data-factory.png)
2. На домашней странице своей фабрики данных щелкните элемент **Схема**.

    ![Плитка "Схема"](./media/data-factory-build-your-first-pipeline-using-vs/diagram-tile.png)
3. В представлении схемы вы увидите все конвейеры и наборы данных, используемые в этом руководстве.

    ![Представление схемы](./media/data-factory-build-your-first-pipeline-using-vs/diagram-view-2.png)
4. Чтобы просмотреть все действия в конвейере, щелкните конвейер в схеме правой кнопкой мыши и выберите пункт "Открыть конвейер".

    ![Откройте меню конвейера](./media/data-factory-build-your-first-pipeline-using-vs/open-pipeline-menu.png)
5. Убедитесь, что действие HDInsightHive отображается в конвейере.

    ![Откройте представление конвейера](./media/data-factory-build-your-first-pipeline-using-vs/open-pipeline-view.png)

    Чтобы перейти к предыдущему представлению, щелкните **Фабрики данных** в меню навигации вверху.
6. В **представлении схемы** дважды щелкните набор данных **AzureBlobInput**. Убедитесь, что срез находится в состоянии **Готово** . Для отображения этого состояния может потребоваться несколько минут. Если это не произойдет через некоторое время, убедитесь, что входной файл (input.log) расположен в правильном контейнере (`adfgetstarted`) и папке (`inputdata`). Задайте для свойства **external** во входном наборе данных значение **true**. 

   ![Срез входных данных в состоянии "Готово"](./media/data-factory-build-your-first-pipeline-using-vs/input-slice-ready.png)
7. Щелкните **X**, чтобы закрыть колонку **AzureBlobInput**.
8. В **представлении схемы** дважды щелкните набор данных **AzureBlobOutput**. Вы увидите срез, который сейчас обрабатывается.

   ![Выборка](./media/data-factory-build-your-first-pipeline-using-vs/dataset-blade.png)
9. Как только обработка завершится, срез перейдет в состояние **Готово** .

   > [!IMPORTANT]
   > Создание используемого по требованию кластера HDInsight обычно занимает некоторое время (около 20 минут). Таким образом, конвейер обработает срез **примерно через 30 минут** .  
   
    ![Выборка](./media/data-factory-build-your-first-pipeline-using-vs/dataset-slice-ready.png)    
10. Когда срез перейдет в состояние **Готово**, проверьте выходные данные в папке `partitioneddata` контейнера `adfgetstarted` в хранилище BLOB-объектов.  

    ![выходные данные](./media/data-factory-build-your-first-pipeline-using-vs/three-ouptut-files.png)
11. Щелкните срез, чтобы просмотреть сведения о нем в колонке **Срез данных** .

    ![Сведения о срезе данных](./media/data-factory-build-your-first-pipeline-using-vs/data-slice-details.png)  
12. Щелкните выполнение действия в списке **Выполнения действий**, чтобы просмотреть сведения о нем (действие Hive в нашем сценарии) в окне **Подробности о выполнении операции**. 
  
    ![СВЕДЕНИЯ О ВЫПОЛНЕННОМ ДЕЙСТВИИ](./media/data-factory-build-your-first-pipeline-using-vs/activity-window-blade.png)    

    В файлах журналов содержатся сведения о выполненном запросе Hive и его состоянии. Эти журналы полезны при устранении неполадок.  

Инструкции по использованию портала Azure для мониторинга конвейера и наборов данных, созданных с помощью этого руководства, см. в разделе [Отслеживание конвейера](data-factory-monitor-manage-pipelines.md).

#### <a name="monitor-pipeline-using-monitor--manage-app"></a>Мониторинг конвейера с использованием приложения по мониторингу и управлению
Для мониторинга конвейеров также можно использовать приложение по мониторингу и управлению. Дополнительные сведения об использовании этого приложения см. в статье [Мониторинг конвейеров фабрики данных Azure и управление ими с помощью нового приложения по мониторингу и управлению](data-factory-monitor-manage-app.md).

1. Щелкните плитку Monitor & Manage (Мониторинг и управление).

    ![Плитка Monitor & Manage (Мониторинг и управление)](./media/data-factory-build-your-first-pipeline-using-vs/monitor-and-manage-tile.png)
2. Вы должны увидеть приложение по мониторингу и управлению. Для параметров **времени начала** и **времени окончания** введите значения "01-04-2016 12:00" и "02-04-2016 12:00" для конвейера и щелкните **Применить**.

    ![Приложение по мониторингу и управлению](./media/data-factory-build-your-first-pipeline-using-vs/monitor-and-manage-app.png)
3. Выберите окно действия в **списке Activity Windows** (Окна действий), чтобы просмотреть сведения о нем.
    ![Сведения об окне действия](./media/data-factory-build-your-first-pipeline-using-vs/activity-window-details.png)

> [!IMPORTANT]
> В случае успешной обработки среза входной файл удаляется. Если вы хотите повторно обработать срез или еще раз выполнить инструкции из руководства, передайте входной файл (input.log) в папку `inputdata` в контейнере `adfgetstarted`.

### <a name="additional-notes"></a>Дополнительные замечания
- Фабрика данных может иметь один или несколько конвейеров. Конвейер может содержать одно или несколько действий. Это может быть, например, действие копирования данных из исходного хранилища в целевое или действие HDInsight Hive для выполнения скрипта Hive, преобразующего входные данные. См. список [поддерживаемых хранилищ данных](data-factory-data-movement-activities.md#supported-data-stores-and-formats) для всех источников и приемников, которые поддерживаются действием копирования. См. список [связанных служб вычислений](data-factory-compute-linked-services.md), поддерживаемых фабрикой данных.
- Связанные службы связывают хранилища данных или службы вычислений с фабрикой данных Azure. См. список [поддерживаемых хранилищ данных](data-factory-data-movement-activities.md#supported-data-stores-and-formats) для всех источников и приемников, которые поддерживаются действием копирования. Ознакомьтесь со списком [связанных служб вычислений](data-factory-compute-linked-services.md), поддерживаемых фабрикой данных, и [действий по преобразованию данных](data-factory-data-transformation-activities.md), которые можно в этих службах выполнить.
- Дополнительные сведения о свойствах JSON, используемых в определении связанной службы хранения Azure, см. в разделе [Связанная служба SAS хранилища Azure](data-factory-azure-blob-connector.md#azure-storage-linked-service).
- Вместо используемого по запросу кластера HDInsight можно использовать собственный кластер HDInsight. Дополнительные сведения см. в статье [Связанные службы вычислений](data-factory-compute-linked-services.md).
-  С помощью приведенного выше JSON-файла фабрика данных создает кластер HDInsight **под управлением Linux**. Дополнительные сведения см. в разделе [Связанная служба Azure HDInsight по запросу](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service).
- Кластер HDInsight создает **контейнер по умолчанию** в хранилище BLOB-объектов, указанном в коде JSON (linkedServiceName). При удалении кластера HDInsight этот контейнер не удаляется. В этом весь замысел. Если используется связанная служба HDInsight по запросу, кластер HDInsight создается при каждой обработке среза данных (если не используется динамический кластер (timeToLive)). После завершения обработки кластер автоматически удаляется.
    
    По мере обработки новых срезов количество контейнеров в хранилище BLOB-объектов будет увеличиваться. Если эти контейнеры не используются для устранения неполадок с заданиями, удалите их — это позволит сократить расходы на хранение. Имена этих контейнеров указаны по шаблону `adf**yourdatafactoryname**-**linkedservicename**-datetimestamp`. Для удаления контейнеров в хранилище BLOB-объектов Azure используйте такие инструменты, как [Microsoft Storage Explorer](http://storageexplorer.com/) .
- В настоящее время расписание активируется с помощью выходного набора данных, поэтому его необходимо создать, даже если действие не создает никаких выходных данных. Если действие не принимает никаких входных данных, входной набор данных можно не создавать. 
- Копирование данных с помощью фабрики данных Azure в этом руководстве не рассматривается. Инструкции по копированию данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных Azure см. в [этой статье](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).


## <a name="use-server-explorer-to-view-data-factories"></a>Использование обозревателя серверов для просмотра фабрик данных
1. В **Visual Studio** щелкните **Вид** в меню и выберите **Обозреватель серверов**.
2. В окне обозревателя серверов разверните элементы **Azure** и **Фабрика данных**. Когда отобразится окно **Выполните вход в Visual Studio**, введите данные **учетной записи**, связанной с вашей подпиской Azure, и нажмите кнопку **Продолжить**. Введите **пароль** и нажмите кнопку **Войти**. Visual Studio пытается получить сведения обо всех фабриках данных Azure в подписке. В окне **Data Factory Task List** (Список задач фабрики данных) будет отображено состояние операции.

    ![Обозреватель серверов](./media/data-factory-build-your-first-pipeline-using-vs/server-explorer.png)
3. Щелкните фабрику данных правой кнопкой мыши и выберите пункт **Export Data Factory to New Project** (Экспорт фабрики данных в новый проект), чтобы создать проект Visual Studio на основе имеющейся фабрики данных.

    ![Экспорт фабрики данных](./media/data-factory-build-your-first-pipeline-using-vs/export-data-factory-menu.png)

## <a name="update-data-factory-tools-for-visual-studio"></a>Обновление средств фабрик данных для Visual Studio
Чтобы обновить средства фабрики данных Azure для Visual Studio, сделайте следующее:

1. Щелкните в меню пункт **Сервис** и выберите **Расширения и обновления**.
2. Выберите пункт **Обновления** слева, а затем — **Галерея Visual Studio**.
3. Выберите **Azure Data Factory tools for Visual Studio** (Средства фабрики данных Azure для Visual Studio) и нажмите кнопку **Обновить**. Если эта запись отсутствует, у вас уже установлена последняя версия средства.

## <a name="use-configuration-files"></a>Использование файлов конфигурации
Файлы конфигурации в Visual Studio можно использовать для индивидуальной настройки свойств связанных служб, таблиц и конвейеров для каждой среды.

Рассмотрим следующее определение JSON для связанной службы хранилища Azure. В нем нужно указать свойство **connectionString**. В зависимости от того, в какую среду вы развертываете сущности фабрики данных (среда разработки, среда тестирования или рабочая среда), значения accountname и accountkey будут разными. Это можно сделать с помощью отдельного файла конфигурации для каждой среды.

```json
{
    "name": "StorageLinkedService",
    "properties": {
        "type": "AzureStorage",
        "description": "",
        "typeProperties": {
            "connectionString": "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"
        }
    }
}
```

### <a name="add-a-configuration-file"></a>Добавление файла конфигурации
Добавьте файл конфигурации для каждой среды, выполнив следующие действия.   

1. Щелкните правой кнопкой мыши проект фабрики данных в решении Visual Studio и последовательно выберите **Добавить** и **Новый элемент**.
2. В левой части окна в списке установленных шаблонов последовательно выберите пункты **Конфигурация** и **Файл конфигурации**, укажите **имя** файла конфигурации и нажмите кнопку **Добавить**.

    ![Добавление файла конфигурации](./media/data-factory-build-your-first-pipeline-using-vs/add-config-file.png)
3. Добавьте параметры конфигурации и их значения в следующем формате:

    ```json
    {
        "$schema": "http://datafactories.schema.management.azure.com/vsschemas/V1/Microsoft.DataFactory.Config.json",
        "AzureStorageLinkedService1": [
            {
                "name": "$.properties.typeProperties.connectionString",
                "value": "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"
            }
        ],
        "AzureSqlLinkedService1": [
            {
                "name": "$.properties.typeProperties.connectionString",
                "value":  "Server=tcp:<Azure sql server name>.database.windows.net,1433;Database=<Azure Sql database>;User ID=<user name>;Password=<password>;Trusted_Connection=False;Encrypt=True;Connection Timeout=30"
            }
        ]
    }
    ```

    В этом примере настраивается свойство connectionString связанной службы хранилища Azure и связанной службы Azure SQL. Обратите внимание, что имя указывается с использованием синтаксиса [JsonPath](http://goessner.net/articles/JsonPath/).   

    Если JSON-файл содержит свойство, которое имеет массив значений (см. ниже):  

    ```json
    "structure": [
          {
              "name": "FirstName",
            "type": "String"
          },
          {
            "name": "LastName",
            "type": "String"
        }
    ],
    ```

    Настройте свойства, как показано в следующем файле конфигурации (используйте индекс, начинающийся с нуля).

    ```json
    {
        "name": "$.properties.structure[0].name",
        "value": "FirstName"
    }
    {
        "name": "$.properties.structure[0].type",
        "value": "String"
    }
    {
        "name": "$.properties.structure[1].name",
        "value": "LastName"
    }
    {
        "name": "$.properties.structure[1].type",
        "value": "String"
    }
    ```

### <a name="property-names-with-spaces"></a>Имена свойств c пробелами
Если имя свойства содержит пробелы, используйте квадратные скобки, как показано в следующем примере (имя сервера базы данных):

```json
 {
     "name": "$.properties.activities[1].typeProperties.webServiceParameters.['Database server name']",
     "value": "MyAsqlServer.database.windows.net"
 }
```

### <a name="deploy-solution-using-a-configuration"></a>Развертывание решения с помощью конфигурации
Во время публикации сущностей фабрики данных Azure в Visual Studio можно указать конфигурацию, которая будет использоваться для этой операции публикации.

Чтобы опубликовать сущности в проекте фабрики данных Azure с помощью файла конфигурации, выполните следующие действия.   

1. Щелкните правой кнопкой мыши проект фабрики данных и выберите пункт **Publish** (Опубликовать). Откроется диалоговое окно **Publish Items** (Публикация элементов).
2. На странице **Configure data factory** (Настройка фабрики данных) выберите имеющуюся фабрику данных или укажите значения для создания новой, а затем нажмите кнопку **Next** (Далее).   
3. На странице **Publish Items** (Публикация элементов) вы найдете раскрывающийся список **Select Deployment Config** (Выбор конфигурации развертывания) с доступными конфигурациями.

    ![Выбор файла конфигурации](./media/data-factory-build-your-first-pipeline-using-vs/select-config-file.png)
4. Выберите нужный **файл конфигурации** и нажмите кнопку **Next** (Далее).
5. Убедитесь, что на странице **Summary** (Сводка) отображается имя нужного JSON-файла, и нажмите кнопку **Next** (Далее).
6. По завершении развертывания нажмите кнопку **Готово** .

При развертывании значения из файла конфигурации используются, чтобы задать значения свойств в JSON-файлах, после чего сущности будут развернуты в службе фабрики данных Azure.   

## <a name="use-azure-key-vault"></a>Использование хранилища ключей Azure
Фиксировать конфиденциальные данные, например строки подключения, в репозитории кода не рекомендуется и часто противоречит политике безопасности. См. пример [ADF Secure Publish](https://github.com/Azure/Azure-DataFactory/tree/master/Samples/ADFSecurePublish) на портале GitHub, чтобы узнать о хранении конфиденциальных сведений в Azure Key Vault и их использовании при публикации сущностей фабрики данных. Расширение Secure Publish для Visual Studio позволяет хранить секреты в службе Key Vault, определяя в конфигурациях связанных служб и развертываний только ссылки на них. Эти ссылки разрешаются при публикации сущностей фабрики данных в Azure. Эти файлы затем можно зафиксировать в репозитории, не раскрывая конфиденциальные сведения.

## <a name="summary"></a>Сводка
Следуя инструкциям из этого руководства, вы создали фабрику данных Azure для обработки данных путем выполнения сценария Hive в кластере Hadoop HDInsight. Вы использовали редактор фабрики данных на портале Azure для выполнения следующих действий:  

1. создание **фабрики данных Azure**;
2. создание двух **связанных служб**.
   1. **Служба хранилища Azure** — связанная служба для связывания хранилища BLOB-объектов Azure, которое содержит входные и выходные файлы, с фабрикой данных.
   2. **Azure HDInsight** — связанная служба по запросу для связывания кластера HDInsight Hadoop с фабрикой данных. Фабрика данных Azure своевременно создает кластер HDInsight Hadoop для обработки входных данных и генерирования выходных данных.
3. Создание двух **наборов данных**, которые описывают входные и выходные данные для действия HDInsight Hive в конвейере.
4. Создание **конвейера** с действием **HDInsight Hive**.  

## <a name="next-steps"></a>Дальнейшие действия
В этой статье вы создали конвейер с действием преобразования (действие HDInsight), которое выполняет сценарий Hive в кластере HDInsight по требованию. Сведения о том, как копировать данные из хранилища BLOB-объектов Azure в SQL Azure с помощью действия копирования, см. в статье [Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).

Можно объединить в цепочку два действия (выполнить одно действие вслед за другим), настроив выходной набор данных одного действия как входной набор данных другого действия. Подробные сведения см. в статье [Планирование и исполнение с использованием фабрики данных](data-factory-scheduling-and-execution.md). 


## <a name="see-also"></a>См. также
| Раздел | ОПИСАНИЕ |
|:--- |:--- |
| [Конвейеры](data-factory-create-pipelines.md) |В этой статье описываются сведения о конвейерах и действиях в фабрике данных Azure, а также использование этих сущностей для создания комплексных рабочих процессов, управляемых данными, для конкретных бизнес-сценариев. |
| [Наборы данных](data-factory-create-datasets.md) |Эта статья поможет вам понять, что такое наборы данных в фабрике данных Azure. |
| [Действия по преобразованию данных](data-factory-data-transformation-activities.md) |В этой статье рассматриваются действия по преобразованию данных (например, преобразование HDInsight Hive, используемое в этом руководстве), поддерживаемые фабрикой данных Azure. |
| [Планирование и исполнение с использованием фабрики данных](data-factory-scheduling-and-execution.md) |Здесь объясняются аспекты планирования и исполнения в модели приложений фабрики данных. |
| [Мониторинг конвейеров фабрики данных Azure и управление ими с помощью нового приложения по мониторингу и управлению](data-factory-monitor-manage-app.md) |В этой статье описывается мониторинг и отладка конвейеров, а также управление ими с помощью приложения мониторинга и управления. |
