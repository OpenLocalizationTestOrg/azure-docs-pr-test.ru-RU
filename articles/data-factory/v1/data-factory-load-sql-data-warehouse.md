---
title: "Загрузка терабайтов данных в хранилище данных SQL | Документация Майкрософт"
description: "Демонстрируется загрузка 1 ТБ данных в хранилище данных SQL Azure с помощью фабрики данных Azure менее чем за 15 минут."
services: data-factory
documentationcenter: 
author: linda33wj
manager: jhubbard
editor: monicar
ms.assetid: a6c133c0-ced2-463c-86f0-a07b00c9e37f
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 01/10/2018
ms.author: jingwang
robots: noindex
ms.openlocfilehash: 3350645d4f173a6d0d007ff9095bb3115600a13b
ms.sourcegitcommit: 9cc3d9b9c36e4c973dd9c9028361af1ec5d29910
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/23/2018
---
# <a name="load-1-tb-into-azure-sql-data-warehouse-under-15-minutes-with-data-factory"></a>Загрузка 1 ТБ в хранилище данных SQL Azure с помощью фабрики данных менее чем за 15 минут
> [!NOTE]
> Статья относится к версии 1 фабрики данных, которая является общедоступной версией. Если вы используете версию 2 службы фабрики данных, которая находится на этапе предварительной версии, ознакомьтесь с [копированием данных в хранилище данных SQL Azure и из него с помощью фабрики данных версии 2](../connector-azure-sql-data-warehouse.md).


[Хранилище данных SQL Azure](../../sql-data-warehouse/sql-data-warehouse-overview-what-is.md) — это развернутая в облаке база данных, способная обрабатывать большие объемы реляционных и нереляционных данных.  Благодаря реализованной архитектуре вычислений с массовым параллелизмом (MPP) хранилище данных SQL оптимизировано для рабочих нагрузок хранилища корпоративных данных.  Оно предоставляет эластичность облака и гибкие возможности масштабирования хранилища и вычислительной мощности независимо друг от друга.

Приступить к работе с хранилищем данных SQL Azure теперь проще, чем когда-либо, с помощью **фабрики данных Azure**.  Фабрика данных Azure — это полностью управляемая облачная служба интеграции данных, которую можно использовать для заполнения хранилища данных SQL данными из существующей системы, экономя ценное время при оценке хранилища данных SQL и создании решений на основе его аналитики. Ниже приведены ключевые преимущества загрузки данных в хранилище данных SQL Azure с помощью фабрики данных Azure.

* **Простота настройки**: вам доступен 5-этапный интуитивно понятный мастер без необходимости создавать сценарии.
* **Расширенная поддержка хранилищ данных**: встроенная поддержка обширного набора локальных и облачных хранилищ.
* **Безопасность и совместимость**: данные передаются по протоколу HTTPS или ExpressRoute, а наличие глобальной службы гарантирует, что ваши данные никогда не покинут заданных географических границ.
* **Беспрецедентная производительность благодаря PolyBase**: применение Polybase является наиболее эффективным способом перемещения данных в хранилище данных SQL Azure. Используя функцию промежуточных больших двоичных объектов, можно достичь высокой скорости загрузки данных из хранилищ всех типов, а не только из хранилища BLOB-объектов Azure, которое по умолчанию поддерживается Polybase.

В этой статье показано, как использовать мастер копирования фабрики данных для загрузки 1 ТБ данных из хранилища BLOB-объектов Azure в хранилище данных SQL Azure менее чем за 15 минут с пропускной способностью более 1,2 Гбит/с.

Кроме того, статья содержит пошаговые инструкции по перемещению данных в хранилище данных SQL Azure с помощью мастера копирования.

> [!NOTE]
>  В статье [Перемещение данных в хранилище данных Azure SQL и из него с помощью фабрики данных Azure](data-factory-azure-sql-data-warehouse-connector.md) приведены общие сведения о возможностях фабрики данных по перемещению данных в хранилище данных SQL Azure и из него.
>
> Можно также создавать конвейеры с помощью портала Azure, Visual Studio PowerShell и т. д. Краткое пошаговое руководство с инструкциями по использованию действия копирования в фабрике данных Azure см. в статье [Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).  
>
>

## <a name="prerequisites"></a>предварительным требованиям
* Хранилище BLOB-объектов Azure: в этом эксперименте хранилище BLOB-объектов Azure (GRS) используется для хранения тестового набора данных TPC-H.  Если у вас нет учетной записи хранения Azure, узнайте, как [создать учетную запись хранения](../../storage/common/storage-create-storage-account.md#create-a-storage-account).
* Данные [TPC-H](http://www.tpc.org/tpch/): в качестве тестового набора данных мы будем использовать TPC-H.  Для этого необходимо использовать `dbgen` из набора средств TPC-H. Это поможет создать набор данных.  Можно скачать исходный код `dbgen` из [инструментов TPC](http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp) и скомпилировать его или скачать скомпилированный двоичный файл с сайта [GitHub](https://github.com/Azure/Azure-DataFactory/tree/master/Samples/TPCHTools).  Выполните dbgen.exe с приведенными ниже командами, чтобы создать неструктурированный файл размером в 1 ТБ для таблицы `lineitem`, распределенной на 10 файлов.

  * `Dbgen -s 1000 -S **1** -C 10 -T L -v`
  * `Dbgen -s 1000 -S **2** -C 10 -T L -v`
  * …
  * `Dbgen -s 1000 -S **10** -C 10 -T L -v`

    Теперь скопируйте созданные файлы в большой двоичный объект Azure.  Ознакомьтесь с разделом [Перемещение данных в локальную файловую систему или из нее с помощью фабрики данных Azure](data-factory-onprem-file-system-connector.md), чтобы узнать, как это можно сделать с помощью фабрики данных Azure.    
* Хранилище данных SQL Azure: в ходе этого эксперимента данные загружаются в хранилище данных SQL Azure, созданное с 6000 DWU.

    В разделе [Создание хранилища данных SQL Azure](../../sql-data-warehouse/sql-data-warehouse-get-started-provision.md) представлены подробные инструкции о том, как создать базу данных хранилища данных SQL.  Чтобы обеспечить максимально возможную производительность загрузки в хранилище данных SQL с помощью Polybase, мы выбираем максимальное число единиц использования хранилища данных, допустимое для параметра производительности — 6000 DWU.

  > [!NOTE]
  > Производительность загрузки из большого двоичного объекта Azure прямо пропорциональна количеству DWU, заданному для хранилища данных SQL.
  >
  > Загрузка 1 ТБ в 1000 DWU хранилища данных SQL занимает 87 минут (при пропускной способности около 200 Мбит/с). Загрузка 1 ТБ в 2000 DWU хранилища данных SQL занимает 46 минут (при пропускной способности около 380 Мбит/с). Загрузка 1 ТБ в 6000 DWU хранилища данных SQL занимает 14 минут (при пропускной способности около 1,2 Гбит/с).
  >
  >

    Чтобы создать хранилище данных SQL с 6000 DWU, сдвиньте ползунок "Производительность" вправо до конца.

    ![Ползунок "Производительность"](media/data-factory-load-sql-data-warehouse/performance-slider.png)

    В случае, если для существующей базы данных не настроено 6000 DWU, ее можно масштабировать с помощью портала Azure.  Перейдите к базе данных на портале Azure. На панели **Обзор** имеется кнопка **Масштаб**, как показано на следующем рисунке.

    ![Кнопка "Масштаб"](media/data-factory-load-sql-data-warehouse/scale-button.png)    

    Нажмите кнопку **Масштаб**, чтобы открыть приведенную ниже панель, передвиньте ползунок до максимального значения и нажмите кнопку **Сохранить**.

    ![Диалоговое окно "Масштаб"](media/data-factory-load-sql-data-warehouse/scale-dialog.png)

    В этом эксперименте данные загружаются в хранилище данных SQL Azure с помощью класса ресурсов `xlargerc`.

    Для получения наилучшей пропускной способности копирование нужно выполнить с помощью пользователя хранилища данных SQL, относящегося к классу ресурсов `xlargerc`.  Узнайте, как это сделать, ознакомившись с [примером изменения класса ресурсов пользователя](../../sql-data-warehouse/sql-data-warehouse-develop-concurrency.md).  
* Создайте схему целевой таблицы в базе данных хранилища данных SQL Azure, выполнив следующую инструкцию DDL.

    ```SQL  
    CREATE TABLE [dbo].[lineitem]
    (
        [L_ORDERKEY] [bigint] NOT NULL,
        [L_PARTKEY] [bigint] NOT NULL,
        [L_SUPPKEY] [bigint] NOT NULL,
        [L_LINENUMBER] [int] NOT NULL,
        [L_QUANTITY] [decimal](15, 2) NULL,
        [L_EXTENDEDPRICE] [decimal](15, 2) NULL,
        [L_DISCOUNT] [decimal](15, 2) NULL,
        [L_TAX] [decimal](15, 2) NULL,
        [L_RETURNFLAG] [char](1) NULL,
        [L_LINESTATUS] [char](1) NULL,
        [L_SHIPDATE] [date] NULL,
        [L_COMMITDATE] [date] NULL,
        [L_RECEIPTDATE] [date] NULL,
        [L_SHIPINSTRUCT] [char](25) NULL,
        [L_SHIPMODE] [char](10) NULL,
        [L_COMMENT] [varchar](44) NULL
    )
    WITH
    (
        DISTRIBUTION = ROUND_ROBIN,
        CLUSTERED COLUMNSTORE INDEX
    )
    ```
Мы выполнили необходимые предварительные действия и готовы к настройке действия копирования с помощью мастера копирования.

## <a name="launch-copy-wizard"></a>Запуск мастера копирования
1. Войдите на [портал Azure](https://portal.azure.com).
2. Нажмите кнопку **+ Создать** в верхнем левом углу, выберите **Аналитика** и щелкните **Фабрика данных**.
3. В колонке **Создать фабрику данных** выполните следующие действия.

   1. В поле **Имя** введите **LoadIntoSQLDWDataFactory**.
       Имя фабрики данных Azure должно быть глобально уникальным. При возникновении ошибки **Имя фабрики данных LoadIntoSQLDWDataFactory недоступно** измените имя этой фабрики данных (например, на <ваше_имя>LoadIntoSQLDWDataFactory) и попробуйте создать ее снова. Ознакомьтесь с разделом [Фабрика данных — правила именования](data-factory-naming-rules.md) , чтобы узнать о правилах именования артефактов фабрики данных.  
   2. Выберите свою **подписку Azure**.
   3. Для группы ресурсов выполните одно из следующих действий.
      1. а) выберите **Использовать существующую** и укажите имеющуюся группу ресурсов;
      2. выберите **Создать** и введите имя для группы ресурсов.
   4. Укажите **расположение** фабрики данных.
   5. Установите флажок **Закрепить на панели мониторинга** в нижней части колонки.  
   6. Нажмите кнопку **Создать**.
4. После создания вы увидите колонку **Фабрика данных**, как показано на рисунке ниже.

   ![Домашняя страница фабрики данных](media/data-factory-load-sql-data-warehouse/data-factory-home-page-copy-data.png)
5. Чтобы запустить **мастер копирования**, на домашней странице фабрики данных щелкните **Копирование данных**.

   > [!NOTE]
   > Если веб-браузер завис на действии "Авторизация...", отключите параметр или снимите флажок **Block third party cookies and site data** (Блокировать сторонние файлы cookie и данные сайта). Либо оставьте флажок и создайте исключение для адреса **login.microsoftonline.com**, а затем попробуйте запустить мастер еще раз.
   >
   >

## <a name="step-1-configure-data-loading-schedule"></a>Шаг 1. Настройка расписания загрузки данных
Первым шагом является настройка расписания загрузки данных.  

Вот что нужно сделать на странице **Свойства** :

1. В качестве **имени задачи** введите **CopyFromBlobToAzureSqlDataWarehouse**.
2. Выберите параметр **Run once now** (Запустить сейчас один раз).   
3. Нажмите кнопку **Далее**.  

    ![Мастер копирования — страница "Свойства"](media/data-factory-load-sql-data-warehouse/copy-wizard-properties-page.png)

## <a name="step-2-configure-source"></a>Шаг 2. Настройка источника
В этом разделе описываются шаги для настройки источника: большого двоичного объекта Azure, содержащего файлы размером в 1 ТБ с элементами строк TPC-H.

1. Выберите **хранилище BLOB-объектов Azure** в качестве хранилища и нажмите кнопку **Далее**.

    ![Мастер копирования — страница "Выбрать источник"](media/data-factory-load-sql-data-warehouse/select-source-connection.png)

2. Укажите сведения о подключении для учетной записи хранения BLOB-объектов Azure и нажмите кнопку **Далее**.

    ![Мастер копирования — сведения о подключении к источнику](media/data-factory-load-sql-data-warehouse/source-connection-info.png)

3. Выберите **папку**, содержащую файлы с элементами строк TPC-H, и нажмите кнопку **Далее**.

    ![Мастер копирования — выбор папки входных данных](media/data-factory-load-sql-data-warehouse/select-input-folder.png)

4. После нажатия кнопки **Далее** параметры формата файлов определяются автоматически.  Убедитесь, что разделителем столбцов является "|", а не запятая ",", используемая умолчанию.  Просмотрев данные, нажмите кнопку **Далее**.

    ![Мастер копирования — параметры формата файлов](media/data-factory-load-sql-data-warehouse/file-format-settings.png)

## <a name="step-3-configure-destination"></a>Шаг 3. Настройка назначения
В этом разделе показано, как настроить назначение: таблицу `lineitem` в базе данных хранилища данных SQL Azure.

1. Выберите **хранилище данных SQL Azure** в качестве назначения и нажмите кнопку **Далее**.

    ![Мастер копирования — выбор целевого хранилища данных](media/data-factory-load-sql-data-warehouse/select-destination-data-store.png)

2. Введите сведения о подключении для хранилища данных SQL Azure.  Обязательно укажите пользователя, который является участником роли `xlargerc` (подробные инструкции приведены в разделе **предварительных требований**), и нажмите кнопку **Далее**.

    ![Мастер копирования — сведения о подключении к целевому хранилищу](media/data-factory-load-sql-data-warehouse/destination-connection-info.png)

3. Выберите целевую таблицу и нажмите кнопку **Далее**.

    ![Мастер копирования — страница сопоставления таблиц](media/data-factory-load-sql-data-warehouse/table-mapping-page.png)

4. На странице сопоставления схемы оставьте флажок "Apply column mapping" (Применить сопоставление столбцов) снятым и нажмите кнопку **Далее**.

## <a name="step-4-performance-settings"></a>Шаг 4. Настройки производительности

Флажок **Allow polybase** (Разрешить использование PolyBase) установлен по умолчанию.  Нажмите кнопку **Далее**.

![Мастер копирования — страница сопоставления столбцов](media/data-factory-load-sql-data-warehouse/performance-settings-page.png)

## <a name="step-5-deploy-and-monitor-load-results"></a>Шаг 5. Развертывание и мониторинг результатов нагрузки
1. Нажмите кнопку **Готово**, чтобы осуществить развертывание.

    ![Мастер копирования — страница сводки](media/data-factory-load-sql-data-warehouse/summary-page.png)

2. После завершения развертывания щелкните `Click here to monitor copy pipeline`, чтобы отслеживать ход выполнения копирования. Выберите конвейер копирования, созданный при работе со списком **Окна действий**.

    ![Мастер копирования — страница сводки](media/data-factory-load-sql-data-warehouse/select-pipeline-monitor-manage-app.png)

    Можно просмотреть сведения о выполнении копирования в **Activity Window Explorer** в правой панели, в том числе объем данных, считанных из источника и записанных в назначение, продолжительность и среднюю пропускную способность копирования.

    Как видно на следующем снимке экрана, копирование 1 ТБ из хранилища BLOB-объектов Azure в хранилище данных SQL занимает 14 минут, то есть фактическая пропускная способность составила 1,22 Гбит/с!

    ![Мастер копирования — диалоговое окно успешного выполнения](media/data-factory-load-sql-data-warehouse/succeeded-info.png)

## <a name="best-practices"></a>Рекомендации
Ниже приведены некоторые советы и рекомендации по выполнению базы данных хранилища данных SQL Azure.

* При загрузке в кластеризованный индекс columnstore используйте класс ресурсов большего размера.
* Чтобы повысить эффективность соединений, рассмотрите возможность использования хэш-распределения по выбранному столбцу вместо используемого по умолчанию циклического распределения.
* Чтобы ускорить загрузку, рекомендуется использовать кучу для хранения временных данных.
* Сформируйте статистику после окончания загрузки хранилища данных SQL Azure.

Дополнительные сведения см. в разделе [Рекомендации по использованию хранилища данных SQL Azure](../../sql-data-warehouse/sql-data-warehouse-best-practices.md).

## <a name="next-steps"></a>Дополнительная информация
* [Мастер копирования фабрики данных](data-factory-copy-wizard.md). В этой статье приведены сведения о мастере копирования.
* [Руководство по настройке производительности действия копирования](data-factory-copy-activity-performance.md). Эта статья содержит эталонные измерения производительности и руководство по настройке.
