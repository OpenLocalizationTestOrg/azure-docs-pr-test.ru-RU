---
title: "Создание моделей текстовой аналитики в Студии машинного обучения Azure | Документация Майкрософт"
description: "Узнайте, как с помощью модулей для предварительной обработки текста, N-грамм и хэширования признаков создавать модели текстовой аналитики в Студии машинного обучения Azure."
services: machine-learning
documentationcenter: 
author: rastala
manager: jhubbard
editor: 
ms.assetid: 08cd6723-3ae6-4e99-a924-e650942e461b
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 12/06/2016
ms.author: roastala
ms.openlocfilehash: 342e81e2497d292ca730bea59e03182d316ffec3
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/11/2017
---
# <a name="create-text-analytics-models-in-azure-machine-learning-studio"></a><span data-ttu-id="573b3-103">Создание моделей текстовой аналитики в Студии машинного обучения Azure</span><span class="sxs-lookup"><span data-stu-id="573b3-103">Create text analytics models in Azure Machine Learning Studio</span></span>
<span data-ttu-id="573b3-104">Машинное обучения Azure можно использовать для создания и ввода в эксплуатацию моделей текстовой аналитики.</span><span class="sxs-lookup"><span data-stu-id="573b3-104">You can use Azure Machine Learning to build and operationalize text analytics models.</span></span> <span data-ttu-id="573b3-105">Эти модели могут быть полезными, например, при решении проблем с классификацией документов или анализом мнений.</span><span class="sxs-lookup"><span data-stu-id="573b3-105">These models can help you solve, for example, document classification or sentiment analysis problems.</span></span>

<span data-ttu-id="573b3-106">В эксперименте текстовой аналитики, как правило, выполняются следующие действия:</span><span class="sxs-lookup"><span data-stu-id="573b3-106">In a text analytics experiment, you would typically:</span></span>

1. <span data-ttu-id="573b3-107">Очистка и предварительная обработка набора текстовых данных.</span><span class="sxs-lookup"><span data-stu-id="573b3-107">Clean and preprocess text dataset</span></span>
2. <span data-ttu-id="573b3-108">Извлечение из предварительно обработанного текста векторов числовых признаков.</span><span class="sxs-lookup"><span data-stu-id="573b3-108">Extract numeric feature vectors from pre-processed text</span></span>
3. <span data-ttu-id="573b3-109">Обучение модели классификации или регрессии.</span><span class="sxs-lookup"><span data-stu-id="573b3-109">Train classification or regression model</span></span>
4. <span data-ttu-id="573b3-110">Оценка и проверка модели.</span><span class="sxs-lookup"><span data-stu-id="573b3-110">Score and validate the model</span></span>
5. <span data-ttu-id="573b3-111">Развертывание модели в рабочей среде.</span><span class="sxs-lookup"><span data-stu-id="573b3-111">Deploy the model to production</span></span>

<span data-ttu-id="573b3-112">В данном учебнике эти действия демонстрируются на примере модели анализа мнений с использованием набора данных Amazon Book Reviews (Обзоры книг на Amazon). См. исследовательскую работу "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification" (Биографии, Болливуд, бум-боксы и блендеры: адаптация домена для классификации мнений пользователей), авторы — Джон Блитцер (John Blitzer), Марк Дредзе (Mark Dredze) и Фернандо Перейра (Fernando Pereira); Ассоциация компьютерной лингвистики (ACL), 2007 г. Этот набор данных состоит из оценок в обзоре (1-2 или 4-5) и текста в произвольной форме.</span><span class="sxs-lookup"><span data-stu-id="573b3-112">In this tutorial, you learn these steps as we walk through a sentiment analysis model using Amazon Book Reviews dataset (see this research paper “Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification” by John Blitzer, Mark Dredze, and Fernando Pereira; Association of Computational Linguistics (ACL), 2007.) This dataset consists of review scores (1-2 or 4-5) and a free-form text.</span></span> <span data-ttu-id="573b3-113">Целью является прогнозирование оценки в обзоре: низкая (1-2) или высокая (4-5).</span><span class="sxs-lookup"><span data-stu-id="573b3-113">The goal is to predict the review score: low (1-2) or high (4-5).</span></span>

<span data-ttu-id="573b3-114">Эксперименты, рассматриваемые в этом учебнике, можно найти в коллекции Cortana Intelligence:</span><span class="sxs-lookup"><span data-stu-id="573b3-114">You can find experiments covered in this tutorial at Cortana Intelligence Gallery:</span></span>

[<span data-ttu-id="573b3-115">Predict Book Reviews (Прогнозирование оценок в обзоре книг).</span><span class="sxs-lookup"><span data-stu-id="573b3-115">Predict Book Reviews</span></span>](https://gallery.cortanaintelligence.com/Experiment/Predict-Book-Reviews-1)

[<span data-ttu-id="573b3-116">Predict Book Reviews - Predictive Experiment (Прогнозирование оценок в обзоре книг — прогнозной эксперимент).</span><span class="sxs-lookup"><span data-stu-id="573b3-116">Predict Book Reviews - Predictive Experiment</span></span>](https://gallery.cortanaintelligence.com/Experiment/Predict-Book-Reviews-Predictive-Experiment-1)

## <a name="step-1-clean-and-preprocess-text-dataset"></a><span data-ttu-id="573b3-117">Шаг 1. Очистка и предварительная обработка набора текстовых данных</span><span class="sxs-lookup"><span data-stu-id="573b3-117">Step 1: Clean and preprocess text dataset</span></span>
<span data-ttu-id="573b3-118">Мы начинаем эксперимент с разделения оценок в обзоре на категориальные контейнеры низких и высоких оценок, чтобы сформулировать проблему как двухклассовую классификацию.</span><span class="sxs-lookup"><span data-stu-id="573b3-118">We begin the experiment by dividing the review scores into categorical low and high buckets to formulate the problem as two-class classification.</span></span> <span data-ttu-id="573b3-119">Для этого мы используем модули [Edit Metadata](https://msdn.microsoft.com/library/azure/dn905986.aspx) (Изменение метаданных) и [Group Categorical Values](https://msdn.microsoft.com/library/azure/dn906014.aspx) (Значения категорий группы).</span><span class="sxs-lookup"><span data-stu-id="573b3-119">We use [Edit Metadata](https://msdn.microsoft.com/library/azure/dn905986.aspx) and [Group Categorical Values](https://msdn.microsoft.com/library/azure/dn906014.aspx) modules.</span></span>

![Создание метки](./media/machine-learning-text-analytics-module-tutorial/create-label.png)

<span data-ttu-id="573b3-121">Затем мы выполняем очистку текста с помощью модуля [Preprocess Text](https://msdn.microsoft.com/library/azure/mt762915.aspx) (Предварительная обработка текста).</span><span class="sxs-lookup"><span data-stu-id="573b3-121">Then, we clean the text using [Preprocess Text](https://msdn.microsoft.com/library/azure/mt762915.aspx) module.</span></span> <span data-ttu-id="573b3-122">Очистка снижает количество шумов в наборе данных, помогает найти наиболее важные признаки и повысить точность конечной модели.</span><span class="sxs-lookup"><span data-stu-id="573b3-122">The cleaning reduces the noise in the dataset, help you find the most important features, and improve the accuracy of the final model.</span></span> <span data-ttu-id="573b3-123">Удаляются стоп-слова (такие как артикли или частицы), числа, специальные знаки, повторяющиеся знаки, адреса электронной почты и URL-адреса.</span><span class="sxs-lookup"><span data-stu-id="573b3-123">We remove stopwords - common words such as "the" or "a" - and numbers, special characters, duplicated characters, email addresses, and URLs.</span></span> <span data-ttu-id="573b3-124">Также текст преобразовывается в нижний регистр, выполняется лемматизация слов и определяются границы предложений, которые затем обозначаются в предварительно обработанном тексте символом |||.</span><span class="sxs-lookup"><span data-stu-id="573b3-124">We also convert the text to lowercase, lemmatize the words, and detect sentence boundaries that are then indicated by "|||" symbol in pre-processed text.</span></span>

![Preprocess Text](./media/machine-learning-text-analytics-module-tutorial/preprocess-text.png)

<span data-ttu-id="573b3-126">Можно ли использовать настраиваемый список стоп-слов?</span><span class="sxs-lookup"><span data-stu-id="573b3-126">What if you want to use a custom list of stopwords?</span></span> <span data-ttu-id="573b3-127">Его можно передать в качестве дополнительных входных данных.</span><span class="sxs-lookup"><span data-stu-id="573b3-127">You can pass it in as optional input.</span></span> <span data-ttu-id="573b3-128">Также можно использовать настраиваемые регулярные выражения с синтаксисом C# для замены подстрок и удаления слов по частям речи (существительные, глаголы или прилагательные).</span><span class="sxs-lookup"><span data-stu-id="573b3-128">You can also use custom C# syntax regular expression to replace substrings, and remove words by part of speech: nouns, verbs, or adjectives.</span></span>

<span data-ttu-id="573b3-129">Когда предварительная обработка завершена, мы разделяем данные на наборы для обучения и тестирования.</span><span class="sxs-lookup"><span data-stu-id="573b3-129">After the preprocessing is complete, we split the data into train and test sets.</span></span>

## <a name="step-2-extract-numeric-feature-vectors-from-pre-processed-text"></a><span data-ttu-id="573b3-130">Шаг 2. Извлечение из предварительно обработанного текста векторов числовых признаков</span><span class="sxs-lookup"><span data-stu-id="573b3-130">Step 2: Extract numeric feature vectors from pre-processed text</span></span>
<span data-ttu-id="573b3-131">Чтобы создать модель из текстовых данных, как правило, требуется преобразовать произвольный текст в векторы числовых признаков.</span><span class="sxs-lookup"><span data-stu-id="573b3-131">To build a model for text data, you typically have to convert free-form text into numeric feature vectors.</span></span> <span data-ttu-id="573b3-132">В этом примере для преобразования текстовых данных в такой формат используется модуль [Extract N-Gram Features from Text](https://msdn.microsoft.com/library/azure/mt762916.aspx) (Извлечение из текста признаков N-грамм).</span><span class="sxs-lookup"><span data-stu-id="573b3-132">In this example, we use [Extract N-Gram Features from Text](https://msdn.microsoft.com/library/azure/mt762916.aspx) module to transform the text data to such format.</span></span> <span data-ttu-id="573b3-133">Этот модуль принимает столбец слов, разделенных пробелами, и вычисляет словарь слов, или N-граммы слов, которые отображаются в наборе данных.</span><span class="sxs-lookup"><span data-stu-id="573b3-133">This module takes a column of whitespace-separated words and computes a dictionary of words, or N-grams of words, that appear in your dataset.</span></span> <span data-ttu-id="573b3-134">Затем модуль подсчитывает, сколько раз каждое слово, или N-грамм, встречается в каждой записи, и создает на основании этих подсчетов векторы признаков.</span><span class="sxs-lookup"><span data-stu-id="573b3-134">Then, it counts how many times each word, or N-gram, appears in each record, and creates feature vectors from those counts.</span></span> <span data-ttu-id="573b3-135">В этом учебнике для N-грамм задано значение 2, поэтому наши векторы признаков включают отдельные слова и сочетания из двух последовательных слов.</span><span class="sxs-lookup"><span data-stu-id="573b3-135">In this tutorial, we set N-gram size to 2, so our feature vectors include single words and combinations of two subsequent words.</span></span>

![Извлечение N-грамм](./media/machine-learning-text-analytics-module-tutorial/extract-ngrams.png)

<span data-ttu-id="573b3-137">При подсчете N-грамм применяются взвешенные значения TF*IDF (частота условия — инверсная частота в документе).</span><span class="sxs-lookup"><span data-stu-id="573b3-137">We apply TF*IDF (Term Frequency Inverse Document Frequency) weighting to N-gram counts.</span></span> <span data-ttu-id="573b3-138">Этот подход добавляет взвешенные значения слов, которые часто встречаются в одной записи, но редко — по всему набору данных.</span><span class="sxs-lookup"><span data-stu-id="573b3-138">This approach adds weight of words that appear frequently in a single record but are rare across the entire dataset.</span></span> <span data-ttu-id="573b3-139">Другие варианты включают двоичные значения, TF и взвешенные значения диаграммы.</span><span class="sxs-lookup"><span data-stu-id="573b3-139">Other options include binary, TF, and graph weighing.</span></span>

<span data-ttu-id="573b3-140">Такие текстовые признаки часто обладают высокой размерностью.</span><span class="sxs-lookup"><span data-stu-id="573b3-140">Such text features often have high dimensionality.</span></span> <span data-ttu-id="573b3-141">Например, если ваш текст состоит из 100 000 уникальных слов, то пространство признаков будет иметь 100 000 размеров, или даже больше (когда используются N-граммы).</span><span class="sxs-lookup"><span data-stu-id="573b3-141">For example, if your corpus has 100,000 unique words, your feature space would have 100,000 dimensions, or more if N-grams are used.</span></span> <span data-ttu-id="573b3-142">Модуль "Extract N-Gram Features from Text" (Извлечение из текста признаков N-грамм) предоставляет набор параметров для уменьшения размерности.</span><span class="sxs-lookup"><span data-stu-id="573b3-142">The Extract N-Gram Features module gives you a set of options to reduce the dimensionality.</span></span> <span data-ttu-id="573b3-143">Можно исключить слова, которые являются короткими, длинными, либо встречаются слишком редко или часто, чтобы влиять на прогнозное значение.</span><span class="sxs-lookup"><span data-stu-id="573b3-143">You can choose to exclude words that are short or long, or too uncommon or too frequent to have significant predictive value.</span></span> <span data-ttu-id="573b3-144">В этом учебнике мы исключаем N-граммы, которые встречаются реже чем в 5 записях или чаще чем в 80 % записей.</span><span class="sxs-lookup"><span data-stu-id="573b3-144">In this tutorial, we exclude N-grams that appear in fewer than 5 records or in more than 80% of records.</span></span>

<span data-ttu-id="573b3-145">Вы также можете использовать выбор признаков, чтобы отбирались только те признаки, которые максимально связаны с целью прогноза.</span><span class="sxs-lookup"><span data-stu-id="573b3-145">Also, you can use feature selection to select only those features that are the most correlated with your prediction target.</span></span> <span data-ttu-id="573b3-146">Мы используем выбор признаков хи-квадрат, чтобы отобрать 1000 признаков.</span><span class="sxs-lookup"><span data-stu-id="573b3-146">We use Chi-Squared feature selection to select 1000 features.</span></span> <span data-ttu-id="573b3-147">Словарь выбранных слов или N-грамм можно просмотреть, щелкнув правую часть выходных данных модуля "Extract N-Gram Features from Text" (Извлечение из текста признаков N-грамм).</span><span class="sxs-lookup"><span data-stu-id="573b3-147">You can view the vocabulary of selected words or N-grams by clicking the right output of Extract N-grams module.</span></span>

<span data-ttu-id="573b3-148">В качестве альтернативы этому модулю можно использовать модуль "Функции хэширования".</span><span class="sxs-lookup"><span data-stu-id="573b3-148">As an alternative approach to using Extract N-Gram Features, you can use Feature Hashing module.</span></span> <span data-ttu-id="573b3-149">Однако имейте в виду, что модуль [Функции хэширования](https://msdn.microsoft.com/library/azure/dn906018.aspx) не имеет встроенной функции выбора признаков или взвешенных значений TF*IDF.</span><span class="sxs-lookup"><span data-stu-id="573b3-149">Note though that [Feature Hashing](https://msdn.microsoft.com/library/azure/dn906018.aspx) does not have build-in feature selection capabilities, or TF*IDF weighing.</span></span>

## <a name="step-3-train-classification-or-regression-model"></a><span data-ttu-id="573b3-150">Шаг 3. Обучение модели классификации или регрессии</span><span class="sxs-lookup"><span data-stu-id="573b3-150">Step 3: Train classification or regression model</span></span>
<span data-ttu-id="573b3-151">Итак, текст преобразован в столбцы числовых признаков.</span><span class="sxs-lookup"><span data-stu-id="573b3-151">Now the text has been transformed to numeric feature columns.</span></span> <span data-ttu-id="573b3-152">Набор данных по-прежнему содержит строковые столбцы из предыдущих шагов, поэтому для их исключения мы используем модуль "Select Columns in Dataset" (Выбор столбцов в наборе данных).</span><span class="sxs-lookup"><span data-stu-id="573b3-152">The dataset still contains string columns from previous stages, so we use Select Columns in Dataset to exclude them.</span></span>

<span data-ttu-id="573b3-153">Затем мы используем [двухклассовую логистическую регрессию](https://msdn.microsoft.com/library/azure/dn905994.aspx) для прогнозирования цели: высокая или низкая оценка в обзоре.</span><span class="sxs-lookup"><span data-stu-id="573b3-153">We then use [Two-Class Logistic Regression](https://msdn.microsoft.com/library/azure/dn905994.aspx) to predict our target: high or low review score.</span></span> <span data-ttu-id="573b3-154">На этом этапе задача текстовой аналитики преобразовывается в обычную задачу классификации.</span><span class="sxs-lookup"><span data-stu-id="573b3-154">At this point, the text analytics problem has been transformed into a regular classification problem.</span></span> <span data-ttu-id="573b3-155">Для улучшения модели можно воспользоваться инструментами, доступными в Машинном обучении Azure.</span><span class="sxs-lookup"><span data-stu-id="573b3-155">You can use the tools available in Azure Machine Learning to improve the model.</span></span> <span data-ttu-id="573b3-156">Например, можно поэкспериментировать с разными классификаторами, чтобы узнать, насколько точные результаты они предоставляют, или воспользоваться настройкой гиперпараметров для повышения точности.</span><span class="sxs-lookup"><span data-stu-id="573b3-156">For example, you can experiment with different classifiers to find out how accurate results they give, or use hyperparameter tuning to improve the accuracy.</span></span>

![Обучение и оценка](./media/machine-learning-text-analytics-module-tutorial/scoring-text.png)

## <a name="step-4-score-and-validate-the-model"></a><span data-ttu-id="573b3-158">Шаг 4. Оценка и проверка модели</span><span class="sxs-lookup"><span data-stu-id="573b3-158">Step 4: Score and validate the model</span></span>
<span data-ttu-id="573b3-159">Как проверить обученную модель?</span><span class="sxs-lookup"><span data-stu-id="573b3-159">How would you validate the trained model?</span></span> <span data-ttu-id="573b3-160">Мы оцениваем ее на основе тестового набора данных и анализируем точность.</span><span class="sxs-lookup"><span data-stu-id="573b3-160">We score it against the test dataset and evaluate the accuracy.</span></span> <span data-ttu-id="573b3-161">Однако модель усвоила словарь N-грамм и их взвешенные значения из набора данных для обучения.</span><span class="sxs-lookup"><span data-stu-id="573b3-161">However, the model learned the vocabulary of N-grams and their weights from the training dataset.</span></span> <span data-ttu-id="573b3-162">Поэтому при извлечении признаков из тестовых данных мы должны использовать этот словарь и эти взвешенные значения, а не создавать словарь заново.</span><span class="sxs-lookup"><span data-stu-id="573b3-162">Therefore, we should use that vocabulary and those weights when extracting features from test data, as opposed to creating the vocabulary anew.</span></span> <span data-ttu-id="573b3-163">Следовательно, мы добавляем модуль "Extract N-Gram Features from Text" (Извлечение из текста признаков N-грамм) в ветвь оценки эксперимента, подключаем выходной словарь из ветви обучения и выбираем режим словаря "только для чтения".</span><span class="sxs-lookup"><span data-stu-id="573b3-163">Therefore, we add Extract N-Gram Features module to the scoring branch of the experiment, connect the output vocabulary from training branch, and set the vocabulary mode to read-only.</span></span> <span data-ttu-id="573b3-164">Мы также отключаем фильтрацию N-грамм по частоте, задав минимальное значение — 1 экземпляр, а максимальное значение — 100 %, и отключаем выбор признаков.</span><span class="sxs-lookup"><span data-stu-id="573b3-164">We also disable the filtering of N-grams by frequency by setting the minimum to 1 instance and maximum to 100%, and turn off the feature selection.</span></span>

<span data-ttu-id="573b3-165">После преобразования текстового столбца в тестовых данных в столбцы числовых признаков мы исключаем строковые столбцы из предыдущих шагов, как в ветви обучения.</span><span class="sxs-lookup"><span data-stu-id="573b3-165">After the text column in test data has been transformed to numeric feature columns, we exclude the string columns from previous stages like in training branch.</span></span> <span data-ttu-id="573b3-166">Затем мы используем модуль "Score Model" (Оценка модели) для выполнения прогнозов и модуль "Evaluate Model" (Анализ модели) для оценки точности.</span><span class="sxs-lookup"><span data-stu-id="573b3-166">We then use Score Model module to make predictions and Evaluate Model module to evaluate the accuracy.</span></span>

## <a name="step-5-deploy-the-model-to-production"></a><span data-ttu-id="573b3-167">Шаг 5. Развертывание модели в рабочей среде</span><span class="sxs-lookup"><span data-stu-id="573b3-167">Step 5: Deploy the model to production</span></span>
<span data-ttu-id="573b3-168">Модель почти готова к развертыванию в рабочей среде.</span><span class="sxs-lookup"><span data-stu-id="573b3-168">The model is almost ready to be deployed to production.</span></span> <span data-ttu-id="573b3-169">Если модель развернута как веб-служба, то в качестве входных данных она принимает строку с текстом в произвольной форме, а возвращает прогнозную оценку — "высокая" или "низкая".</span><span class="sxs-lookup"><span data-stu-id="573b3-169">When deployed as web service, it takes free-form text string as input, and return a prediction "high" or "low."</span></span> <span data-ttu-id="573b3-170">Она использует усвоенный словарь N-грамм для преобразования текста в признаки, а обученную модель логистической регрессии — для формирования прогноза на основе этих признаков.</span><span class="sxs-lookup"><span data-stu-id="573b3-170">It uses the learned N-gram vocabulary to transform the text to features, and trained logistic regression model to make a prediction from those features.</span></span> 

<span data-ttu-id="573b3-171">Чтобы настроить прогнозный эксперимент, сначала необходимо сохранить словарь N-грамм как набор данных, а также обученную модель логистической регрессии из ветви обучения эксперимента.</span><span class="sxs-lookup"><span data-stu-id="573b3-171">To set up the predictive experiment, we first save the N-gram vocabulary as dataset, and the trained logistic regression model from the training branch of the experiment.</span></span> <span data-ttu-id="573b3-172">Затем мы сохраняем эксперимент с помощью команды "Сохранить как", чтобы создать диаграмму эксперимента для прогнозного эксперимента.</span><span class="sxs-lookup"><span data-stu-id="573b3-172">Then, we save the experiment using "Save As" to create an experiment graph for predictive experiment.</span></span> <span data-ttu-id="573b3-173">Мы удаляем из эксперимента модуль "Split Data" (Разделение данных) и ветвь обучения.</span><span class="sxs-lookup"><span data-stu-id="573b3-173">We remove the Split Data module and the training branch from the experiment.</span></span> <span data-ttu-id="573b3-174">Затем мы подключаем сохраненные ранее словарь N-грамм и модель к модулям "Extract N-Gram Features from Text" (Извлечение из текста признаков N-грамм) и "Score Model" (Оценка модели) соответственно.</span><span class="sxs-lookup"><span data-stu-id="573b3-174">We then connect the previously saved N-gram vocabulary and model to Extract N-Gram Features and Score Model modules, respectively.</span></span> <span data-ttu-id="573b3-175">Кроме того, мы удаляем модуль "Evaluate Model" (Анализ модели).</span><span class="sxs-lookup"><span data-stu-id="573b3-175">We also remove the Evaluate Model module.</span></span>

<span data-ttu-id="573b3-176">Вставляем модуль "Select Columns in Dataset" (Выбор столбцов в наборе данных) перед модулем "Preprocess Text" (Предварительная обработка текста), чтобы удалить столбец меток, и снимаем флажок "Append score column to dataset" (Добавить в набор данных столбец оценок) в модуле "Score Model" (Оценка модели).</span><span class="sxs-lookup"><span data-stu-id="573b3-176">We insert Select Columns in Dataset module before Preprocess Text module to remove the label column, and unselect "Append score column to dataset" option in Score Module.</span></span> <span data-ttu-id="573b3-177">Таким образом, веб-служба не запрашивает метку, которую она пытается прогнозировать, и в результате входные признаки не выводятся на экран.</span><span class="sxs-lookup"><span data-stu-id="573b3-177">That way, the web service does not request the label it is trying to predict, and does not echo the input features in response.</span></span>

![Прогнозный эксперимент](./media/machine-learning-text-analytics-module-tutorial/predictive-text.png)

<span data-ttu-id="573b3-179">Теперь у нас готов эксперимент, который можно опубликовать как веб-службу и вызывать с помощью интерфейсов API "запрос-ответ" или пакетного выполнения.</span><span class="sxs-lookup"><span data-stu-id="573b3-179">Now we have an experiment that can be published as a web service and called using request-response or batch execution APIs.</span></span>

## <a name="next-steps"></a><span data-ttu-id="573b3-180">Дальнейшие действия</span><span class="sxs-lookup"><span data-stu-id="573b3-180">Next Steps</span></span>
<span data-ttu-id="573b3-181">Дополнительные сведения о модулях текстовой аналитики см. в [документации MSDN](https://msdn.microsoft.com/library/azure/dn905886.aspx).</span><span class="sxs-lookup"><span data-stu-id="573b3-181">Learn about text analytics modules from [MSDN documentation](https://msdn.microsoft.com/library/azure/dn905886.aspx).</span></span>

