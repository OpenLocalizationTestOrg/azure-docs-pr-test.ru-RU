---
title: "Платформы и средства Azure для командных проектов обработки и анализа данных | Документация Майкрософт"
description: "Список и описание ресурсов для получения и анализа данных, которые предприятия могут использовать для стандартизации командного процесса обработки и анализа данных."
documentationcenter: 
author: bradsev
manager: cgronlun
editor: cgronlun
ms.assetid: 
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 09/04/2017
ms.author: bradsev;
ms.openlocfilehash: 3ec2eaaf4e8d54e7b1ea3d272c47eac96451f317
ms.sourcegitcommit: f8437edf5de144b40aed00af5c52a20e35d10ba1
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/03/2017
---
# <a name="platforms-and-tools-for-data-science-team-projects"></a>Платформы и средства для командных проектов обработки и анализа данных

Корпорация Майкрософт предоставляет широкий спектр служб и ресурсов для получения и анализа данных как для облачных, так и для локальных платформ. Развертывание этих ресурсов повысит эффективность и масштабируемость выполнения проектов по обработке и анализу данных. Для команд, выполняющих проекты по обработке и анализу данных, предлагается [командный процесс обработки и анализа данных](overview.md) (TDSP), который обеспечивает отслеживаемость, контроль версий и совместную работу.  Роли и задачи сотрудников в области стандартизации обработки и анализа данных описаны в статье [Team Data Science Process roles and tasks](roles-tasks.md) (Роли и задачи в командном процессе обработки и анализа данных).

Командам обработки и анализа данных, использующим TDSP, доступны следующие службы обработки и анализа данных:

- виртуальные машины для обработки и анализа данных (Windows и Linux CentOS);
- кластеры Spark в HDInsight;
- Хранилище данных SQL
- Озеро данных Azure
- кластеры Hive в HDInsight;
- хранилище файлов Azure;
- службы R в SQL Server 2016.

В этом документе мы кратко опишем эти ресурсы и приведем ссылки на руководства и пошаговые инструкции, опубликованные командами TDSP. Они помогут вам постепенно изучить все ресурсы и успешно применить их для создания интеллектуальных приложений. Дополнительные сведения об этих ресурсах можно найти на страницах соответствующих продуктов. 

## <a name="data-science-virtual-machine-dsvm"></a>Виртуальная машина для обработки и анализа данных

Виртуальная машина для обработки и анализа данных, которую корпорация Майкрософт предоставляет для ОС Windows и Linux, содержит набор популярных средств моделирования и разработки для систем обработки и анализа данных. Среди прочего, она оснащена такими средствами:

- Microsoft R Server Developer Edition 
- дистрибутив Anaconda Python;
- записные книжки Jupyter для Python и R; 
- Visual Studio Community Edition в комплекте со средствами Python и R для Windows (или Eclipse для Linux);
- Power BI Desktop для Windows;
- SQL Server 2016 Developer Edition для Windows (или Postgres для Linux).

Она также включает **средства машинного обучения и искусственного интеллекта**, например CNTK (набор средств с открытым кодом для углубленного обучения, созданный корпорацией Майкрософт), xgboost, mxnet и Vowpal Wabbit.

Сейчас виртуальная машина для обработки и анализа данных доступна для операционных систем **Windows** и **Linux CentOS**. Выберите нужный размер виртуальной машины для обработки и анализа данных (число ядер и размер памяти), исходя из потребностей проектов обработки и анализа данных, которые вы будете на ней выполнять. 

Дополнительные сведения о выпуске виртуальной машины для обработки и анализа данных для Windows см. в [этом описании](https://azure.microsoft.com/marketplace/partners/microsoft-ads/standard-data-science-vm/) на Azure Marketplace. Выпуск виртуальной машины для обработки и анализа данных для Linux [описан здесь](https://azure.microsoft.com/marketplace/partners/microsoft-ads/linux-data-science-vm/).

Чтобы научиться эффективно выполнять некоторые распространенные задачи обработки и анализа данных на виртуальной машинt для обработки и анализа данных, см. статью [10 задач, которые можно выполнить на виртуальной машине для обработки и анализа данных](../data-science-virtual-machine/vm-do-ten-things.md).


## <a name="azure-hdinsight-spark-clusters"></a>Кластеры Spark в Azure HDInsight

Apache Spark — это платформа параллельной обработки с открытым кодом, которая поддерживает обработку в памяти, чтобы повысить производительность приложений для анализа больших данных. Подсистема обработки Spark призвана ускорить разработку, повысить удобство использования и реализовать сложную аналитику. Возможности вычисления в памяти позволяют Spark эффективно применять итеративные алгоритмы в машинном обучении и графовых вычислениях. Подсистема Spark также совместима с хранилищем BLOB-объектов Azure (WASB), поэтому сможет легко обрабатывать существующие данные, хранящиеся в Azure.

При создании кластера Spark в HDInsight создание вычислительных ресурсов Azure следует выполнять после установки и настройки Spark. Создание кластера Spark в HDInsight займет около 10 минут. Разместите данные для обработки в хранилище BLOB-объектов Azure. Сведения об использовании хранилища BLOB-объектов Azure совместно с кластером см. в разделе [Использование службы хранилища Azure с кластерами Azure HDInsight](../../hdinsight/hdinsight-hadoop-use-blob-storage.md).

Команда TDSP корпорации Майкрософт опубликовала два полных пошаговых руководства (одно для Python, другое — для Scala) по использованию кластеров Spark в Azure HDInsight для создания решений по обработке и анализу данных. Дополнительные сведения о **кластерах Spark** в Azure HDInsight см. в статье [Общие сведения о Spark в HDInsight](../../hdinsight/spark/apache-spark-overview.md). Чтобы узнать, как с помощью **Python** создать решение по обработке и анализу данных в кластере Spark в Azure HDInsight, см. статью [Общие сведения об обработке и анализе данных с помощью платформы Spark в Azure HDInsight](spark-overview.md). Чтобы узнать, как с помощью **Scala** создать решение по обработке и анализу данных в кластере Spark в Azure HDInsight, см. статью [Обработка и анализ данных с использованием Scala и Spark в Azure](scala-walkthrough.md). 


##  <a name="azure-sql-data-warehouse"></a>Хранилище данных SQL Azure

Хранилище данных SQL Azure позволяет легко масштабировать вычислительные ресурсы за считаные секунды, не выделяя лишних ресурсов и не переплачивая за их использование. Также оно предоставляет уникальную возможность приостановить использование вычислительных ресурсов, обеспечивая более гибкое управление затратами на облачные решения. Возможность развертывать масштабируемые вычислительные ресурсы позволяет перенести все данные в хранилище данных SQL Azure. Затраты на хранение здесь минимальны, а вычисления можно выполнять только для тех сегментов наборов данных, которые нужны вам для анализа. 

Дополнительные сведения о хранилище данных SQL Azure см. на [веб-странице](https://azure.microsoft.com/services/sql-data-warehouse), посвященной этой службе. Чтобы научиться создавать комплексные решения для углубленной аналитики на базе хранилища данных SQL, см. статью [Командный процесс обработки и анализа данных на практике: использование хранилища данных SQL](sqldw-walkthrough.md).


## <a name="azure-data-lake"></a>Озеро данных Azure

Azure Data Lake — это репозиторий корпоративного уровня, позволяющий собрать в едином расположении данные любого типа, прежде чем применять к ним формальные требования или строгие схемы. Такая гибкость позволяет хранить в репозитории данные любого типа, любого размера и структуры, и принимать их с любой скоростью. Организации могут применить Hadoop или углубленную аналитику для поиска закономерностей в репозиториях типа Data Lake. Репозитории типа Data Lake могут также служить бюджетными репозиториями для подготовки данных перед их очисткой и переносом в хранилище данных.

Дополнительные сведения об Azure Data Lake см. в записи блога [Introducing Azure Data Lake](https://azure.microsoft.com/blog/introducing-azure-data-lake/) (Знакомство с Azure Data Lake). Чтобы узнать, как создать комплексное решение для обработки и анализа данных на базе Azure Data Lake, изучите [полное пошаговое руководство по масштабируемому анализу данных с помощью Azure Data Lake](data-lake-walkthrough.md).


## <a name="azure-hdinsight-hive-hadoop-clusters"></a>Кластеры Hive (Hadoop) в Azure HDInsight

Apache Hive — это система хранилища данных для Hadoop, которая позволяет обобщать и анализировать данные, а также обрабатывать запросы с использованием HiveQL (язык запросов, подобный SQL). Hive можно использовать для интерактивного исследования данных или создания многократно используемых заданий пакетного задания обработки.

Hive позволяет создавать структуру для преимущественно неструктурированных данных. Определив такую структуру, вы сможете использовать Hive для отправки запросов к данным в кластере Hadoop. Для этого не нужно изучать Java или MapReduce. HiveQL (язык запросов Hive) позволяет создавать запросы, используя операторы, подобные операторам T-SQL.

Hive позволяет включать пользовательские функции Python в запросы Hive для обработки и анализа данных в записях. Так вы сможете значительно расширить потенциал запросов Hive при анализе данных. В частности, специалисты по обработке и анализу данных смогут создавать масштабируемые функции на языках, с которыми они лучше всего знакомы: HiveQL, который похож на SQL, и Python. 

Дополнительные сведения о кластерах Hive в Azure HDInsight см. в статье [Использование Hive и HiveQL с Hadoop в HDInsight](../../hdinsight/hadoop/hdinsight-use-hive.md). Чтобы научиться создавать масштабируемые комплексные решения для обработки и анализа данных на базе кластеров Hive в Azure HDInsight, см. статью [Командный процесс обработки и анализа данных на практике: использование хранилища данных SQL](hive-walkthrough.md).


## <a name="azure-file-storage"></a>Хранилище файлов Azure 

Хранилище файлов Azure — это служба, которая предоставляет доступ к общим папкам в облаке с использованием стандартного протокола SMB. Поддерживаются версии SMB 2.1 и SMB 3.0. Хранилище файлов Azure позволяет быстро и без дорогостоящей перезаписи выполнить перенос приложений прежних версий, связанных с общими папками. Приложения, работающие на виртуальных машинах Azure, в облачных службах или на локальных клиентах, могут подключать общую папку в облаке так же, как настольное приложение подключает обычную общую папку SMB. Любое количество компонентов приложений может одновременно подключаться и получать доступ к ресурсам хранилища файлов.

Особенно полезной для проектов по обработке и анализу данных будет возможность создать хранилище файлов Azure для совместного использования всеми участниками команды проекта. Все специалисты будут обращаться к одной и той же копии данных, размещенной в хранилище файлов Azure. Также с помощью этого хранилища файлов они смогут совместно использовать наборы функций, созданные во время выполнения проекта. Если проект предусматривает взаимодействие с клиентом, клиент может создать хранилище файлов Azure в своей подписке Azure и разместить в нем данные и компоненты проекта, чтобы предоставить вам доступ к ним. Это позволит клиенту сохранить полный контроль над ресурсами данных, используемыми в проекте. Дополнительные сведения о хранилище файлов Azure см. в статьях [Разработка для службы файлов Azure с помощью .NET](https://azure.microsoft.com/documentation/articles/storage-dotnet-how-to-use-files) и [Использование файлов Azure в Linux](../../storage/files/storage-how-to-use-files-linux.md).


## <a name="sql-server-2016-r-services"></a>Службы R в SQL Server 2016

Службы R (в базе данных) предоставляют платформу для разработки и развертывания интеллектуальных приложений, которые позволяют получить новые ценные сведения. Вы можете использовать полнофункциональный мощный язык R и множество пакетов, созданных сообществом R, чтобы разрабатывать модели и формировать прогнозы на основе данных, хранящихся в SQL Server. Службы R (в базе данных) интегрируют язык R с SQL Server, что позволяет выполнять анализ там же, где расположены данные. Так вы сможете избежать лишних затрат и рисков для безопасности, связанных с перемещением данных.

Службы R (в базе данных) поддерживают язык R с открытым кодом, а также широкий набор средств и технологий SQL Server. Они обеспечивают высокую производительность, безопасность, надежность и управляемость. Для развертывания решений R вам доступны удобные и знакомые средства. Ваши рабочие приложения могут вызывать среду выполнения R, получать прогнозы и визуальные элементы с использованием Transact-SQL. Кроме того, вы можете использовать библиотеки ScaleR для увеличения масштаба и производительности решений R. Дополнительные сведения см. в статье [SQL Server Machine Learning Services](https://msdn.microsoft.com/library/mt604845.aspx) (Службы машинного обучения SQL Server).

Команда TDSP корпорации Майкрософт опубликовала два полных пошаговых руководства (одно для R-программистов, другое — для разработчиков SQL) по созданию решений для обработки и анализа данных в службах R для SQL Server 2016. Для **R-программистов** предлагается [полное пошаговое руководство по обработке и анализу данных](https://msdn.microsoft.com/library/mt612857.aspx). Для **разработчиков SQL** будет полезно руководство по [аналитике в базе данных R для разработчиков SQL](https://msdn.microsoft.com/library/mt683480.aspx).


## <a name="appendix"></a>Приложение. Средства для настройки проектов по обработке и анализу данных

### <a name="install-git-credential-manager-on-windows"></a>Установка диспетчера учетных данных Git в Windows

Если вы организуете процесс TDSP на **Windows**, вам потребуется **диспетчер учетных данных Git (GCM)** для обмена данными с репозиториями Git. Перед установкой GCM необходимо сначала установить **Chocolaty**. Чтобы установить Chocolaty и GCM, выполните следующие команды в Windows PowerShell **с правами администратора**.  

    iwr https://chocolatey.org/install.ps1 -UseBasicParsing | iex
    choco install git-credential-manager-for-windows -y
    

### <a name="install-git-on-linux-centos-machines"></a>Установка Git на компьютерах Linux (CentOS)

Выполните следующую команду в оболочке bash, чтобы установить Git на компьютере Linux (CentOS):

    sudo yum install git


### <a name="generate-public-ssh-key-on-linux-centos-machines"></a>Создание открытого ключа SSH на компьютерах Linux (CentOS)

Если вы используете для выполнения команд Git компьютер Linux (CentOS), на нем необходимо установить открытый ключ SSH для связи с сервером VSTS. Для этого сначала нужно создать этот открытый ключ SSH, а затем добавить его в список открытых ключей SSH на странице настроек безопасности для VSTS. 

- Чтобы создать ключ SSH, выполните следующие две команды. 

        ssh-keygen
        cat .ssh/id_rsa.pub

![](./media/platforms-and-tools/resources-1-generate_ssh.png)

- Скопируйте полный текст ключа SSH, включая *ssh-rsa*. 
- Войдите на сервер VSTS. 
- В правом верхнем углу страницы щелкните **<свое имя\>** и выберите пункт **Безопасность**. 
    
    ![](./media/platforms-and-tools/resources-2-user-setting.png)

- Щелкните **Открытые ключи SSH** и нажмите кнопку **+Добавить**. 

    ![](./media/platforms-and-tools/resources-3-add-ssh.png)

- Вставьте в текстовое поле скопированный ключ SSH и сохраните его.


## <a name="next-steps"></a>Дальнейшие действия

Также предоставляются полные пошаговые руководства, которые демонстрируют все этапы процесса для **конкретных сценариев** . Эти этапы с иллюстрациями и краткими описаниями перечислены в [примерах пошаговых руководств](walkthroughs.md). В них показано, как объединить облачные и локальные средства и службы в единый рабочий процесс или конвейер, чтобы создать интеллектуальное приложение. 

Примеры выполнения шагов командного процесса обработки и анализа данных, в которых задействуется студия машинного обучения Azure, см. в [схеме обучения с использованием службы "Машинное обучение Azure"](http://aka.ms/datascienceprocess).