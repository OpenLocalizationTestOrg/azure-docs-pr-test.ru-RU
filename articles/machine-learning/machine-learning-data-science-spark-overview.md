---
title: "Общие сведения об обработке и анализе данных с помощью платформы Spark в Azure HDInsight | Документация Майкрософт"
description: "Набор средств Spark MLlib предоставляет широкие возможности моделирования машинного обучения в распределенной среде HDInsight."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: a4e1de99-a554-4240-9647-2c6d669593c8
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/15/2017
ms.author: deguhath;bradsev;gokuma
ms.openlocfilehash: 379b32f4e533f48f1593a97e73737a0c5bfb9135
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/11/2017
---
# <a name="overview-of-data-science-using-spark-on-azure-hdinsight"></a><span data-ttu-id="7afc6-103">Общие сведения об обработке и анализе данных с помощью платформы Spark в Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="7afc6-103">Overview of data science using Spark on Azure HDInsight</span></span>
[!INCLUDE [machine-learning-spark-modeling](../../includes/machine-learning-spark-modeling.md)]

<span data-ttu-id="7afc6-104">В этой группе статей рассказывается, как использовать HDInsight Spark для выполнения общих задач обработки и анализа данных, таких как прием данных, проектирование функций, моделирование и оценка моделей.</span><span class="sxs-lookup"><span data-stu-id="7afc6-104">This suite of topics shows how to use HDInsight Spark to complete common data science tasks such as data ingestion, feature engineering, modeling, and model evaluation.</span></span> <span data-ttu-id="7afc6-105">В качестве данных используется пример с числом поездок и тарифами нью-йоркского такси за 2013 год.</span><span class="sxs-lookup"><span data-stu-id="7afc6-105">The data used is a sample of the 2013 NYC taxi trip and fare dataset.</span></span> <span data-ttu-id="7afc6-106">В моделях используются логистическая и линейная регрессия, случайные леса и градиентный бустинг деревьев.</span><span class="sxs-lookup"><span data-stu-id="7afc6-106">The models built include logistic and linear regression, random forests, and gradient boosted trees.</span></span> <span data-ttu-id="7afc6-107">Кроме того, в этих статьях рассказывается, как хранить эти модели в хранилище BLOB-объектов Azure (WASB), а также оценивать и анализировать прогнозируемую производительность.</span><span class="sxs-lookup"><span data-stu-id="7afc6-107">The topics also show how to store these models in Azure blob storage (WASB) and how to score and evaluate their predictive performance.</span></span> <span data-ttu-id="7afc6-108">В более расширенных разделах рассматриваются способы обучения моделей с помощью перекрестной проверки и перебора гиперпараметров.</span><span class="sxs-lookup"><span data-stu-id="7afc6-108">More advanced topics cover how models can be trained using cross-validation and hyper-parameter sweeping.</span></span> <span data-ttu-id="7afc6-109">В этой обзорной статье также упоминаются разделы, посвященные настройке кластера Spark, который вам потребуется для выполнения действий, описанных в предоставленных руководствах.</span><span class="sxs-lookup"><span data-stu-id="7afc6-109">This overview topic also references the topics that describe how to set up the Spark cluster that you need to complete the steps in the walkthroughs provided.</span></span> 

## <a name="spark-and-mllib"></a><span data-ttu-id="7afc6-110">Spark и MLlib</span><span class="sxs-lookup"><span data-stu-id="7afc6-110">Spark and MLlib</span></span>
<span data-ttu-id="7afc6-111">[Spark](http://spark.apache.org/) — это платформа параллельной обработки с открытым исходным кодом, которая поддерживает обработку в памяти, повышая производительность приложений для анализа больших данных.</span><span class="sxs-lookup"><span data-stu-id="7afc6-111">[Spark](http://spark.apache.org/) is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.</span></span> <span data-ttu-id="7afc6-112">Подсистема обработки Spark призвана ускорить разработку, повысить удобство использования и реализовать сложную аналитику.</span><span class="sxs-lookup"><span data-stu-id="7afc6-112">The Spark processing engine is built for speed, ease of use, and sophisticated analytics.</span></span> <span data-ttu-id="7afc6-113">Возможности распределенного вычисления в памяти Spark отлично подходят для итеративных алгоритмов, используемых в машинном обучении и графовых вычислениях.</span><span class="sxs-lookup"><span data-stu-id="7afc6-113">Spark's in-memory distributed computation capabilities make it a good choice for the iterative algorithms used in machine learning and graph computations.</span></span> <span data-ttu-id="7afc6-114">[MLlib](http://spark.apache.org/mllib/) — это масштабируемая библиотека машинного обучения Spark, которая предоставляет возможности алгоритмического моделирования для этой распределенной среды.</span><span class="sxs-lookup"><span data-stu-id="7afc6-114">[MLlib](http://spark.apache.org/mllib/) is Spark's scalable machine learning library that brings the algorithmic modeling capabilities to this distributed environment.</span></span> 

## <a name="hdinsight-spark"></a><span data-ttu-id="7afc6-115">HDInsight Spark</span><span class="sxs-lookup"><span data-stu-id="7afc6-115">HDInsight Spark</span></span>
<span data-ttu-id="7afc6-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) представляет собой версию платформы Spark с открытым исходным кодом, размещенную в Azure.</span><span class="sxs-lookup"><span data-stu-id="7afc6-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) is the Azure hosted offering of open-source Spark.</span></span> <span data-ttu-id="7afc6-117">Она также поддерживает **записные книжки Jupyter PySpark** в кластере Spark, которые могут выполнять интерактивные запросы Spark SQL для преобразования, фильтрации и визуализации данных, хранящихся в больших двоичных объектах Azure (WASB).</span><span class="sxs-lookup"><span data-stu-id="7afc6-117">It also includes support for **Jupyter PySpark notebooks** on the Spark cluster that can run Spark SQL interactive queries for transforming, filtering, and visualizing data stored in Azure Blobs (WASB).</span></span> <span data-ttu-id="7afc6-118">PySpark — это API Python для Spark.</span><span class="sxs-lookup"><span data-stu-id="7afc6-118">PySpark is the Python API for Spark.</span></span> <span data-ttu-id="7afc6-119">Здесь фрагменты кода, которые предоставляют решения и формируют соответствующие графики с целью визуализации данных, выполняются в записных книжках Jupyter, установленных в кластерах Spark.</span><span class="sxs-lookup"><span data-stu-id="7afc6-119">The code snippets that provide the solutions and show the relevant plots to visualize the data here run in Jupyter notebooks installed on the Spark clusters.</span></span> <span data-ttu-id="7afc6-120">Этапы моделирования, описанные в этих разделах, содержат код, который демонстрирует способ обучения, анализа, сохранения и использования каждого типа модели.</span><span class="sxs-lookup"><span data-stu-id="7afc6-120">The modeling steps in these topics contain code that shows how to train, evaluate, save, and consume each type of model.</span></span> 

## <a name="setup-spark-clusters-and-jupyter-notebooks"></a><span data-ttu-id="7afc6-121">Настройка кластеров Spark и записных книжек Jupyter</span><span class="sxs-lookup"><span data-stu-id="7afc6-121">Setup: Spark clusters and Jupyter notebooks</span></span>
<span data-ttu-id="7afc6-122">Действия по настройке и код, указанные в этом пошаговом руководстве, применимы к использованию для HDInsight Spark 1.6.</span><span class="sxs-lookup"><span data-stu-id="7afc6-122">Setup steps and code are provided in this walkthrough for using an HDInsight Spark 1.6.</span></span> <span data-ttu-id="7afc6-123">Но записные книжки Jupyter можно использовать для кластеров HDInsight Spark 1.6 и Spark 2.0.</span><span class="sxs-lookup"><span data-stu-id="7afc6-123">But Jupyter notebooks are provided for both HDInsight Spark 1.6 and Spark 2.0 clusters.</span></span> <span data-ttu-id="7afc6-124">Описание записных книжек и ссылки на них вы можете найти в файле [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md) для репозитория GitHub с записными книжками.</span><span class="sxs-lookup"><span data-stu-id="7afc6-124">A description of the notebooks and links to them are provided in the [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md) for the GitHub repository containing them.</span></span> <span data-ttu-id="7afc6-125">Более того, код здесь и в связанных записных книжках является универсальным и должен работать в любом кластере Spark.</span><span class="sxs-lookup"><span data-stu-id="7afc6-125">Moreover, the code here and in the linked notebooks is generic and should work on any Spark cluster.</span></span> <span data-ttu-id="7afc6-126">Действия по настройке кластера и управлению им могут немного отличаться от приведенных здесь, если вы не используете HDInsight Spark.</span><span class="sxs-lookup"><span data-stu-id="7afc6-126">If you are not using HDInsight Spark, the cluster setup and management steps may be slightly different from what is shown here.</span></span> <span data-ttu-id="7afc6-127">Для удобства мы приводим ссылки на записные книжки Jupyter для Spark 1.6 (выполняемые в ядре PySpark на сервере записной книжки Jupyter) и Spark 2.0 (выполняемые в ядре PySpark3 на сервере записной книжки Jupyter).</span><span class="sxs-lookup"><span data-stu-id="7afc6-127">For convenience, here are the links to the Jupyter notebooks for Spark 1.6 (to be run in the pySpark kernel of the Jupyter Notebook server) and  Spark 2.0 (to be run in the pySpark3 kernel of the Jupyter Notebook server):</span></span>

### <a name="spark-16-notebooks"></a><span data-ttu-id="7afc6-128">Записные книжки для Spark 1.6</span><span class="sxs-lookup"><span data-stu-id="7afc6-128">Spark 1.6 notebooks</span></span>
<span data-ttu-id="7afc6-129">Эти записные книжки должны выполняться в ядре PySpark на сервере записной книжки Jupyter:</span><span class="sxs-lookup"><span data-stu-id="7afc6-129">These notebooks are to be run in the pySpark kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="7afc6-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb). Содержит сведения о том, как выполнять просмотр данных, моделирование и оценку с использованием нескольких различных алгоритмов.</span><span class="sxs-lookup"><span data-stu-id="7afc6-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb): Provides information on how to perform data exploration, modeling, and scoring with several different algorithms.</span></span>
- <span data-ttu-id="7afc6-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb). Содержит разделы в записной книжке №1 и варианты моделей с использованием настройки гиперпараметров и перекрестной проверки.</span><span class="sxs-lookup"><span data-stu-id="7afc6-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): Includes topics in notebook #1, and model development using hyperparameter tuning and cross-validation.</span></span>
- <span data-ttu-id="7afc6-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb). Показывает, как применять сохраненную модель в кластерах HDInsight с помощью Python.</span><span class="sxs-lookup"><span data-stu-id="7afc6-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb): Shows how to operationalize a saved model using Python on HDInsight clusters.</span></span>

### <a name="spark-20-notebooks"></a><span data-ttu-id="7afc6-133">Записные книжки для Spark 2.0</span><span class="sxs-lookup"><span data-stu-id="7afc6-133">Spark 2.0 notebooks</span></span>
<span data-ttu-id="7afc6-134">Эти записные книжки должны выполняться в ядре PySpark3 на сервере записной книжки Jupyter:</span><span class="sxs-lookup"><span data-stu-id="7afc6-134">These notebooks are to be run in the pySpark3 kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="7afc6-135">[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb). Содержит сведения о том, как выполнять просмотр данных, моделирование и оценку в кластерах Spark 2.0 на примере набора данных о расстояниях и ценах на такси в Нью-Йорке, описанного [здесь](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span><span class="sxs-lookup"><span data-stu-id="7afc6-135">[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): This file provides information on how to perform data exploration, modeling, and scoring in Spark 2.0 clusters using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span> <span data-ttu-id="7afc6-136">Эту записную книжку можно использовать в качестве отправной точки для быстрого изучения кода, предоставленного для Spark 2.0.</span><span class="sxs-lookup"><span data-stu-id="7afc6-136">This notebook may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> <span data-ttu-id="7afc6-137">В следующей записной книжке списка приведен более детальный анализ данных о поездках в такси по Нью-Йорку</span><span class="sxs-lookup"><span data-stu-id="7afc6-137">For a more detailed notebook analyzes the NYC Taxi data, see the next notebook in this list.</span></span> <span data-ttu-id="7afc6-138">(см. примечания после списка сравнения этих записных книжек).</span><span class="sxs-lookup"><span data-stu-id="7afc6-138">See the notes following this list that compare these notebooks.</span></span> 
- <span data-ttu-id="7afc6-139">[Spark2.0-pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb). Демонстрирует, как можно выполнять структурирование данных (Spark SQL и операции с кадрами данных), просмотр данных, моделирование и оценку на примере набора данных о расстояниях и ценах на такси в Нью-Йорке, который описан [здесь](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span><span class="sxs-lookup"><span data-stu-id="7afc6-139">[Spark2.0-pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span>
- <span data-ttu-id="7afc6-140">[Spark2.0-pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb). Демонстрирует, как можно выполнять структурирование данных (Spark SQL и операции с кадрами данных), просмотр данных, моделирование и оценку на примере известного набора данных о расписании вылетов авиакомпании за 2011 и 2012 гг.</span><span class="sxs-lookup"><span data-stu-id="7afc6-140">[Spark2.0-pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the well-known Airline On-time departure dataset from 2011 and 2012.</span></span> <span data-ttu-id="7afc6-141">До моделирования мы дополнили набор данных об авиарейсах набором данных о погоде в аэропортах (скорость ветра, температура, высота над уровнем моря и т. д.), чтобы включить в модель эту информацию о погоде.</span><span class="sxs-lookup"><span data-stu-id="7afc6-141">We integrated the airline dataset with the airport weather data (e.g. windspeed, temperature, altitude etc.) prior to modeling, so these weather features can be included in the model.</span></span>

<!-- -->

> [!NOTE]
> <span data-ttu-id="7afc6-142">В записные книжки для Spark 2.0 был добавлен набор данных об авиарейсах, который лучше иллюстрирует использование алгоритмов классификации.</span><span class="sxs-lookup"><span data-stu-id="7afc6-142">The airline dataset was added to the Spark 2.0 notebooks to better illustrate the use of classification algorithms.</span></span> <span data-ttu-id="7afc6-143">Следующие ссылки помогут получить информацию о наборах данных о расписании вылетов авиакомпании и о погоде.</span><span class="sxs-lookup"><span data-stu-id="7afc6-143">See the following links for information about airline on-time departure dataset and weather dataset:</span></span>

>- <span data-ttu-id="7afc6-144">Данные о расписании вылетов авиакомпании: [http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span><span class="sxs-lookup"><span data-stu-id="7afc6-144">Airline on-time departure data: [http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span></span>

>- <span data-ttu-id="7afc6-145">Данные о погоде в аэропортах: [https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span><span class="sxs-lookup"><span data-stu-id="7afc6-145">Airport weather data: [https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span></span> 
> 
> 

<!-- -->

<!-- -->

> [!NOTE]
<span data-ttu-id="7afc6-146">Выполнение записных книжек Spark 2.0 с наборами данных о поездках в такси по Нью-Йорку и задержке рейсов может занять примерно 10 минут или больше (в зависимости от размера кластера HDI).</span><span class="sxs-lookup"><span data-stu-id="7afc6-146">The Spark 2.0 notebooks on the NYC taxi and airline flight delay data-sets can take 10 mins or more to run (depending on the size of your HDI cluster).</span></span> <span data-ttu-id="7afc6-147">В первой записной книжке из приведенного выше списка показаны многие аспекты исследования и визуализации данных, а также обучения модели машинного обучения. За счет уменьшения набора данных о поездках в такси по Нью-Йорку, в котором данные о поездках и тарифах на них объединены в один файл [Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb), выполнение и завершение этой записной книжки занимает значительно меньше времени (2–3 минуты), и поэтому ее хорошо использовать в качестве отправной точки для быстрого изучения кода, предоставленного для Spark 2.0.</span><span class="sxs-lookup"><span data-stu-id="7afc6-147">The first notebook in the above list shows many aspects of the data exploration, visualization and ML model training in a notebook that takes less time to run with down-sampled NYC data set, in which the taxi and fare files have been pre-joined: [Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb) This notebook takes a much shorter time to finish (2-3 mins) and may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> 

<!-- -->

<span data-ttu-id="7afc6-148">Руководство по применению модели Spark 2.0 и использованию моделей для оценки вы найдете в [этом документе для Spark 1.6](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb), который содержит пример с описанием выполняемых шагов.</span><span class="sxs-lookup"><span data-stu-id="7afc6-148">For guidance on the operationalization of a Spark 2.0 model and model consumption for scoring, see the [Spark 1.6 document on consumption](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb) for an example outlining the steps required.</span></span> <span data-ttu-id="7afc6-149">Чтобы использовать этот пример в Spark 2.0, замените файл кода Python [этим файлом](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py).</span><span class="sxs-lookup"><span data-stu-id="7afc6-149">To use this on Spark 2.0, replace the Python code file with [this file](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py).</span></span>

### <a name="prerequisites"></a><span data-ttu-id="7afc6-150">Предварительные требования</span><span class="sxs-lookup"><span data-stu-id="7afc6-150">Prerequisites</span></span>
<span data-ttu-id="7afc6-151">Описанные ниже процедуры относятся к Spark 1.6.</span><span class="sxs-lookup"><span data-stu-id="7afc6-151">The following procedures are related to Spark 1.6.</span></span> <span data-ttu-id="7afc6-152">Для Spark 2.0 используйте записные книжки, описанные ранее.</span><span class="sxs-lookup"><span data-stu-id="7afc6-152">For  the Spark 2.0 version, use the notebooks described and linked to previously.</span></span> 

<span data-ttu-id="7afc6-153">1. У вас должна быть подписка Azure.</span><span class="sxs-lookup"><span data-stu-id="7afc6-153">1.You must have an Azure subscription.</span></span> <span data-ttu-id="7afc6-154">Если у вас ее нет, см. статью [о получении бесплатной пробной версии Azure](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span><span class="sxs-lookup"><span data-stu-id="7afc6-154">If you do not already have one, see [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span></span>

<span data-ttu-id="7afc6-155">2. Для выполнения инструкций этого руководства требуется кластер Spark 1.6.</span><span class="sxs-lookup"><span data-stu-id="7afc6-155">2.You need a Spark 1.6 cluster to complete this walkthrough.</span></span> <span data-ttu-id="7afc6-156">Создайте его, выполнив инструкции в статье [Начало работы: создание кластера Apache Spark в Azure HDInsight](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="7afc6-156">To create one, see the instructions provided in [Get started: create Apache Spark on Azure HDInsight](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md).</span></span> <span data-ttu-id="7afc6-157">Тип и версию кластера можно указать с помощью меню **Выбор типа кластера** .</span><span class="sxs-lookup"><span data-stu-id="7afc6-157">The cluster type and version is specified from the **Select Cluster Type** menu.</span></span> 

![Настройка кластера](./media/machine-learning-data-science-spark-overview/spark-cluster-on-portal.png)

<!-- -->

> [!NOTE]
> <span data-ttu-id="7afc6-159">О том, как использовать Scala вместо Python для выполнения задач по полной обработке и анализу данных, вы узнаете из статьи [Обработка и анализ данных с использованием Scala и Spark в Azure](machine-learning-data-science-process-scala-walkthrough.md).</span><span class="sxs-lookup"><span data-stu-id="7afc6-159">For a topic that shows how to use Scala rather than Python to complete tasks for an end-to-end data science process, see the [Data Science using Scala with Spark on Azure](machine-learning-data-science-process-scala-walkthrough.md).</span></span>
> 
> 

<!-- -->

> [!INCLUDE [delete-cluster-warning](../../includes/hdinsight-delete-cluster-warning.md)]
> 
> 

## <a name="the-nyc-2013-taxi-data"></a><span data-ttu-id="7afc6-160">Данные о такси в Нью-Йорке, 2013 г.</span><span class="sxs-lookup"><span data-stu-id="7afc6-160">The NYC 2013 Taxi data</span></span>
<span data-ttu-id="7afc6-161">Сведения о поездках на такси в Нью-Йорке заключены в сжатые файлы данных с разделителями-запятыми (CSV) объемом около 20 ГБ (примерно 48 ГБ без сжатия), которые содержат данные о 173 миллионах поездок и о стоимости каждой из них.</span><span class="sxs-lookup"><span data-stu-id="7afc6-161">The NYC Taxi Trip data is about 20 GB of compressed comma-separated values (CSV) files (~48 GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip.</span></span> <span data-ttu-id="7afc6-162">Каждая запись о поездке содержит время и место посадки и высадки, анонимизированный номер лицензии водителя и номер медальона (уникальный идентификатор такси).</span><span class="sxs-lookup"><span data-stu-id="7afc6-162">Each trip record includes the pick up and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi’s unique id) number.</span></span> <span data-ttu-id="7afc6-163">Данные включают в себя все поездки за 2013 год и предоставляются в виде следующих двух наборов данных за каждый месяц:</span><span class="sxs-lookup"><span data-stu-id="7afc6-163">The data covers all trips in the year 2013 and is provided in the following two datasets for each month:</span></span>

1. <span data-ttu-id="7afc6-164">В CSV-файлах "trip_data" содержатся сведения о поездках, например количество пассажиров, места посадки и высадки, продолжительность и дальность поездки.</span><span class="sxs-lookup"><span data-stu-id="7afc6-164">The 'trip_data' CSV files contain trip details, such as number of passengers, pick up and dropoff points, trip duration, and trip length.</span></span> <span data-ttu-id="7afc6-165">Вот несколько примеров записей:</span><span class="sxs-lookup"><span data-stu-id="7afc6-165">Here are a few sample records:</span></span>
   
        medallion,hack_license,vendor_id,rate_code,store_and_fwd_flag,pickup_datetime,dropoff_datetime,passenger_count,trip_time_in_secs,trip_distance,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,1,N,2013-01-01 15:11:48,2013-01-01 15:18:10,4,382,1.00,-73.978165,40.757977,-73.989838,40.751171
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-06 00:18:35,2013-01-06 00:22:54,1,259,1.50,-74.006683,40.731781,-73.994499,40.75066
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-05 18:49:41,2013-01-05 18:54:23,1,282,1.10,-74.004707,40.73777,-74.009834,40.726002
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:54:15,2013-01-07 23:58:20,2,244,.70,-73.974602,40.759945,-73.984734,40.759388
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:25:03,2013-01-07 23:34:24,1,560,2.10,-73.97625,40.748528,-74.002586,40.747868
2. <span data-ttu-id="7afc6-166">В CSV-файлах trip_fare содержатся сведения о плате за каждую поездку, например тип оплаты, стоимость поездки, добавочная стоимость и налоги, чаевые и специальные тарифы, а также общая сумма оплаты.</span><span class="sxs-lookup"><span data-stu-id="7afc6-166">The 'trip_fare' CSV files contain details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid.</span></span> <span data-ttu-id="7afc6-167">Вот несколько примеров записей:</span><span class="sxs-lookup"><span data-stu-id="7afc6-167">Here are a few sample records:</span></span>
   
        medallion, hack_license, vendor_id, pickup_datetime, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,2013-01-01 15:11:48,CSH,6.5,0,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-06 00:18:35,CSH,6,0.5,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-05 18:49:41,CSH,5.5,1,0.5,0,0,7
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:54:15,CSH,5,0.5,0.5,0,0,6
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:25:03,CSH,9.5,0.5,0.5,0,0,10.5

<span data-ttu-id="7afc6-168">Мы взяли 0,1 % этой информации и объединили CSV-файлы trip\_data и trip\_fare в единый набор данных, который в этом руководстве будем использовать в качестве входного набора данных.</span><span class="sxs-lookup"><span data-stu-id="7afc6-168">We have taken a 0.1% sample of these files and joined the trip\_data and trip\_fare CVS files into a single dataset to use as the input dataset for this walkthrough.</span></span> <span data-ttu-id="7afc6-169">Уникальный ключ для объединения trip\_data и trip\_fare состоит из полей medallion, hack\_licence и pickup\_datetime.</span><span class="sxs-lookup"><span data-stu-id="7afc6-169">The unique key to join trip\_data and trip\_fare is composed of the fields: medallion, hack\_licence and pickup\_datetime.</span></span> <span data-ttu-id="7afc6-170">Каждая запись набора данных содержит следующие атрибуты, представляющие поездку на такси в Нью-Йорке:</span><span class="sxs-lookup"><span data-stu-id="7afc6-170">Each record of the dataset contains the following attributes representing a NYC Taxi trip:</span></span>

| <span data-ttu-id="7afc6-171">Поле</span><span class="sxs-lookup"><span data-stu-id="7afc6-171">Field</span></span> | <span data-ttu-id="7afc6-172">Краткое описание</span><span class="sxs-lookup"><span data-stu-id="7afc6-172">Brief Description</span></span> |
| --- | --- |
| <span data-ttu-id="7afc6-173">medallion</span><span class="sxs-lookup"><span data-stu-id="7afc6-173">medallion</span></span> |<span data-ttu-id="7afc6-174">Обезличенный медальон такси (уникальный идентификатор такси)</span><span class="sxs-lookup"><span data-stu-id="7afc6-174">Anonymized taxi medallion (unique taxi id)</span></span> |
| <span data-ttu-id="7afc6-175">hack_license</span><span class="sxs-lookup"><span data-stu-id="7afc6-175">hack_license</span></span> |<span data-ttu-id="7afc6-176">Обезличенный номер лицензии такси</span><span class="sxs-lookup"><span data-stu-id="7afc6-176">Anonymized Hackney Carriage License number</span></span> |
| <span data-ttu-id="7afc6-177">vendor_id</span><span class="sxs-lookup"><span data-stu-id="7afc6-177">vendor_id</span></span> |<span data-ttu-id="7afc6-178">Идентификатор поставщика услуг такси</span><span class="sxs-lookup"><span data-stu-id="7afc6-178">Taxi vendor id</span></span> |
| <span data-ttu-id="7afc6-179">rate_code</span><span class="sxs-lookup"><span data-stu-id="7afc6-179">rate_code</span></span> |<span data-ttu-id="7afc6-180">Тариф на такси в Нью-Йорке</span><span class="sxs-lookup"><span data-stu-id="7afc6-180">NYC taxi rate of fare</span></span> |
| <span data-ttu-id="7afc6-181">store_and_fwd_flag</span><span class="sxs-lookup"><span data-stu-id="7afc6-181">store_and_fwd_flag</span></span> |<span data-ttu-id="7afc6-182">Отметка записи и дальнейшей передачи</span><span class="sxs-lookup"><span data-stu-id="7afc6-182">Store and forward flag</span></span> |
| <span data-ttu-id="7afc6-183">pickup_datetime</span><span class="sxs-lookup"><span data-stu-id="7afc6-183">pickup_datetime</span></span> |<span data-ttu-id="7afc6-184">Дата и время посадки пассажира</span><span class="sxs-lookup"><span data-stu-id="7afc6-184">Pick up date & time</span></span> |
| <span data-ttu-id="7afc6-185">dropoff_datetime</span><span class="sxs-lookup"><span data-stu-id="7afc6-185">dropoff_datetime</span></span> |<span data-ttu-id="7afc6-186">Дата и время высадки пассажира</span><span class="sxs-lookup"><span data-stu-id="7afc6-186">Dropoff date & time</span></span> |
| <span data-ttu-id="7afc6-187">pickup_hour</span><span class="sxs-lookup"><span data-stu-id="7afc6-187">pickup_hour</span></span> |<span data-ttu-id="7afc6-188">Час посадки пассажира</span><span class="sxs-lookup"><span data-stu-id="7afc6-188">Pick up hour</span></span> |
| <span data-ttu-id="7afc6-189">pickup_week</span><span class="sxs-lookup"><span data-stu-id="7afc6-189">pickup_week</span></span> |<span data-ttu-id="7afc6-190">Номер недели посадки пассажира</span><span class="sxs-lookup"><span data-stu-id="7afc6-190">Pick up week of the year</span></span> |
| <span data-ttu-id="7afc6-191">weekday</span><span class="sxs-lookup"><span data-stu-id="7afc6-191">weekday</span></span> |<span data-ttu-id="7afc6-192">День недели (диапазон 1–7)</span><span class="sxs-lookup"><span data-stu-id="7afc6-192">Weekday (range 1-7)</span></span> |
| <span data-ttu-id="7afc6-193">passenger_count</span><span class="sxs-lookup"><span data-stu-id="7afc6-193">passenger_count</span></span> |<span data-ttu-id="7afc6-194">Количество пассажиров во время поездки на такси</span><span class="sxs-lookup"><span data-stu-id="7afc6-194">Number of passengers in a taxi trip</span></span> |
| <span data-ttu-id="7afc6-195">trip_time_in_secs</span><span class="sxs-lookup"><span data-stu-id="7afc6-195">trip_time_in_secs</span></span> |<span data-ttu-id="7afc6-196">Длительность поездки в секундах</span><span class="sxs-lookup"><span data-stu-id="7afc6-196">Trip time in seconds</span></span> |
| <span data-ttu-id="7afc6-197">trip_distance</span><span class="sxs-lookup"><span data-stu-id="7afc6-197">trip_distance</span></span> |<span data-ttu-id="7afc6-198">Расстояние поездки в милях</span><span class="sxs-lookup"><span data-stu-id="7afc6-198">Trip distance traveled in miles</span></span> |
| <span data-ttu-id="7afc6-199">pickup_longitude</span><span class="sxs-lookup"><span data-stu-id="7afc6-199">pickup_longitude</span></span> |<span data-ttu-id="7afc6-200">Долгота места посадки пассажира</span><span class="sxs-lookup"><span data-stu-id="7afc6-200">Pick up longitude</span></span> |
| <span data-ttu-id="7afc6-201">pickup_latitude</span><span class="sxs-lookup"><span data-stu-id="7afc6-201">pickup_latitude</span></span> |<span data-ttu-id="7afc6-202">Широта места посадки пассажира</span><span class="sxs-lookup"><span data-stu-id="7afc6-202">Pick up latitude</span></span> |
| <span data-ttu-id="7afc6-203">dropoff_longitude</span><span class="sxs-lookup"><span data-stu-id="7afc6-203">dropoff_longitude</span></span> |<span data-ttu-id="7afc6-204">Долгота высадки пассажира</span><span class="sxs-lookup"><span data-stu-id="7afc6-204">Dropoff longitude</span></span> |
| <span data-ttu-id="7afc6-205">dropoff_latitude</span><span class="sxs-lookup"><span data-stu-id="7afc6-205">dropoff_latitude</span></span> |<span data-ttu-id="7afc6-206">Широта высадки пассажира</span><span class="sxs-lookup"><span data-stu-id="7afc6-206">Dropoff latitude</span></span> |
| <span data-ttu-id="7afc6-207">direct_distance</span><span class="sxs-lookup"><span data-stu-id="7afc6-207">direct_distance</span></span> |<span data-ttu-id="7afc6-208">Расстояние по прямой между местами посадки и высадки</span><span class="sxs-lookup"><span data-stu-id="7afc6-208">Direct distance between pick up and dropoff locations</span></span> |
| <span data-ttu-id="7afc6-209">payment_type</span><span class="sxs-lookup"><span data-stu-id="7afc6-209">payment_type</span></span> |<span data-ttu-id="7afc6-210">Тип оплаты (наличные, кредитная карта и т. д.)</span><span class="sxs-lookup"><span data-stu-id="7afc6-210">Payment type (cas, credit-card etc.)</span></span> |
| <span data-ttu-id="7afc6-211">fare_amount</span><span class="sxs-lookup"><span data-stu-id="7afc6-211">fare_amount</span></span> |<span data-ttu-id="7afc6-212">Сумма к оплате</span><span class="sxs-lookup"><span data-stu-id="7afc6-212">Fare amount in</span></span> |
| <span data-ttu-id="7afc6-213">surcharge</span><span class="sxs-lookup"><span data-stu-id="7afc6-213">surcharge</span></span> |<span data-ttu-id="7afc6-214">Доплата</span><span class="sxs-lookup"><span data-stu-id="7afc6-214">Surcharge</span></span> |
| <span data-ttu-id="7afc6-215">mta_tax</span><span class="sxs-lookup"><span data-stu-id="7afc6-215">mta_tax</span></span> |<span data-ttu-id="7afc6-216">Налог MTA</span><span class="sxs-lookup"><span data-stu-id="7afc6-216">Mta tax</span></span> |
| <span data-ttu-id="7afc6-217">tip_amount</span><span class="sxs-lookup"><span data-stu-id="7afc6-217">tip_amount</span></span> |<span data-ttu-id="7afc6-218">Сумма чаевых</span><span class="sxs-lookup"><span data-stu-id="7afc6-218">Tip amount</span></span> |
| <span data-ttu-id="7afc6-219">tolls_amount</span><span class="sxs-lookup"><span data-stu-id="7afc6-219">tolls_amount</span></span> |<span data-ttu-id="7afc6-220">Дорожные пошлины</span><span class="sxs-lookup"><span data-stu-id="7afc6-220">Tolls amount</span></span> |
| <span data-ttu-id="7afc6-221">total_amount</span><span class="sxs-lookup"><span data-stu-id="7afc6-221">total_amount</span></span> |<span data-ttu-id="7afc6-222">Общая сумма</span><span class="sxs-lookup"><span data-stu-id="7afc6-222">Total amount</span></span> |
| <span data-ttu-id="7afc6-223">tipped</span><span class="sxs-lookup"><span data-stu-id="7afc6-223">tipped</span></span> |<span data-ttu-id="7afc6-224">Чаевые (0 — нет, 1 — да)</span><span class="sxs-lookup"><span data-stu-id="7afc6-224">Tipped (0/1 for no or yes)</span></span> |
| <span data-ttu-id="7afc6-225">tip_class</span><span class="sxs-lookup"><span data-stu-id="7afc6-225">tip_class</span></span> |<span data-ttu-id="7afc6-226">Класс чаевых (0: 0 долларов, 1: 0–5 долларов, 2: 6–10 долларов, 3: 11–20 долларов, 4: >20 долларов)</span><span class="sxs-lookup"><span data-stu-id="7afc6-226">Tip class (0: $0, 1: $0-5, 2: $6-10, 3: $11-20, 4: > $20)</span></span> |

## <a name="execute-code-from-a-jupyter-notebook-on-the-spark-cluster"></a><span data-ttu-id="7afc6-227">Выполнение кода из записной книжки Jupyter в кластере Spark</span><span class="sxs-lookup"><span data-stu-id="7afc6-227">Execute code from a Jupyter notebook on the Spark cluster</span></span>
<span data-ttu-id="7afc6-228">Записную книжку Jupyter можно запустить с портала Azure.</span><span class="sxs-lookup"><span data-stu-id="7afc6-228">You can launch the Jupyter Notebook from the Azure portal.</span></span> <span data-ttu-id="7afc6-229">Найдите кластер Spark на панели мониторинга и щелкните его, чтобы войти на страницу управления кластером.</span><span class="sxs-lookup"><span data-stu-id="7afc6-229">Find your Spark cluster on your dashboard and click it to enter management page for your cluster.</span></span> <span data-ttu-id="7afc6-230">Последовательно выберите **Панели мониторинга кластера** -> **Записная книжка Jupyter**, чтобы открыть записную книжку, связанную с кластером Spark.</span><span class="sxs-lookup"><span data-stu-id="7afc6-230">To open the notebook associated with the Spark cluster, click **Cluster Dashboards** -> **Jupyter Notebook** .</span></span>

![Панели мониторинга кластера](./media/machine-learning-data-science-spark-overview/spark-jupyter-on-portal.png)

<span data-ttu-id="7afc6-232">Также записные книжки Jupyter можно просмотреть по адресу ***https://CLUSTERNAME.azurehdinsight.net/jupyter***.</span><span class="sxs-lookup"><span data-stu-id="7afc6-232">You can also browse to ***https://CLUSTERNAME.azurehdinsight.net/jupyter*** to access the Jupyter Notebooks.</span></span> <span data-ttu-id="7afc6-233">Замените в этом URL-адресе фрагмент CLUSTERNAME именем вашего кластера.</span><span class="sxs-lookup"><span data-stu-id="7afc6-233">Replace the CLUSTERNAME part of this URL with the name of your own cluster.</span></span> <span data-ttu-id="7afc6-234">Для доступа к записным книжкам необходим пароль к учетной записи администратора.</span><span class="sxs-lookup"><span data-stu-id="7afc6-234">You need the password for your admin account to access the notebooks.</span></span>

![Просмотр записных книжек Jupyter](./media/machine-learning-data-science-spark-overview/spark-jupyter-notebook.png)

<span data-ttu-id="7afc6-236">Выберите PySpark, чтобы открыть каталог, содержащий несколько примеров предварительно подготовленных объектов Notebook на основе API PySpark. Объекты Notebook с примерами кода для этой группы статьей о Spark доступны на сайте [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark).</span><span class="sxs-lookup"><span data-stu-id="7afc6-236">Select PySpark to see a directory that contains a few examples of pre-packaged notebooks that use the PySpark API.The notebooks that contain the code samples for this suite of Spark topic are available at [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark)</span></span>

<span data-ttu-id="7afc6-237">Объекты Notebook можно передать непосредственно из [Github](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) на сервер Jupyter Notebook в кластере Spark.</span><span class="sxs-lookup"><span data-stu-id="7afc6-237">You can upload the notebooks directly from [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) to the Jupyter notebook server on your Spark cluster.</span></span> <span data-ttu-id="7afc6-238">На домашней странице записной книжки Jupyter нажмите кнопку **Отправить** в правой части экрана.</span><span class="sxs-lookup"><span data-stu-id="7afc6-238">On the home page of your Jupyter, click the **Upload** button on the right part of the screen.</span></span> <span data-ttu-id="7afc6-239">Откроется окно проводника.</span><span class="sxs-lookup"><span data-stu-id="7afc6-239">It opens a file explorer.</span></span> <span data-ttu-id="7afc6-240">Здесь вы можете вставить URL-адрес GitHub (необработанное содержимое) для Notebook и нажать кнопку **Открыть**.</span><span class="sxs-lookup"><span data-stu-id="7afc6-240">Here you can paste the GitHub (raw content) URL of the Notebook and click **Open**.</span></span> 

<span data-ttu-id="7afc6-241">В списке файлов Jupyter отобразится имя файла с кнопкой **Отправить**.</span><span class="sxs-lookup"><span data-stu-id="7afc6-241">You see the file name on your Jupyter file list with an **Upload** button again.</span></span> <span data-ttu-id="7afc6-242">Нажмите кнопку **Отправить** .</span><span class="sxs-lookup"><span data-stu-id="7afc6-242">Click this **Upload** button.</span></span> <span data-ttu-id="7afc6-243">Записная книжка импортирована.</span><span class="sxs-lookup"><span data-stu-id="7afc6-243">Now you have imported the notebook.</span></span> <span data-ttu-id="7afc6-244">Повторите эти действия для отправки остальных записных книжек из этого руководства.</span><span class="sxs-lookup"><span data-stu-id="7afc6-244">Repeat these steps to upload the other notebooks from this walkthrough.</span></span>

> [!TIP]
> <span data-ttu-id="7afc6-245">Чтобы получить URL-адрес необработанного содержимого, размещенного на Github, щелкните правой кнопкой мыши ссылку на странице браузера и выберите команду **Копировать ссылку**.</span><span class="sxs-lookup"><span data-stu-id="7afc6-245">You can right-click the links on your browser and select **Copy Link** to get the github raw content URL.</span></span> <span data-ttu-id="7afc6-246">Скопированный адрес можно вставить в диалоговое окно загрузки файла Jupyter.</span><span class="sxs-lookup"><span data-stu-id="7afc6-246">You can paste this URL into the Jupyter Upload file explorer dialog box.</span></span>
> 
> 

<span data-ttu-id="7afc6-247">Теперь вы можете:</span><span class="sxs-lookup"><span data-stu-id="7afc6-247">Now you can:</span></span>

* <span data-ttu-id="7afc6-248">просмотреть код, щелкнув записную книжку;</span><span class="sxs-lookup"><span data-stu-id="7afc6-248">See the code by clicking the notebook.</span></span>
* <span data-ttu-id="7afc6-249">выполнить отдельную ячейку, нажав клавиши **SHIFT+ВВОД**;</span><span class="sxs-lookup"><span data-stu-id="7afc6-249">Execute each cell by pressing **SHIFT-ENTER**.</span></span>
* <span data-ttu-id="7afc6-250">выполнить всю записную книжку, последовательно щелкнув **Ячейка** -> **Выполнить**;</span><span class="sxs-lookup"><span data-stu-id="7afc6-250">Run the entire notebook by clicking on **Cell** -> **Run**.</span></span>
* <span data-ttu-id="7afc6-251">использовать автоматическую визуализацию запросов.</span><span class="sxs-lookup"><span data-stu-id="7afc6-251">Use the automatic visualization of queries.</span></span>

> [!TIP]
> <span data-ttu-id="7afc6-252">Ядро Pyspark автоматически визуализирует выходные данные запросов SQL (HiveQL).</span><span class="sxs-lookup"><span data-stu-id="7afc6-252">The PySpark kernel automatically visualizes the output of SQL (HiveQL) queries.</span></span> <span data-ttu-id="7afc6-253">С помощью кнопок в меню **Тип** в записной книжке можно выбрать один из нескольких типов визуализации (таблица либо круговая, линейная, комбинированная или столбчатая диаграмма):</span><span class="sxs-lookup"><span data-stu-id="7afc6-253">You are given the option to select among several different types of visualizations (Table, Pie, Line, Area, or Bar) by using the **Type** menu buttons in the notebook:</span></span>
> 
> 

![Кривая ROC логистической регрессии для универсального подхода](./media/machine-learning-data-science-spark-overview/pyspark-jupyter-autovisualization.png)

## <a name="whats-next"></a><span data-ttu-id="7afc6-255">Что дальше?</span><span class="sxs-lookup"><span data-stu-id="7afc6-255">What's next?</span></span>
<span data-ttu-id="7afc6-256">Теперь, когда вы настроили кластер HDInsight Spark и отправили записные книжки Jupyter, все готово для работы со статьями, посвященными этим трем записным книжкам PySpark.</span><span class="sxs-lookup"><span data-stu-id="7afc6-256">Now that you are set up with an HDInsight Spark cluster and have uploaded the Jupyter notebooks, you are ready to work through the topics that correspond to the three PySpark notebooks.</span></span> <span data-ttu-id="7afc6-257">В них рассказывается, как исследовать данные, а также как создавать и использовать модели.</span><span class="sxs-lookup"><span data-stu-id="7afc6-257">They show how to explore your data and then how to create and consume models.</span></span> <span data-ttu-id="7afc6-258">С помощью записной книжки по расширенному исследованию и моделированию данных можно более подробно ознакомиться с тем, как использовать перекрестную проверку, перебор гиперпараметров и оценку модели.</span><span class="sxs-lookup"><span data-stu-id="7afc6-258">The advanced data exploration and modeling notebook shows how to include cross-validation, hyper-parameter sweeping, and model evaluation.</span></span> 

<span data-ttu-id="7afc6-259">**Исследование и моделирование данных с помощью Spark**. Исследуйте набор данных, а затем создайте, оцените и проанализируйте модели машинного обучения, выполнив инструкции из статьи, посвященной [созданию моделей двоичной классификации и регрессии для данных с помощью набора средств Spark MLlib](machine-learning-data-science-spark-data-exploration-modeling.md).</span><span class="sxs-lookup"><span data-stu-id="7afc6-259">**Data Exploration and modeling with Spark:** Explore the dataset and create, score, and evaluate the machine learning models by working through the [Create binary classification and regression models for data with the Spark MLlib toolkit](machine-learning-data-science-spark-data-exploration-modeling.md) topic.</span></span>

<span data-ttu-id="7afc6-260">**Использование модели**. Дополнительные сведения об оценке моделей классификации и регрессии, созданных в этой статье, см. в статье [Оценка моделей машинного обучения, созданных с помощью Spark](machine-learning-data-science-spark-model-consumption.md).</span><span class="sxs-lookup"><span data-stu-id="7afc6-260">**Model consumption:** To learn how to score the classification and regression models created in this topic, see [Score and evaluate Spark-built machine learning models](machine-learning-data-science-spark-model-consumption.md).</span></span>

<span data-ttu-id="7afc6-261">**Перекрестная проверка и перебор гиперпараметров**. Сведения об обучении моделей с помощью перекрестной проверки и перебора гиперпараметров см. в статье [Расширенное исследование и моделирование данных с помощью Spark](machine-learning-data-science-spark-advanced-data-exploration-modeling.md).</span><span class="sxs-lookup"><span data-stu-id="7afc6-261">**Cross-validation and hyperparameter sweeping**: See [Advanced data exploration and modeling with Spark](machine-learning-data-science-spark-advanced-data-exploration-modeling.md) on how models can be trained using cross-validation and hyper-parameter sweeping</span></span>

