---
title: "Анализ коллекции документов в Azure | Документация Майкрософт"
description: "Практическое руководство по обобщению и анализу больших коллекций документов и использованию таких методов, как изучение фраз, тематическое моделирование и анализ тематических моделей с помощью Azure Machine Learning Workbench."
services: machine-learning
author: kehuan
ms.author: kehuan
manager: mwinkle
ms.reviewer: garyericson, jasonwhowell, MicrosoftDocs/mlreview, mldocs
ms.service: machine-learning
ms.workload: data-services
ms.topic: article
ms.date: 09/20/2017
ms.openlocfilehash: a6034652f27765bb20db4dbbb4c25741b261e50a
ms.sourcegitcommit: 68aec76e471d677fd9a6333dc60ed098d1072cfc
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/18/2017
---
# <a name="document-collection-analysis"></a>Анализ коллекции документов

В этом сценарии показано, как обобщать и анализировать большие коллекции документов, используя такие методы, как изучение фраз, тематическое моделирование и анализ тематических моделей с помощью Azure Machine Learning Workbench. Azure Machine Learning Workbench позволяет легко увеличивать масштаб для очень большой коллекции документов и предоставляет механизмы обучения и настройки моделей в различных контекстах вычислений: от локальных вычислений до виртуальных машин для обработки и анализа данных в кластере Spark. Записные книжки Jupyter в Azure Machine Learning Workbench позволяют упростить процесс разработки.

## <a name="link-to-the-gallery-github-repository"></a>Ссылка на репозиторий коллекции на GitHub

Общедоступный репозиторий GitHub содержит все материалы для этого реального сценария, в том числе примеры кода, которые потребуются в нашем примере.

[https://github.com/Azure/MachineLearningSamples-DocumentCollectionAnalysis](https://github.com/Azure/MachineLearningSamples-DocumentCollectionAnalysis)

## <a name="overview"></a>Обзор

При работе с большим объемом данных (особенно неструктурированных текстовых данных), которые собираются каждый день, не так-то просто упорядочивать, искать и анализировать большие объемы таких текстов. В этом сценарии анализа коллекции документов представлен эффективный автоматический комплексный рабочий процесс для анализа больших коллекций документов и выполнения подчиненных задач обработки естественного языка (NLP).

В этом сценарии предоставлены следующие основные элементы:

1. Изучение ключевых фраз из нескольких слов в документах.

1. Определение базовых тем, представленных в коллекции документов.

1. Представление документов с использованием распределения по темам.

1. Представление методов для упорядочения, поиска и обобщения документов на основе тематического содержимого.

Методы, представленные в этом сценарии, можно задействовать для ряда критически важных рабочих нагрузок в промышленных организациях, например для обнаружения аномалий в тенденциях тем, обобщения коллекций документов и поиска похожих документов. Эти методы применимы к различным типам анализа документов, например государственного законодательства, новостей, обзоров продуктов, отзывов клиентов и научно-исследовательских статей.

В этом сценарии используются следующие методы и алгоритмы машинного обучения:

1. Обработка и очистка текста.

1. Изучение фраз

1. тематического моделирования;

1. Обобщение совокупностей.

1. Обнаружение тематических тенденций и аномалий.

## <a name="prerequisites"></a>Технические условия

Предварительные требования для выполнения этого сценария:

* Правильно установите [Azure Machine Learning Workbench](./overview-what-is-azure-ml.md), следуя [краткому руководству по установке и созданию](quickstart-installation.md).

* Этот пример можно выполнить в любом контексте вычислений. Но мы рекомендуем использовать компьютер с несколькими ядрами, как минимум с 16 ГБ памяти и 5 ГБ дискового пространства.

## <a name="create-a-new-workbench-project"></a>Создание проекта в Workbench

Создайте проект, используя в качестве шаблона следующий пример:
1.  Откройте Azure Machine Learning Workbench.
2.  На странице **Projects** (Проекты) щелкните знак **+** и выберите **New Project** (Создать проект).
3.  В области **Create New Project** (Создание проекта) введите информацию о новом проекте.
4.  В поле поиска **Search Project Templates** (Поиск шаблонов проектов) введите Document Collection Analysis (Анализ коллекции документов) и выберите шаблон.
5.  Нажмите кнопку **Создать**.

## <a name="data-description"></a>Описание данных

Набор данных, используемый в этом сценарии содержит текст сводки и связанные метаданные для каждой регистрации действие, предпринимаемое Конгресса нам. Данные собраны с сайта [GovTrack.us](https://www.govtrack.us/), который позволяет отслеживать деятельность Конгресса США, помогая американцам участвовать в национальном законодательном процессе. Массовые данные можно скачать по [этой ссылке](https://www.govtrack.us/data/congress/), выполнив вручную скрипт, который не входит в этот сценарий. Сведения о том, как скачать данные, можно найти в [документации по API GovTrack](https://www.govtrack.us/developers/api).

### <a name="data-source"></a>Источник данных

В нашем случае собранные необработанные данные представляют собой ряд законодательных актов, предложенных Конгрессом (внесенные законопроекты и резолюции) с 1973 г. по июнь 2017 г. (с 93-го по 115-й созыв Конгресса). Данные собраны в формате JSON и содержат обширную информацию о законодательных актах. Чтобы просмотреть подробное описание полей данных, перейдите на GitHub [по этой ссылке ](https://github.com/unitedstates/congress/wiki/bills). В качестве примера для этого сценария мы извлекли из файлов JSON только несколько полей данных. В этом сценарии предоставлен предварительно скомпилированный TSV-файл `CongressionalDataAll_Jun_2017.tsv` с записями этих законодательных актов. Этот TSV-файл можно скачать автоматически с помощью записной книжки `1_Preprocess_Text.ipynb` в папке записных книжек или с помощью скрипта `preprocessText.py` в пакете Python.

### <a name="data-structure"></a>Структура данных

Файл данных содержит девять полей данных. Имена и описания полей данных перечислены ниже.

| Имя поля | type | ОПИСАНИЕ | Содержит недостающие значения |
|------------|------|-------------|---------------|
| `ID` | Строка | Идентификатор закона или резолюции. Формат этого поля: [bill_type][number]-[congress]. Например, поле hconres1-93, где hconres — тип законопроекта (означает совместную резолюцию обеих палат; дополнительные сведения см. в [этом документе](https://github.com/unitedstates/congress/wiki/bills#basic-information)), 1 — номер законопроекта, а 93 — номер созыва Конгресса. | Нет  |
| `Text` | Строка | Содержимое законопроекта или резолюции. | Нет  |
| `Date` | Строка | Дата первого внесения законопроекта или резолюции. Указывается в формате "гггг-мм-дд". | Нет  |
| `SponsorName` | Строка | Имя основного автора, который предложил законопроект или резолюцию. | Yes |
| `Type` | Строка | Тип должности основного автора: rep (представитель) или sen (сенатор). | Yes |
| `State` | Строка | Штат основного автора. | Yes |
| `District` | Целое число  | Номер округа основного автора, если в качестве должности указан представитель. | Yes |
| `Party` | Строка | Партия основного автора. | Yes |
| `Subjects` | Строка | Термины предметной области, совокупно добавленные в законопроект в Библиотеке Конгресса США. Эти термины указываются через запятую. Они записываются сотрудником Библиотеки Конгресса США и, как правило, отсутствуют при первой публикации сведений о законопроекте. Термины могут быть добавлены в любое время. Таким образом, к концу жизненного цикла законопроекта некоторые термины могут больше не относиться к нему. | Yes |

## <a name="scenario-structure"></a>Структура сценария

Пример анализа коллекции документов представлен для конечных результатов двух типов. Первый тип представляет собой серию записных книжек IPython, содержащих пошаговое описание всего рабочего процесса. Второй тип — это пакет Python, а также примеры кода по использованию этого пакета. Пакет Python довольно универсален и может использоваться в разных сценариях.

Файлы в этом примере упорядочены следующим образом:

| Имя файла | type | ОПИСАНИЕ |
|-----------|------|-------------|
| `aml_config` | Папка | Папка конфигурации Azure Machine Learning Workbench. Дополнительные сведения о конфигурации для выполнения эксперимента см. [в этой документации](./experimentation-service-configuration-reference.md). |
| `Code` | Папка | Папка с кодом для хранения скриптов Python и пакета Python. |
| `Data` | Папка | Папка данных для хранения промежуточных файлов. |
| `notebooks` | Папка | Папка для записных книжек Jupyter. |
| `Code/documentAnalysis/__init__.py` | Файл Python | Файл инициализации пакета Python. |
| `Code/documentAnalysis/configs.py` | Файл Python | Файл конфигурации, используемый пакетом Python для анализа документов, с предопределенными константами. |
| `Code/documentAnalysis/preprocessText.py` | Файл Python | Файл Python, используемый для предварительной обработки данных в подчиненных задачах. |
| `Code/documentAnalysis/phraseLearning.py` | Файл Python | Файл Python, используемый для изучения фраз из данных и преобразования необработанных данных. |
| `Code/documentAnalysis/topicModeling.py` | Файл Python | Файл Python, используемый для обучения тематической модели латентного размещения Дирихле (LDA). |
| `Code/step1.py` | Файл Python | Первый шаг анализа коллекции документов: предварительная обработка текста. |
| `Code/step2.py` | Файл Python | Второй шаг анализа коллекции документов: изучение фраз. |
| `Code/step3.py` | Файл Python | Третий шаг анализа коллекции документов: обучение и анализ тематической модели LDA. |
| `Code/runme.py` | Файл Python | Пример выполнения всех шагов в одном файле. |
| `notebooks/1_Preprocess_Text.ipynb` | Записная книжка IPython | Предварительная обработка текста и преобразование необработанных данных. |
| `notebooks/2_Phrase_Learning.ipynb` | Записная книжка IPython | Изучение фраз из текстовых данных (после преобразования данных). |
| `notebooks/3_Topic_Model_Training.ipynb` | Записная книжка IPython | Обучение тематической модели LDA. |
| `notebooks/4_Topic_Model_Summarization.ipynb` | Записная книжка IPython | Обобщение содержимого коллекции документов на основе обученной тематической модели LDA. |
| `notebooks/5_Topic_Model_Analysis.ipynb` | Записная книжка IPython | Анализ тематического содержимого коллекции текстовых документов и сопоставление тематической информации с другими метаданными, связанными с коллекцией документов. |
| `notebooks/6_Interactive_Visualization.ipynb` | Записная книжка IPython | Интерактивная визуализация обученной тематической модели |
| `notebooks/winprocess.py` | Файл Python | Скрипт Python для многопроцессной обработки, используемой в записных книжках. |
| `README.md` | Файл Markdown | Файл сведений в формате Markdown |

### <a name="data-ingestion-and-transformation"></a>Прием и преобразование данных

Скомпилированный набор данных `CongressionalDataAll_Jun_2017.tsv` сохраняется в хранилище BLOB-объектов. Доступ к нему можно получить с помощью записных книжек и скриптов Python. Прием и преобразование данных состоят из двух этапов: предварительной обработки текста и изучения фраз.

#### <a name="preprocess-text-data"></a>Предварительная обработка текстовых данных

При предварительной обработке применяются методы обработки естественного языка для очистки и подготовки необработанных текстовых данных. Этот этап предшествует неконтролируемому изучению фраз и латентному тематическому моделированию. Подробное описание с пошаговыми инструкциями можно найти в записной книжке `1_Preprocess_Text.ipynb`. Также существует скрипт Python `step1.py`, соответствующий этой записной книжке.

На этом этапе TSV-файл данных скачивается из хранилища BLOB-объектов Azure и импортируется как структура Pandas DataFrame. Каждый элемент строки в кадре данных является одной целостной длинной строкой текста, то есть документом. Затем каждый документ разбивается на блоки текста, которые могут быть предложениями, фразами или подфразами. Разбиение позволяет предотвратить использование строк со словами из разных предложений или разных фраз при изучении фраз.

Существует большое количество функций, определенных для этапа предварительной обработки, с использованием как записной книжки, так и пакета Python. Большую часть задания можно выполнить, вызвав эти две строки кода.

```python
# Read raw data into a Pandas DataFrame
textDF = getData()

# Write data frame with preprocessed text out to TSV file
cleanedDataFrame = CleanAndSplitText(textDF, saveDF=True)
```

#### <a name="phrase-learning"></a>Изучение фраз

На этапе изучения фраз реализуется базовая структура для изучения ключевых фраз в большой коллекции документов. Эта структура описана в документе [Modeling Multiword Phrases with Constrained Phrases Trees for Improved Topic Modeling of Conversational Speech](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf) (Моделирование фраз из нескольких слов с использованием ограниченных деревьев фраз для эффективного тематического моделирования разговорной речи), впервые представленном на семинаре IEEE по технологиям анализа разговорной речи в 2012 г. Подробные инструкции по реализации этапа изучения фраз приведены в записной книжке IPython `2_Phrase_Learning.ipynb` и скрипте Python `step2.py`.

На этом этапе в качестве входных данных используется очищенный текст и изучаются основные ключевые фразы, присутствующие в большой коллекции документов. Изучение фраз — это циклический процесс, который можно разделить на три задачи: вычисление N-грамм, ранжирование потенциальных фраз методом вычисления взвешенного среднего поточечной взаимной информации составных слов и повторного создания фразы в тексте. Эти три задачи выполняются последовательно в несколько итераций, пока указанные фразы не будут изучены.

В пакете Python для анализа документов класс Python `PhraseLearner` определяется в файле `phraseLearning.py`. Ниже приведен фрагмент кода, используемый для изучения фраз.

```python
# Instantiate a PhraseLearner and run a configuration
phraseLearner = PhraseLearner(cleanedDataFrame, "CleanedText", numPhrase,
                        maxPhrasePerIter, maxPhraseLength, minInstanceCount)

# The chunks of text in a list
textData = list(phraseLearner.textFrame['LowercaseText'])

# Learn most salient phrases present in a large collection of documents
phraseLearner.RunConfiguration(textData,
            phraseLearner.learnedPhrases,
            addSpace=True,
            writeFile=True,
            num_workers=cpu_count()-1)
```

> [!NOTE]
> Этот этап изучения фраз реализуется путем многопроцессной обработки. Но наличие большего количества ядер ЦП **НЕ** означает, что процесс будет выполняться быстрее. В наших тестах производительность не улучшится, если на компьютере больше восьми ядер, из-за дополнительной нагрузки при многопроцессной обработке. На компьютере с восемью ядрами ЦП (3,6 ГГц) изучение 25 000 фраз заняло примерно два с половиной часа.
>

### <a name="topic-modeling"></a>тематического моделирования;

Третий шаг в этом сценарии — обучение латентной тематической модели использованию алгоритма LDA. На этом шаге потребуется пакет Python [gensim](https://radimrehurek.com/gensim/) для обучения [тематической модели LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). Соответствующая записная книжка для этого шага — `3_Topic_Model_Training.ipynb`. Вы также можете ознакомиться с инструкциями по использованию пакета для анализа документов в файле `step3.py`.

Основная задача на этом шаге — обучить тематическую модель LDA и настроить гиперпараметры. При обучении модели LDA требуется настроить большое количество параметров, но самым важным является количество тем. Если указать слишком мало тем, представление о коллекции документов будет неполным, а слишком большое количество может стать причиной чрезмерного разбиения совокупности на много мелких и очень похожих тем. Задача этого сценария — показать, как настроить количество тем. Azure Machine Learning Workbench позволяет экспериментировать с использованием различных конфигураций параметров в разных контекстах вычислений.

В пакете Python для анализа документов есть функции, которые помогают пользователям определить оптимальное количество тем. Первый подход заключается в вычислении степени согласованности тематической модели. Поддерживается четыре метрики вычисления: `u_mass`, `c_v`, `c_uci` и `c_npmi`. Сведения об этих четырех метриках приведены в [этом документе](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf). Второй подход заключается в вычислении степени неопределенности в совокупности контрольных данных.

При вычислении степени неопределенности наиболее подходящее количество тем определяется по U-образной кривой. Лучший вариант будет в изгибе этой кривой. Рекомендуется выполнить вычисление несколько раз с разным случайным начальным значением и вычислить среднее значение. Вычисление степени согласованности определяется по n-образной кривой, что означает повышение степени согласованности с увеличением количества тем и последующее снижение. Графики неопределенности и согласованности `c_v` показаны ниже.

![Неопределенность](./media/scenario-document-collection-analysis/Perplexity_Value.png)

![Согласованность c_v](./media/scenario-document-collection-analysis/c_v_Coherence.png)

В этом сценарии после 200 тем степень неопределенности существенно повышается, а степень согласованности значительно снижается. Исходя из данных этих графиков и необходимости использовать более общие темы вместо чрезмерно подробных тем, можно сделать вывод, что оптимальное количество равно 200.

В ходе одного эксперимента вы можете обучить одну тематическую модель LDA или же обучить и проанализировать несколько моделей LDA с разными вариантами количества тем. Мы рекомендуем выполнять эксперимент несколько раз для одной конфигурации, а затем на основе результатов получить среднее значение степени согласованность и (или) неопределенности. Сведения об использовании пакета для анализа документов содержатся в файле `step3.py`. Просмотрите пример в следующем фрагменте кода.

```python
topicmodeler = TopicModeler(docs,
        stopWordFile=FUNCTION_WORDS_FILE,
        minWordCount=MIN_WORD_COUNT,
        minDocCount=MIN_DOC_COUNT,
        maxDocFreq=MAX_DOC_FREQ,
        workers=cpu_count()-1,
        numTopics=NUM_TOPICS,
        numIterations=NUM_ITERATIONS,
        passes=NUM_PASSES,
        chunksize=CHUNK_SIZE,
        random_state=RANDOM_STATE,
        test_ratio=test_ratio)

# Train an LDA topic model
lda = topicmodeler.TrainLDA(saveModel=saveModel)

# Evaluate coherence metrics
coherence = topicmodeler.EvaluateCoherence(lda, coherence_types)

# Evaluate perplexity on a held-out corpus
perplex = topicmodeler.EvaluatePerplexity(lda)
```

> [!NOTE]
> Время, необходимое для обучения тематической модели LDA, зависит от нескольких факторов, таких как размер совокупности, конфигурация гиперпараметров, а также количество ядер на компьютере. Использование нескольких ядер ЦП позволяет ускорить обучение модели. Но при использовании одних и тех же значений гиперпараметров больше ядер означает меньше обновлений при обучении. Мы рекомендуем выполнять **не менее 100 обновлений для обучения конвергированной модели LDA**. Связь между числом обновлений и гиперпараметрами рассматривается в [этой](https://groups.google.com/forum/#!topic/gensim/ojySenxQHi4) и [этой](http://miningthedetails.com/blog/python/lda/GensimLDA/) записях блога. В наших тестах для обучения модели LDA с 200 темами и конфигурацией `workers=15`, `passes=10`, `chunksize=1000` на компьютере с 16 ядрами (2,0 ГГц) потребуется около 3 часов.
>

### <a name="topic-summarization-and-analysis"></a>Обобщение и анализ тем

Анализ и обобщение тем описаны в двух записных книжках, в то время как в пакете для анализа документов нет соответствующих функций.

В файле `4_Topic_Model_Summarization.ipynb` показано, как обобщить содержимое документа на основе обученной тематической модели LDA. Обобщение применяется к тематической модели LDA, обученной на шаге 3. В этом примере показано, как измерить степень важности или качества темы, используя ее для документирования меры чистоты. Мера чистоты предполагает, что скрытые темы, которые часто встречаются в документах, имеют большую семантическую важность, чем скрытые темы, которые неравномерно распределены в большом количестве документов. Эта концепция была представлена в документе [Latent Topic Modeling for Audio Corpus Summarization](http://people.csail.mit.edu/hazen/publications/Hazen-Interspeech11.pdf) (Моделирование скрытых тем для обобщения совокупности аудиоданных).

В записной книжке `5_Topic_Model_Analysis.ipynb` показано, как анализировать тематическое содержимое коллекции документов и соотносить тематическую информацию с другими метаданными, связанными с коллекцией документов. В этой записной книжке представлены несколько графиков для лучшего понимания изученной темы и коллекции документов.

В записной книжке `6_Interactive_Visualization.ipynb` показано, как интерактивно визуализировать обученную тематическую модель. Она включает четыре задачи интерактивной визуализации.

## <a name="conclusion"></a>Заключение

В этом реальном сценарии мы рассмотрели, как используются известные методы анализа текста (в нашем случае изучение фраз и тематическое моделирование LDA) для создания надежной модели и как с помощью Azure Machine Learning Workbench можно отслеживать производительность модели и выполнять алгоритмы обучения в большом масштабе. Более подробно:

* Используйте изучение фраз и тематическое моделирование для обработки коллекции документов и создания надежной модели. При работе с очень большими коллекциями документов Azure Machine Learning Workbench позволяет легко увеличивать и уменьшать их масштаб. Кроме того, с помощью Azure Machine Learning Workbench пользователи могут легко выполнять эксперименты в разных контекстах вычислений.

* Azure Machine Learning Workbench предусматривает оба способа работы: последовательный запуск записных книжек и запуск всего эксперимента с помощью скрипта Python.

* Настройте гиперпараметры с помощью Azure Machine Learning Workbench для определения оптимального количества тем, необходимых для изучения. Azure Machine Learning Workbench позволяет отслеживать производительность модели и просто выполнять алгоритмы обучения в большом масштабе.

* Используя Azure Machine Learning Workbench, вы можете управлять журналом выполнения и обученными моделями. Благодаря этому специалисты по анализу и обработке данных могут быстро определить модели с наилучшей производительностью, а также найти скрипты и данные, используемые для их создания.

## <a name="references"></a>Ссылки

* **Тимоти Дж. Хейзен (Timothy J. Hazen), Фред Ричардсон (Fred Richardson)**. [_Modeling Multiword Phrases with Constrained Phrases Trees for Improved Topic Modeling of Conversational Speech_](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf) (Моделирование фраз из нескольких слов с использованием ограниченных деревьев фраз для эффективного тематического моделирования разговорной речи). Семинар IEEE по технологиям анализа разговорной речи, 2012 г. IEEE, 2012 г.

* **Тимоти Дж. Хейзен**. [_Latent Topic Modeling for Audio Corpus Summarization_](http://people.csail.mit.edu/hazen/publications/Hazen-Interspeech11.pdf) (Моделирование скрытых тем для обобщения совокупности аудиоданных). 12-я ежегодная конференция Международной ассоциации по речевой коммуникации. 2011 г.

* **Майкл Родер (Michael Roder), Андреас Бот (Andreas Both), Александр Хайнбург (Alexander Hinneburg)**. [_Exploring the Space of Topic Coherence Measures_](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf) (Исследование пространства измерения степени согласованности тем). Материалы 8-й международной конференции ACM по поиску в Интернете и интеллектуальному анализу данных. ACM, 2015 Г.
