---
title: "Классификация изображений с использованием CNTK в Azure Machine Learning Workbench | Документация Майкрософт"
description: "Обучение, оценка и развертывание собственной модели классификации изображений с помощью Azure ML Workbench."
services: machine-learning
documentationcenter: 
author: PatrickBue
ms.author: pabuehle
manager: mwinkle
ms.reviewer: mawah, marhamil, mldocs, garyericson, jasonwhowell
ms.service: machine-learning
ms.workload: data-services
ms.topic: article
ms.date: 10/17/2017
ms.openlocfilehash: 53d182d84c8f28c7b4055780a5b41df00fdc8583
ms.sourcegitcommit: 68aec76e471d677fd9a6333dc60ed098d1072cfc
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/18/2017
---
# <a name="image-classification-using-azure-machine-learning-workbench"></a>Классификация изображений в Azure Machine Learning Workbench

Модели классификации изображений можно использовать для решения большого числа задач, связанных с компьютерным зрением.
В частности, можно создать модели, которые отвечают на вопрос *Присутствует ли на изображении объект?*, где объектом может быть, к примеру, *собака*, *автомобиль* или *корабль*. Это также могут быть более сложные вопросы, такие как: *Какой класс тяжести глазной болезни обнаруживается при сканировании сетчатки пациента?*

В этом руководстве рассматривается решение таких задач. Вы узнаете, как обучать, оценивать и развертывать вашу собственную модель классификации изображений с помощью набора средств [Microsoft Cognitive Toolkit (CNTK)](https://docs.microsoft.com/cognitive-toolkit/) для глубокого обучения.
Примеры изображений предоставляются, но читатель может также использовать свой собственный набор данных и обучить собственные модели.

В решениях компьютерного зрения традиционно требуются экспертные знания, чтобы вручную определить и реализовать так называемые *признаки*, которые выделяют требуемую информацию на изображениях.
Этот подход был изменен в 2012 году, когда появился знаменитый документ, посвященный сети [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) [1] в контексте глубокого обучения, а в настоящее время для автоматического поиска этих признаков используются глубокие нейронные сети (DNN).
Глубокие нейронные сети привели к значительному улучшению в области машинного обучения не только для классификации изображений, но и для других задач компьютерного зрения, таких как обнаружение объектов и сходство изображений.


## <a name="link-to-the-gallery-github-repository"></a>Ссылка на репозиторий коллекции на GitHub
[https://github.com/Azure/MachineLearningSamples-ImageClassificationUsingCNTK](https://github.com/Azure/MachineLearningSamples-ImageClassificationUsingCNTK)

## <a name="overview"></a>Обзор

Это руководство состоит из трех частей:

- В части 1 показано, как обучать, оценивать и развертывать систему классификации изображений с использованием предварительно подготовленных глубоких нейронных сетей в качестве характеризатора, а также обучать модель опорных векторов (SVM) на основе выходных данных.
- В части 2 показано, как повысить точность, например путем уточнения DNN вместо использования ее в качестве постоянного характеризатора.
- В части 3 описывается, как использовать собственный набор данных вместо представленных примеров изображений и при необходимости создать собственный набор данных с помощью изображений из сети.

Хотя предыдущий опыт работы с машинным обучением и CNTK не требуется, он поможет понять базовые принципы. Точность, время обучения и т. д, указанные в руководстве, предназначены только для справки, а фактические значения, которые вы получите при выполнении кода, почти наверняка будут отличаться.


## <a name="prerequisites"></a>Технические условия

Предварительные требования для выполнения этого сценария:

1. [Учетная запись Azure](https://azure.microsoft.com/free/) (доступны бесплатные пробные версии).
2. [Azure Machine Learning Workbench](./overview-what-is-azure-ml.md). Чтобы установить эту программу и создать рабочую область, выполните инструкции из [краткого руководства по установке](./quickstart-installation.md).  
3. Компьютер Windows. ОС Windows необходима, так как Workbench поддерживает только Windows и MacOS, а Cognitive Toolkit от Microsoft (который мы используем как библиотеку глубокого обучения) поддерживает только Windows и Linux.
4. Выделенный GPU не требуется для выполнения обучения SVM в части 1, однако он нужен для уточнения DNN, описанного в части 2. Если вам не хватает мощного графического процессора, вы хотите выполнить обучение с использованием нескольких графических процессоров или у вас нет компьютера Windows, тогда рассмотрите возможность использования виртуальной машины глубокого обучения Azure с операционной системой Windows. Руководство по развертыванию одним щелчком мыши см. [здесь](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-ads.dsvm-deep-learning). После развертывания подключитесь к виртуальной машине через подключение к удаленному рабочему столу, установите на ней Workbench и выполните код локально на виртуальной машине.
5. Необходимо установить различные библиотеки Python, например OpenCV. В Workbench в меню *File* (Файл) щелкните *Open Command Prompt* (Открыть командную строку) и выполните следующие команды, чтобы установить эти зависимости:  
    - `pip install https://cntk.ai/PythonWheel/GPU/cntk-2.2-cp35-cp35m-win_amd64.whl`  
    - `pip install opencv_python-3.3.1-cp35-cp35m-win_amd64.whl` после скачивания расширения wheel OpenCV по адресу http://www.lfd.uci.edu/~gohlke/pythonlibs/ (точное имя файла и версия могут изменяться).
    - `conda install pillow`
    - `pip install -U numpy`
    - `pip install bqplot`
    - `jupyter nbextension enable --py --sys-prefix bqplot`

### <a name="troubleshooting--known-bugs"></a>Устранение неполадок и известные ошибки
- Графический процессор необходим для работы со второй частью руководства. В противном случае при попытке уточнения DNN возникнет ошибка "Batch normalization training on CPU is not yet implemented" (Пакетная нормализация обучения в ЦП еще не реализована).
- Ошибок нехватки памяти во время обучения DNN можно избежать, уменьшив размер мини-пакета (переменная `cntk_mb_size` в `PARAMETERS.py`).
- Код был протестирован с помощью CNTK 2.2, а должен также выполнения на более старые размера (до версии 2.0) и более поздних версиях, без каких-либо или только незначительные изменения.
- На момент написания этой статьи в Azure Machine Learning Workbench возникали проблемы с отображением записных книжек размером более 5 МБ. Записные книжки могут иметь такой размер, если их сохранить при отображении всех выходных данных ячеек. Если вы столкнулись с этой ошибкой, откройте командную строку из меню File (Файл) внутри Workbench, выполните `jupyter notebook`, откройте записную книжку, очистите все выходные данные и сохраните ее. После выполнения этих действий записная книжка откроется в Azure Machine Learning Workbench надлежащим образом.
- Все сценарии, описанные в этом примере должны быть выполнены локально, а не на например удаленной среде docker. Всех записных книжек должны выполняться с ядра присвоено ядра локального проекта с именем «ИМЯ_ПРОЕКТА локального» (например «myImgClassUsingCNTK локального»).

    
## <a name="create-a-new-workbench-project"></a>Создание проекта в Workbench

Создайте проект, используя в качестве шаблона следующий пример:
1.  Откройте Azure Machine Learning Workbench.
2.  На странице **Projects** (Проекты) щелкните знак **+** и выберите **New Project** (Создать проект).
3.  В области **Create New Project** (Создание проекта) введите информацию о новом проекте.
4.  В поле поиска **Search Project Templates** (Поиск шаблонов проектов) введите Image Classification (Классификация изображений) и выберите шаблон.
5.  Нажмите кнопку **Создать**.

В результате выполнения этих шагов будет создана структура проекта, показанная ниже. Каталог проекта ограничен размером 25 МБ, так как Azure Machine Learning Workbench создает копию этой папки после каждого запуска (для ведения журнала выполнения). Следовательно, все изображения и временные файлы сохраняются в каталоге *~/Desktop/imgClassificationUsingCntk_data* (в этом документе упоминается как *DATA_DIR*).

  Папка| ОПИСАНИЕ
  ---|---
  aml_config/|                           Каталог, содержащий файлы конфигурации Azure Machine Learning Workbench
  libraries/|                              Каталог, содержащий все вспомогательные функции Python и Jupyter
  notebooks/|                              Каталог, содержащий все записные книжки
  resources/|                              Каталог, содержащий все ресурсы (например, URL-адреса изображений для рекламы)
  scripts/|                              Каталог, содержащий все скрипты
  PARAMETERS.py|                       Скрипт Python, указывающий все параметры
  readme.md|                           Файл сведений


## <a name="data-description"></a>Описание данных

В этом руководстве в качестве рабочего примера используется набор данных с типами текстур тканей верхней одежды, состоящий из 428 изображений. Каждое изображение помечается как одна из трех различных текстур (в крапинку, в полоску, леопардовая). Мы уменьшили размер изображений, чтобы этот пример можно было быстро выполнить. Однако код хорошо проверен и работает с десятками тысяч изображений или более. Все изображения были найдены с помощью API Bing для поиска изображений и подписаны вручную, как описано в [части 3](#using-a-custom-dataset). URL-адреса изображений с соответствующими атрибутами перечислены в файле */resources/fashionTextureUrls.tsv*.

Скрипт `0_downloadData.py` скачивает все изображения в каталог *DATA_DIR/images/fashionTexture/*. Некоторые из 428 URL-адресов могут быть неработающими. Это просто означает, что у вас будет меньше изображений для обучения и тестирования. Все сценарии, описанные в этом примере должны быть выполнены локально, а не на например удаленной среде docker.

На следующем изображении показаны примеры для атрибутов "в крапинку" (слева), "в полоску" (посередине) и "леопард" (справа). Заметки сделаны в соответствии с элементами верхней одежды.

<p align="center">
<img src="media/scenario-image-classification-using-cntk/examples_all.jpg"  alt="alt text" width="700">
</p>


## <a name="part-1---model-training-and-evaluation"></a>Часть 1. Обучение и проверка модели

В первой части этого руководства мы обучаем систему, которая использует, но не модифицирует, предварительно обученную глубокую нейронную сеть. Эта предварительно подготовленная DNN используется в качестве характеризатора, а линейная модель опорных векторов обучается, чтобы предсказать атрибут данного изображения (в крапинку, в полоску или леопардовая).

Мы пошагово опишем этот подход и покажем, какие скрипты необходимо выполнить. После каждого шага рекомендуется проверять, какие файлы записаны и куда они записаны.

Все важные параметры указаны, а краткое описание представлено в одном расположении: файл `PARAMETERS.py`.




### <a name="step-1-data-preparation"></a>Шаг 1. Подготовка данных
`Script: 1_prepareData.py. Notebook: showImages.ipynb`

Записную книжку `showImages.ipynb` можно использовать для визуализации изображений и для исправления их заметок по мере необходимости. Чтобы запустить Блокнот, откройте его в Workbench обучения машины Azure, щелкните на «запуск записной книжки Server», если этот параметр отображается, измените в ядро локального проекта с именем «ИМЯ_ПРОЕКТА локального» (например «myImgClassUsingCNTK локальные»), и выполните все ячейки в портативный компьютер. См. раздел об устранении неполадок в этом документе, если возникнет сообщение о том, что записная книжка слишком большая для отображения.
<p align="center">
<img src="media/scenario-image-classification-using-cntk/notebook_showImages.jpg" alt="alt text" width="700"/>
</p>

Теперь выполните скрипт с именем `1_prepareData.py`, который присваивает все изображения либо набору обучения, либо набору тестирования. Это назначение является взаимоисключающим. Для тестирования не используется изображение для обучения и наоборот. По умолчанию для обучения назначаются 75 % случайных изображений из каждого класса атрибутов, а остальные 25 % назначаются для тестирования. Все данные, созданные скриптом, сохраняются в папке *DATA_DIR/proc/fashionTexture/*.

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_1_white.jpg" alt="alt text" width="700"/>
</p>



### <a name="step-2-refining-the-deep-neural-network"></a>Шаг 2. Уточнение глубокой нейронной сети
`Script: 2_refineDNN.py`

Как мы объяснили в первой части этого руководства, предварительно подготовленная DNN находится в фиксированном состоянии (то есть она не уточняется). Однако скрипт с именем `2_refineDNN.py` все еще выполняется в части 1, так как он загружает предварительно обученную модель [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) [2] и модифицирует ее, например, чтобы принимать входные изображения более высокого разрешения. Этот шаг выполняется быстро (в секундах) и не требует графического процессора.

Во второй части руководства изменение файла PARAMETERS.py приведет к тому, что скрипт `2_refineDNN.py` также будет уточнять предварительно обученную DNN. По умолчанию во время уточнения выполняется 45 эпох обучения.

В обоих случаях окончательная модель записывается в файл *DATA_DIR/proc/fashionTexture/cntk_fixed.model*.

### <a name="step-3-evaluate-dnn-for-all-images"></a>Шаг 3. Оценка всех изображений с помощью DNN
`Script: 3_runDNN.py`

Теперь мы можем использовать (возможно, уточненную) DNN из последнего шага, чтобы присвоить признаки изображениям. Используя изображение в качестве входных данных для DNN, мы получаем 512-битный вектор с плавающей запятой из предпоследнего слоя модели. Данный вектор имеет гораздо меньший размер, чем само изображение. Тем не менее он содержит (и даже выделяет) всю информацию на изображении, относящуюся к распознаванию атрибута изображения, то есть если предмет одежды имеет полосатую, леопардовую текстуру или текстуру в крапинку.

Все представления изображений DNN сохраняются в файле  *DATA_DIR/proc/fashionTexture/cntkFiles/features.pickle*.

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_4_white.jpg" alt="alt text" width="700"/>
</p>


### <a name="step-4-support-vector-machine-training"></a>Шаг 4. Метод обучения с помощью опорных векторов
`Script: 4_trainSVM.py`

512-битные представления с плавающей запятой, вычисленные на последнем шаге, теперь используются для обучения классификатора SVM. Используя изображение в качестве входных данных, SVM возвращает оценку для каждого атрибута, который должен присутствовать. В нашем примере набора данных это означает оценку для различных текстур (в крапинку, в полоску, леопардовая).

Скрипт `4_trainSVM.py` загружает обучающие изображения, обучает SVM при разных значениях параметра регуляризации С (slack) и обеспечивает максимальную точность SVM. Точность классификации выводится в консоли и отображается в Workbench. Для предоставленных данных текстуры эти значения должны быть около 100 % и 88 % соответственно. Затем обученная SVM записывается в файл *DATA_DIR/proc/fashionTexture/cntkFiles/svm.np*.

<p align="center">
<img src="media/scenario-image-classification-using-cntk/vienna_svm_log_zoom.jpg" alt="alt text" width="700"/>
</p>



### <a name="step-5-evaluation-and-visualization"></a>Шаг 5.Оценка и визуализация
`Script: 5_evaluate.py. Notebook: showResults.ipynb`

Точность обученного классификатора изображений можно измерить с помощью скрипта `5_evaluate.py`. Скрипт оценивает все тестовые изображения, используя обученный классификатор SVM, присваивает каждому изображению атрибут с наивысшей оценкой и сравнивает предсказанные атрибуты с реальными заметками.

Выходные данные скрипта `5_evaluate.py` показаны ниже. Вычисляется точность классификации каждого отдельного класса, а также точность полного тестового набора (общая точность) и среднее значение индивидуальной точности (усредненная точность в классе). 100 % соответствует максимально возможной точности, а 0 % — наихудшей. Случайное угадывание в среднем дает усредненную точность в классе 1 по числу атрибутов. В нашем случае эта точность составляет 33,33 %. Результат значительно улучшится при использовании более высокого разрешения входных данных, например `rf_inputResoluton = 1000`, однако за счет более длительного вычисления DNN.

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_6_white.jpg" alt="alt text" width="700"/>
</p>

Для демонстрирования точности также строится кривая ROC с соответствующей областью под кривой (слева). Матрица неточностей показана справа.

<p align="center">
<img src="media/scenario-image-classification-using-cntk/roc_confMat.jpg" alt="alt text" width="700"/>
</p>

Наконец, записной книжки `showResults.py` предоставляется прокручивать тестовых изображений и визуализировать их соответствующих классификации оценки. Как описано в шаг 1, каждый записной книжки в этом образце должен использовать ядро локального проекта с именем «ИМЯ_ПРОЕКТА локального».
<p align="center">
<img src="media/scenario-image-classification-using-cntk/notebook_showResults.jpg" alt="alt text" width="700"/>
</p>





### <a name="step-6-deployment"></a>Шаг 6. Развертывание
`Scripts: 6_callWebservice.py, deploymain.py. Notebook: deploy.ipynb`

Опытным системным теперь могут быть опубликованы как REST API. Описывается развертывание в записной книжке `deploy.ipynb`и на основе функциональности Azure Machine Learning рабочей среде (не забудьте задать в качестве ядра ядро локального проекта с именем «ИМЯ_ПРОЕКТА local»). В разделе также отлично развертывания [учебника IRIS](https://docs.microsoft.com/azure/machine-learning/preview/tutorial-classifying-iris-part-3) для развертывания, Дополнительные сведения, связанные с.

После развертывания веб-службу можно вызвать с помощью скрипта `6_callWebservice.py`. Обратите внимание, что IP-адрес (локальный или облачный) веб-службы необходимо сначала задать в скрипте. В записной книжке `deploy.ipynb` объясняется, как найти этот IP-адрес.








## <a name="part-2---accuracy-improvements"></a>Часть 2. Повышение точности

В первой части было показано, как классифицировать изображение, обучая линейный метод опорных векторов на основе 512-битных выходных данных с плавающей запятой глубокой нейронной сети. Эта DNN была предварительно обучена на миллионах изображений, а предпоследний слой возвращен в виде вектора признаков. Это быстрый подход, так как DNN используется "как есть", но тем не менее часто дает хорошие результаты.

Теперь рассмотрим несколько способов повышения точности модели из части 1. В частности, мы уточним DNN, вместо того чтобы оставить ее фиксированной.

### <a name="dnn-refinement"></a>Уточнение DNN

Вместо SVM можно выполнить классификацию непосредственно в нейронной сети. Это достигается путем добавления нового последнего слоя в предварительно обученной DNN, который принимает 512-битные данные с плавающей запятой предпоследнего слоя в качестве входных данных. Преимущество выполнения классификации в DNN заключается в том, что теперь полную сеть можно переобучить с использованием обратного распространения. Такой подход часто обеспечивает более высокую точность классификации по сравнению с использованием предварительно подготовленной DNN "как есть", однако за счет гораздо более продолжительного времени обучения (даже с использованием GPU).

Обучение нейронной сети вместо SVM выполняется путем изменения переменной `classifier` в `PARAMETERS.py` с `svm` на `dnn`. Затем, как описано в части 1, все скрипты, за исключением скрипта для подготовки данных (шаг 1) и обучения SVM (этап 3), необходимо выполнить снова. Уточнение DNN требует наличия графического процессора. Если GPU не найден или заблокирован (например, предыдущим запуском CNTK), тогда скрипт `2_refineDNN.py` выдаст ошибку. Ошибок нехватки памяти во время обучения DNN можно избежать, уменьшив размер мини-пакета (переменная `cntk_mb_size` в `PARAMETERS.py`).

После завершения обучения уточненная модель сохраняется в каталоге *DATA_DIR/proc/fashionTexture/cntk_refined.model* и отображается график, на котором показано, как во время обучения меняются ошибки классификации наборов обучения и тестирования. Обратите внимание, что ошибок в обучающем наборе намного меньше, чем в тестовом наборе. Это так называемое поведение чрезмерной детализации можно исправить, например, используя более высокое значение коэффициента отсева `rf_dropoutRate`.
<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_3_plot.png" alt="alt text" height="300"/>
</p>

Как видно на приведенном ниже графике, точность при использовании уточненной DNN для предоставленного набора данных составляет 92,35 % против 88,92 % (часть 1). В частности, изображения с текстурой "в крапинку" значительно улучшаются, а площадь ROC под кривой имеет значение 0,98 с уточнением по сравнению с предыдущим значением 0,94. Так как используется небольшой набор данных, фактическая точность выполнения кода отличается. Это несоответствие обусловлено стохастическими эффектами, такими как случайное разделение изображений на обучающие и тестовые наборы.
<p align="center">
<img src="media/scenario-image-classification-using-cntk/roc_confMat_dnn.jpg" alt="alt text" width="700"/>
</p>

### <a name="run-history-tracking"></a>Отслеживание журнала выполнения

Azure Machine Learning Workbench хранит историю каждого запуска в Azure, чтобы можно было сравнивать несколько выполнений, даже с разницей в несколько недель. Это подробно рассматривается в [руководстве по классификации цветков ириса](https://docs.microsoft.com/azure/machine-learning/preview/tutorial-classifying-iris-part-2). Это также проиллюстрировано на следующих снимках экрана, где сравниваются два выполнения скрипта `5_evaluate.py`, используя либо уточнение DNN, то есть `classifier = "dnn"` (номер выполнения 148), либо обучение SVM, то есть `classifier = "svm"` (номер выполнения 150).

На первом снимке экрана уточнение DNN обеспечивает более высокую точность, чем обучение SVM для всех классов. На втором снимке экрана отображаются все метрики, которые отслеживаются, включая классификатор. Это отслеживание выполняется в скрипте `5_evaluate.py` путем вызова средства ведения журнала Azure Machine Learning Workbench. Кроме того, скрипт также сохраняет кривую ROC и матрицу неточности в папке *outputs*. Функция ведения журнала Workbench может отслеживать содержимое этой *папки*, таким образом выходные данные доступны в любое время, независимо от того, были ли перезаписаны локальные копии.

<p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison1.jpg" alt="alt text" width="700"/>  
</p>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison2b.jpg" alt="alt text" width="700"/>
</p>


### <a name="parameter-tuning"></a>Настройка параметров
Как и для большинства проектов машинного обучения, для получения хороших результатов при работе с новым набором данных требуется тщательная настройка параметров, а также следует рассмотреть различные проектные решения. Чтобы упростить эти задачи, все важные параметры указаны, а краткое описание представлено в одном расположении: файл `PARAMETERS.py`.

Некоторые из наиболее эффективных методов улучшения:

- Качество данных: убедитесь в том, что обучающий и тестовый наборы имеют высокое качество. То есть изображения имеют правильные обозначения, удалены неоднозначные изображения (например, предметы одежды с полосками и крапинками одновременно), а атрибуты являются взаимоисключающими (то есть каждое изображение относится к одному атрибуту).
- Если нужный объект на изображении небольшой, то методы классификации изображений работают не лучшим образом. В таких случаях рекомендуется использовать метод обнаружения объектов, описанный в этом [руководстве](https://github.com/Azure/ObjectDetectionUsingCntk).
- Уточнение DNN: возможно, самый важный параметр, который нужно задать правильно, — это скорость обучения `rf_lrPerMb`. Если точность в обучающем наборе (первая диаграмма в части 2) не составляет 0–5 %, скорее всего, это связано с неправильной скоростью обучения. Другие параметры, начинающиеся с `rf_`, менее важны. Как правило, после обучения ошибка должна уменьшаться экспоненциально и быть близкой к 0 %.
- Разрешение входных данных: разрешение по умолчанию составляет 224 x 224 пикселя. Использование более высокого разрешения изображений (параметр: `rf_inputResoluton`), например 448 x 448 или 896 x 896 пикселей, зачастую значительно повышает точность, но замедляет уточнение DNN. **Использование более высокого разрешения изображений почти всегда повышает точность**.
- Чрезмерная детализация DNN: избегайте большого окна между точностью обучения и тестирования во время уточнения DNN (первая диаграмма в части 2). Это окно можно уменьшить за счет использования коэффициентов отсева `rf_dropoutRate` со значением 0,5 или больше и увеличения веса регуляризатора `rf_l2RegWeight`. Использование высокой скорости отсева может быть особенно полезно в случае большого разрешения входного изображения DNN.
- Попробуйте использовать большую глубину сетей DNN, изменив `rf_pretrainedModelFilename` из `ResNet_18.model` на `ResNet_34.model` или `ResNet_50.model`. Модель Resnet-50 не только глубже, но ее выходные данные предпоследнего слоя представляют собой 2048-битные данные с плавающей запятой (по сравнению с 512-битными данными с плавающей запятой в моделях ResNet-18 и ResNet 34). Такое увеличение размера может быть особенно полезным при обучении классификатора SVM.

## <a name="part-3---custom-dataset"></a>Часть 3. Пользовательский набор данных

В частях 1 и 2 мы обучали и оценивали модель классификации изображений, используя предоставленные изображения текстур верхней одежды. Теперь рассмотрим, как использовать пользовательский набор данных, предоставляемый пользователем. Или, если он недоступен, как создать такой набор данных и добавить к нему заметки, используя API Bing для поиска изображений.

### <a name="using-a-custom-dataset"></a>Использование пользовательского набора данных

Взглянем на структуру папок для данных текстуры одежды. Обратите внимание, как все изображения для разных атрибутов находятся в соответствующих подпапках *dotted*, *leopard и *striped* в каталоге *DATA_DIR/images/ashionTexture/*. Обратите внимание также, что имя папки изображений также встречается в файле `PARAMETERS.py`:
```python
datasetName = "fashionTexture"
```

Использовать пользовательский набор данных так же просто, как и воспроизводить структуру этих папок, где все изображения находятся в подпапках в соответствии с атрибутами, и копировать эти подпапки в новый указанный пользователем каталог *DATA_DIR/images/newDataSetName/*. Необходимо только изменить значение переменной `datasetName` на *newDataSetName*. Скрипты 1–5 можно выполнить по порядку, а все промежуточные файлы будут записаны в папку *DATA_DIR/proc/newDataSetName/*. Другие изменения кода не требуются.

Важно, чтобы каждому изображению можно было присвоить ровно один атрибут. Например, было бы неправильно присваивать изображению одновременно атрибуты animal и leopard, так как изображение с атрибутом animal также принадлежит к атрибуту animal. Кроме того, рекомендуется удалять изображения, которые являются неоднозначными и трудно описываемыми.



### <a name="image-scraping-and-annotation"></a>Импорт изображений и добавление к ним описания

Собрать достаточно большое количество обозначенных изображений для обучения и тестирования может быть непросто. Один из способов решения этой проблемы — взять изображения из Интернета. Например, см. ниже результаты API Bing для поиска изображений на запрос *t-shirt striped*. Как и ожидалось, большинство изображений на самом деле являются футболками в полоску. Несколько нерелевантных или неоднозначных изображений (например, столбец 1, строка 1 или столбец 3, строка 2) можно легко определить и удалить:
<p align="center">
<img src="media/scenario-image-classification-using-cntk/bing_search_striped.jpg" alt="alt text" width="600"/>
</p>

Чтобы создать большой и разнообразный набор данных, необходимо использовать несколько запросов. Например, запросы 7 \* 3 = 21 можно синтезировать автоматически, используя все комбинации предметов одежды {блуза, толстовка, пуловер, свитер, рубашка, футболка, жилет} и атрибуты {в полоску, в крапинку, леопардовый}. В таком случае скачивание 50 лучших изображений на запрос приведет к максимальному значению 21 х 50 = 1050 изображений.

Вместо того чтобы вручную загружать изображения с помощью API Bing для поиска изображений, гораздо проще использовать [API Bing Cognitive Services для поиска изображений](https://www.microsoft.com/cognitive-services/bing-image-search-api), который возвращает набор URL-адресов изображений по строке запроса.

Некоторые из скачанных изображений являются точными или близкими к ним дубликатами (например, отличаются только разрешением изображения или артефактами JPG). Эти дубликаты следует удалить, чтобы наборы для обучения и тестирования не содержали одинаковых изображений. Удаление повторяющихся изображений можно выполнить с помощью хэширования в два этапа: (1) сначала хэш-строка вычисляется для всех изображений; (2) далее сохраняются только изображения с хэш-строкой, которая еще не была замечена. Остальные изображения игнорируются. Подход `dhash` находится в библиотеке Python `imagehash` и описан в этом [блоге](http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html). Он лучше работает, если установить для параметра `hash_size` значение 16. Некорректное удаление некоторых неповторяющихся изображений не является проблемой, если удаляется большинство реальных дубликатов.





## <a name="conclusion"></a>Заключение

Вот несколько важных выводов по этому примеру.
- Код для обучения, оценки и развертывания моделей классификации изображений.
- Демонстрационные изображения предоставляются, но легко адаптируются (изменение одной строки) для использования собственного набора данных изображений.
- Современные специализированные признаки, реализованные для обучения высокоточных моделей, основанные на переносе обучения.
- Интерактивная разработка моделей с использованием Azure Machine Learning Workbench и записной книжки Jupyter.


## <a name="references"></a>Ссылки

[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton (Алекс Крижевский, Илья Суцкевер и Джеффри Э. Хинтон), [_ImageNet Classification with Deep Convolutional Neural Networks_](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (Классификация ImageNet с помощью глубоких сверточных нейронных сетей). NIPS 2012.  
[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (Каймин Хэ, Сянгюй Чжан, Шаоцин Жень и Цзянь Сунь) [_Deep Residual Learning for Image Recognition_](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) (Глубокое остаточное обучение для распознавания изображений). CVPR 2016.
