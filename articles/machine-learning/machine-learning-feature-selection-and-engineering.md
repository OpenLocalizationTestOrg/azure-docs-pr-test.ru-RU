---
title: "Конструирование и выбор признаков в машинном обучении Azure | Документация Майкрософт"
description: "В этой статье объясняются цели выбора и конструирования признаков и приводятся примеры того, как эти процессы влияют на совершенствование данных в машинном обучении."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 9ceb524d-842e-4f77-9eae-a18e599442d6
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 01/18/2017
ms.author: zhangya;bradsev
ROBOTS: NOINDEX
redirect_url: machine-learning-data-science-create-features
redirect_document_id: TRUE
ms.openlocfilehash: 51a5d8fed492cb9301e048c2b6a721e4573a47d9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/11/2017
---
# <a name="feature-engineering-and-selection-in-azure-machine-learning"></a><span data-ttu-id="93c3f-103">Реконструирование и выбор признаков в машинном обучении Azure</span><span class="sxs-lookup"><span data-stu-id="93c3f-103">Feature engineering and selection in Azure Machine Learning</span></span>
<span data-ttu-id="93c3f-104">В этой статье объясняются цели конструирования и выбора признаков в процессе совершенствования данных в машинном обучении.</span><span class="sxs-lookup"><span data-stu-id="93c3f-104">This topic explains the purposes of feature engineering and feature selection in the data-enhancement process of machine learning.</span></span> <span data-ttu-id="93c3f-105">На примерах, предоставляемых студией машинного обучения Azure, показаны составляющие этих процессов.</span><span class="sxs-lookup"><span data-stu-id="93c3f-105">It illustrates what these processes involve by using examples provided by Azure Machine Learning Studio.</span></span>

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="93c3f-106">Обучающие данные, используемые в машинном обучении, можно улучшить путем выбора или извлечения признаков из собранных необработанных данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-106">The training data used in machine learning can often be enhanced by the selection or extraction of features from the raw data collected.</span></span> <span data-ttu-id="93c3f-107">Пример конструирования признака в контексте обучения для классификации изображений рукописных символов содержит карту битовой плотности, построенную на основе необработанных данных распределения битов.</span><span class="sxs-lookup"><span data-stu-id="93c3f-107">An example of an engineered feature in the context of learning how to classify the images of handwritten characters is a bit-density map constructed from the raw bit distribution data.</span></span> <span data-ttu-id="93c3f-108">Эта карта помогает более эффективно находить края символов, чем в случае необработанных данных о распределении.</span><span class="sxs-lookup"><span data-stu-id="93c3f-108">This map can help locate the edges of the characters more efficiently than the raw distribution.</span></span>

<span data-ttu-id="93c3f-109">Реконструированные и выбранные признаки повышают эффективность процесса обучения, который пытается извлечь ключевые сведения, содержащиеся в данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-109">Engineered and selected features increase the efficiency of the training process, which attempts to extract the key information contained in the data.</span></span> <span data-ttu-id="93c3f-110">Они также дают возможность повысить возможности этих моделей, чтобы точнее классифицировать входные данные и более надежно предсказывать нужные результаты.</span><span class="sxs-lookup"><span data-stu-id="93c3f-110">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span></span> <span data-ttu-id="93c3f-111">Реконструирование и выбор признаков можно также объединять, чтобы сделать процесс обучения более алгоритмизируемым.</span><span class="sxs-lookup"><span data-stu-id="93c3f-111">Feature engineering and selection can also combine to make the learning more computationally tractable.</span></span> <span data-ttu-id="93c3f-112">Этого можно достичь путем расширения и сокращения числа признаков, необходимых для калибровки или обучения модели.</span><span class="sxs-lookup"><span data-stu-id="93c3f-112">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span></span> <span data-ttu-id="93c3f-113">С точки зрения математики выбранные для обучения модели признаки являются минимальным набором независимых переменных, которые определяют структуры в данных и затем успешно прогнозируют результаты.</span><span class="sxs-lookup"><span data-stu-id="93c3f-113">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span></span>

<span data-ttu-id="93c3f-114">Реконструирование и выбор признаков является частью большего процесса, который обычно состоит из четырех этапов:</span><span class="sxs-lookup"><span data-stu-id="93c3f-114">The engineering and selection of features is one part of a larger process, which typically consists of four steps:</span></span>

* <span data-ttu-id="93c3f-115">Сбор данных</span><span class="sxs-lookup"><span data-stu-id="93c3f-115">Data collection</span></span>
* <span data-ttu-id="93c3f-116">совершенствование данных;</span><span class="sxs-lookup"><span data-stu-id="93c3f-116">Data enhancement</span></span>
* <span data-ttu-id="93c3f-117">построение модели;</span><span class="sxs-lookup"><span data-stu-id="93c3f-117">Model construction</span></span>
* <span data-ttu-id="93c3f-118">постобработка.</span><span class="sxs-lookup"><span data-stu-id="93c3f-118">Post-processing</span></span>

<span data-ttu-id="93c3f-119">Конструирование и выбор признаков относятся к этапу совершенствования данных в машинном обучении.</span><span class="sxs-lookup"><span data-stu-id="93c3f-119">Engineering and selection make up the data enhancement step of machine learning.</span></span> <span data-ttu-id="93c3f-120">Для наших целей можно выделить три следующих аспекта этого процесса.</span><span class="sxs-lookup"><span data-stu-id="93c3f-120">Three aspects of this process may be distinguished for our purposes:</span></span>

* <span data-ttu-id="93c3f-121">**Предварительная обработка данных**. Этот процесс должен гарантировать, что собраны чистые и согласованные данные.</span><span class="sxs-lookup"><span data-stu-id="93c3f-121">**Data pre-processing**: This process tries to ensure that the collected data is clean and consistent.</span></span> <span data-ttu-id="93c3f-122">Он предусматривает объединение наборов данных, обработку отсутствующих данных, обработку несогласованных данных и преобразование типов данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-122">It includes tasks such as integrating multiple data sets, handling missing data, handling inconsistent data, and converting data types.</span></span>
* <span data-ttu-id="93c3f-123">**Конструирование признаков**. Этот процесс направлен на создание дополнительных признаков на основе соответствующих существующих необработанных признаков и повышение эффективности прогнозирования алгоритма обучения.</span><span class="sxs-lookup"><span data-stu-id="93c3f-123">**Feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data and to increase predictive power to the learning algorithm.</span></span>
* <span data-ttu-id="93c3f-124">**Выбор признаков**. В этом процессе выбирается ключевое подмножество исходных признаков с целью сокращения размерности задачи обучения.</span><span class="sxs-lookup"><span data-stu-id="93c3f-124">**Feature selection**: This process selects the key subset of original data features to reduce the dimensionality of the training problem.</span></span>

<span data-ttu-id="93c3f-125">В этом разделе описываются только аспекты реконструирования признаков и выбора признаков в процессе совершенствования данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-125">This topic only covers the feature engineering and feature selection aspects of the data enhancement process.</span></span> <span data-ttu-id="93c3f-126">Дополнительную информацию об этапе предварительной обработки данных см. в видео [Preprocessing Data in Azure Machine Learning Studio](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/) (Предварительная обработка данных в студии машинного обучения Azure).</span><span class="sxs-lookup"><span data-stu-id="93c3f-126">For more information on the data pre-processing step, see [Pre-processing data in Azure Machine Learning Studio](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/).</span></span>

## <a name="creating-features-from-your-data--feature-engineering"></a><span data-ttu-id="93c3f-127">Создание признаков из данных. Конструирование признаков</span><span class="sxs-lookup"><span data-stu-id="93c3f-127">Creating features from your data--feature engineering</span></span>
<span data-ttu-id="93c3f-128">Обучающие данные образуют матрицу из примеров (записей или наблюдений, хранимых в строках), каждый из которых имеет набор признаков (переменных или полей, хранящихся в столбцах).</span><span class="sxs-lookup"><span data-stu-id="93c3f-128">The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</span></span> <span data-ttu-id="93c3f-129">Признаки, указанные в схеме эксперимента должны характеризовать закономерности в данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-129">The features specified in the experimental design are expected to characterize the patterns in the data.</span></span> <span data-ttu-id="93c3f-130">Несмотря на то что многие поля необработанных данных можно напрямую включить в набор выбранных признаков, используемых для обучения модели, часто для формирования усовершенствованного набора данных для обучения дополнительные сконструированные признаки требуется создавать из признаков в необработанных данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-130">Although many of the raw data fields can be directly included in the selected feature set used to train a model, additional engineered features often need to be constructed from the features in the raw data to generate an enhanced training data set.</span></span>

<span data-ttu-id="93c3f-131">Какие признаки нужно создавать для усовершенствования набора данных при обучении модели?</span><span class="sxs-lookup"><span data-stu-id="93c3f-131">What kind of features should be created to enhance the data set when training a model?</span></span> <span data-ttu-id="93c3f-132">Реконструированные признаки, совершенствующие обучение, содержат сведения, которые лучше выделяют закономерности в данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-132">Engineered features that enhance the training provide information that better differentiates the patterns in the data.</span></span> <span data-ttu-id="93c3f-133">Новые признаки должны предоставлять дополнительные сведения, нечетко зафиксированные или не очевидные в исходном или существующем наборе, но это довольно сложный процесс.</span><span class="sxs-lookup"><span data-stu-id="93c3f-133">You expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set, but this process is something of an art.</span></span> <span data-ttu-id="93c3f-134">Обоснованные и эффективные решения часто требуют определенного знания предметной области.</span><span class="sxs-lookup"><span data-stu-id="93c3f-134">Sound and productive decisions often require some domain expertise.</span></span>

<span data-ttu-id="93c3f-135">Начиная работу с машинным обучением Azure, этот процесс проще всего понять с помощью примеров, которые поставляются в комплекте со студией машинного обучения.</span><span class="sxs-lookup"><span data-stu-id="93c3f-135">When starting with Azure Machine Learning, it is easiest to grasp this process concretely by using samples provided in Machine Learning Studio.</span></span> <span data-ttu-id="93c3f-136">Здесь представлены два примера.</span><span class="sxs-lookup"><span data-stu-id="93c3f-136">Two examples are presented here:</span></span>

* <span data-ttu-id="93c3f-137">Пример регрессии ([прогнозирование количества сдаваемых напрокат велосипедов](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)) в контролируемом эксперименте с известными целевыми значениями.</span><span class="sxs-lookup"><span data-stu-id="93c3f-137">A regression example ([Prediction of the number of bike rentals](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)) in a supervised experiment where the target values are known</span></span>
* <span data-ttu-id="93c3f-138">Пример классификации интеллектуального анализа текста с использованием [хэширования признаков][feature-hashing].</span><span class="sxs-lookup"><span data-stu-id="93c3f-138">A text-mining classification example using [Feature Hashing][feature-hashing]</span></span>

### <a name="example-1-adding-temporal-features-for-a-regression-model"></a><span data-ttu-id="93c3f-139">Пример 1. Добавление временных признаков для регрессионной модели</span><span class="sxs-lookup"><span data-stu-id="93c3f-139">Example 1: Adding temporal features for a regression model</span></span>
<span data-ttu-id="93c3f-140">Воспользуемся экспериментом "Прогнозирование спроса на велосипеды" в Студии машинного обучения Azure, чтобы продемонстрировать реконструкцию признаков для задачи регрессии.</span><span class="sxs-lookup"><span data-stu-id="93c3f-140">To demonstrate how to engineer features for a regression task, let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio.</span></span> <span data-ttu-id="93c3f-141">Цель этого эксперимента — прогноз спроса на велосипеды, то есть количество сдаваемых напрокат велосипедов в конкретный месяц, день или час.</span><span class="sxs-lookup"><span data-stu-id="93c3f-141">The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month, day, or hour.</span></span> <span data-ttu-id="93c3f-142">**Набор данных по прокату велосипедов UCI** используется в качестве необработанных входных данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-142">The data set **Bike Rental UCI data set** is used as the raw input data.</span></span>

<span data-ttu-id="93c3f-143">Этот набор данных основывается на реальных данных компании Capital Bikeshare, которая содержит сеть проката велосипедов в городе Вашингтоне (США).</span><span class="sxs-lookup"><span data-stu-id="93c3f-143">This data set is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</span></span> <span data-ttu-id="93c3f-144">Набор данных представляет количество сдаваемых напрокат велосипедов в определенный час дня в 2011–2012 гг. и содержит 17 379 строк и 17 столбцов.</span><span class="sxs-lookup"><span data-stu-id="93c3f-144">The data set represents the number of bike rentals within a specific hour of a day, from 2011 to 2012, and it contains 17379 rows and 17 columns.</span></span> <span data-ttu-id="93c3f-145">Набор необработанных признаков содержит погодные условия (температура, влажность и скорость ветра) и тип дня (выходной или будний день).</span><span class="sxs-lookup"><span data-stu-id="93c3f-145">The raw feature set contains weather conditions (temperature, humidity, wind speed) and the type of the day (holiday or weekday).</span></span> <span data-ttu-id="93c3f-146">Поле для прогнозирования **cnt** — количество сдаваемых напрокат велосипедов в конкретный час, которое меняется в диапазоне от 1 до 977.</span><span class="sxs-lookup"><span data-stu-id="93c3f-146">The field to predict is **cnt**, a count that represents the bike rentals within a specific hour and that ranges from 1 to 977.</span></span>

<span data-ttu-id="93c3f-147">Для создания эффективных признаков в обучающих данных строятся четыре регрессионные модели с использованием одного и того же алгоритма, но с четырьмя разными наборами данных для обучения.</span><span class="sxs-lookup"><span data-stu-id="93c3f-147">To construct effective features in the training data, four regression models are built by using the same algorithm, but with four different training data sets.</span></span> <span data-ttu-id="93c3f-148">Четыре набора данных представляют одни и те же необработанные входные данные, но с увеличивающимся числом набора признаков.</span><span class="sxs-lookup"><span data-stu-id="93c3f-148">The four data sets represent the same raw input data, but with an increasing number of features set.</span></span> <span data-ttu-id="93c3f-149">Эти признаки сгруппированы в четыре категории:</span><span class="sxs-lookup"><span data-stu-id="93c3f-149">These features are grouped into four categories:</span></span>

1. <span data-ttu-id="93c3f-150">А = «погода» + «праздник» + «рабочий день» + «выходной день» для прогнозируемого дня</span><span class="sxs-lookup"><span data-stu-id="93c3f-150">A = weather + holiday + weekday + weekend features for the predicted day</span></span>
2. <span data-ttu-id="93c3f-151">Б = количество велосипедов, которые сданы напрокат в каждый из предыдущих 12 часов</span><span class="sxs-lookup"><span data-stu-id="93c3f-151">B = number of bikes that were rented in each of the previous 12 hours</span></span>
3. <span data-ttu-id="93c3f-152">В = количество велосипедов, которые были сданы напрокат в каждый из предыдущих 12 дней в течение одного и того же часа</span><span class="sxs-lookup"><span data-stu-id="93c3f-152">C = number of bikes that were rented in each of the previous 12 days at the same hour</span></span>
4. <span data-ttu-id="93c3f-153">Г = количество велосипедов, которые были сданы напрокат в каждую из предыдущих 12 недель в течение одного и того же часа и дня</span><span class="sxs-lookup"><span data-stu-id="93c3f-153">D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</span></span>

<span data-ttu-id="93c3f-154">Помимо набора признаков А, который уже существует в исходных необработанных данных, другие три набора признаков создаются в процессе реконструирования признаков.</span><span class="sxs-lookup"><span data-stu-id="93c3f-154">Besides feature set A, which already exists in the original raw data, the other three sets of features are created through the feature engineering process.</span></span> <span data-ttu-id="93c3f-155">Набор признаков Б охватывает новейший спрос на велосипеды.</span><span class="sxs-lookup"><span data-stu-id="93c3f-155">Feature set B captures the recent demand for the bikes.</span></span> <span data-ttu-id="93c3f-156">Набор признак В спрос на велосипеды в конкретный час.</span><span class="sxs-lookup"><span data-stu-id="93c3f-156">Feature set C captures the demand for bikes at a particular hour.</span></span> <span data-ttu-id="93c3f-157">Набор признаков Г охватывает спрос на велосипеды в определенный час и определенный день недели.</span><span class="sxs-lookup"><span data-stu-id="93c3f-157">Feature set D captures demand for bikes at particular hour and particular day of the week.</span></span> <span data-ttu-id="93c3f-158">Каждый из четырех наборов данных для обучения содержит наборы признаков А, А + Б, А + Б + В и A + Б + В + Г соответственно.</span><span class="sxs-lookup"><span data-stu-id="93c3f-158">Each of the four training data sets includes feature sets A, A+B, A+B+C, and A+B+C+D, respectively.</span></span>

<span data-ttu-id="93c3f-159">В эксперименте машинного обучения Azure эти четыре набора данных для обучения формируются через четыре ветви предварительно обработанного входного набора данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-159">In the Azure Machine Learning experiment, these four training data sets are formed via four branches from the pre-processed input data set.</span></span> <span data-ttu-id="93c3f-160">За исключением крайней левой ветви, каждая из этих ветвей содержит модуль [Выполнить сценарий R][execute-r-script], в котором набор производных признаков (набор признаков Б, В и Г) соответственно конструируется и добавляется в импортируемый набор данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-160">Except for the leftmost branch, each of these branches contains an [Execute R Script][execute-r-script] module in which a set of derived features (feature sets B, C, and D) is respectively constructed and appended to the imported data set.</span></span> <span data-ttu-id="93c3f-161">На следующем рисунке показан сценарий R, используемый для создания набора признаков Б во второй слева ветви.</span><span class="sxs-lookup"><span data-stu-id="93c3f-161">The following figure demonstrates the R script used to create feature set B in the second left branch.</span></span>

![Создание набора признаков](./media/machine-learning-feature-selection-and-engineering/addFeature-Rscripts.png)

<span data-ttu-id="93c3f-163">Сравнение результатов производительности четырех моделей приведено в таблице ниже.</span><span class="sxs-lookup"><span data-stu-id="93c3f-163">The following table summarizes the comparison of the performance results of the four models.</span></span> <span data-ttu-id="93c3f-164">Наилучшие результаты даются набором признаков А + Б + В.</span><span class="sxs-lookup"><span data-stu-id="93c3f-164">The best results are shown by features A+B+C.</span></span> <span data-ttu-id="93c3f-165">Обратите внимание, что частота ошибок уменьшается, когда в обучающие данные включается дополнительный набор признаков.</span><span class="sxs-lookup"><span data-stu-id="93c3f-165">Note that the error rate decreases when additional feature sets are included in the training data.</span></span> <span data-ttu-id="93c3f-166">Это подтверждает наше предположение, что наборы признаков Б и В предоставляют дополнительные данные для задачи регрессии.</span><span class="sxs-lookup"><span data-stu-id="93c3f-166">This verifies our presumption that the feature sets B and C provide additional relevant information for the regression task.</span></span> <span data-ttu-id="93c3f-167">Добавление набора признаков Г не дает дополнительного сокращения частоты ошибок.</span><span class="sxs-lookup"><span data-stu-id="93c3f-167">Adding the D feature set does not seem to provide any additional reduction in the error rate.</span></span>

![Сравнение результатов производительности](./media/machine-learning-feature-selection-and-engineering/result1.png)

### <span data-ttu-id="93c3f-169"><a name="example2"></a> Пример 2. Создание признаков в интеллектуальном анализе текста</span><span class="sxs-lookup"><span data-stu-id="93c3f-169"><a name="example2"></a> Example 2: Creating features in text mining</span></span>
<span data-ttu-id="93c3f-170">Реконструирование признаков широко применяется в задачах, связанных с интеллектуальным анализом текста, например классификации документов и анализе тональностей.</span><span class="sxs-lookup"><span data-stu-id="93c3f-170">Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</span></span> <span data-ttu-id="93c3f-171">Например, если вы хотите классифицировать документы по нескольким категориям, типичным предположением является следующее: слова или фразы, которые встречаются в одной категории документов, с меньшей вероятностью встречаются в другой категории.</span><span class="sxs-lookup"><span data-stu-id="93c3f-171">For example, when you want to classify documents into several categories, a typical assumption is that the words or phrases included in one document category are less likely to occur in another document category.</span></span> <span data-ttu-id="93c3f-172">Иными словами, частота распределения слов или фраз может характеризовать разные категории документов.</span><span class="sxs-lookup"><span data-stu-id="93c3f-172">In other words, the frequency of the word or phrase distribution is able to characterize different document categories.</span></span> <span data-ttu-id="93c3f-173">В приложениях интеллектуального анализа текста отдельные части текстового содержимого обычно служат в качестве входных данных, поэтому для создания признаков, связанных с частотой слова или фразы, необходим процесс конструирования признаков.</span><span class="sxs-lookup"><span data-stu-id="93c3f-173">In text mining applications, the feature engineering process is needed to create the features involving word or phrase frequencies because individual pieces of text-contents usually serve as the input data.</span></span>

<span data-ttu-id="93c3f-174">Для выполнения этой задачи вызывается метод *хэширования признаков*, чтобы эффективно превратить произвольные признаки текста в индексы.</span><span class="sxs-lookup"><span data-stu-id="93c3f-174">To achieve this task, a technique called *feature hashing* is applied to efficiently turn arbitrary text features into indices.</span></span> <span data-ttu-id="93c3f-175">Вместо того чтобы сопоставлять каждый признак текста (слова или фразы) для определенного индекса, этот метод применяет хэш-функции к признакам и непосредственно использует их хэш-значения как индексы.</span><span class="sxs-lookup"><span data-stu-id="93c3f-175">Instead of associating each text feature (words or phrases) to a particular index, this method functions by applying a hash function to the features and by using their hash values as indices directly.</span></span>

<span data-ttu-id="93c3f-176">В Машинном обучении Azure есть модуль [Feature Hashing][feature-hashing] (Хэширование признаков), с помощью которого можно создавать признаки этих слов или фраз.</span><span class="sxs-lookup"><span data-stu-id="93c3f-176">In Azure Machine Learning, there is a [Feature Hashing][feature-hashing] module that creates these word or phrase features.</span></span> <span data-ttu-id="93c3f-177">На рисунке ниже показан пример использования этого модуля.</span><span class="sxs-lookup"><span data-stu-id="93c3f-177">The following figure shows an example of using this module.</span></span> <span data-ttu-id="93c3f-178">Входной набор данных содержит два столбца: рейтинг книги от 1 до 5 и содержимое фактической рецензии.</span><span class="sxs-lookup"><span data-stu-id="93c3f-178">The input data set contains two columns: the book rating ranging from 1 to 5 and the actual review content.</span></span> <span data-ttu-id="93c3f-179">Задача этого модуля [хэширования признаков][feature-hashing] состоит в извлечении новых признаков, чтобы показать частоту вхождения соответствующих слов или фраз в рецензии на определенную книгу.</span><span class="sxs-lookup"><span data-stu-id="93c3f-179">The goal of this [Feature Hashing][feature-hashing] module is to retrieve new features that show the occurrence frequency of the corresponding words or phrases within the particular book review.</span></span> <span data-ttu-id="93c3f-180">Чтобы использовать этот модуль, необходимо выполнить такие действия.</span><span class="sxs-lookup"><span data-stu-id="93c3f-180">To use this module, you need to complete the following steps:</span></span>

1. <span data-ttu-id="93c3f-181">Выберите столбец, содержащий входной текст (в этом примере — **Col2**).</span><span class="sxs-lookup"><span data-stu-id="93c3f-181">Select the column that contains the input text (**Col2** in this example).</span></span>
2. <span data-ttu-id="93c3f-182">В поле *Hashing bitsize* (Размер бита хэширования) задайте значение 8, означающее, что создается 2^8 = 256 признаков.</span><span class="sxs-lookup"><span data-stu-id="93c3f-182">Set *Hashing bitsize* to 8, which means 2^8=256 features are created.</span></span> <span data-ttu-id="93c3f-183">Слово или фраза в тексте будут хэшированы по 256 индексам.</span><span class="sxs-lookup"><span data-stu-id="93c3f-183">The word or phrase in the text is then hashed to 256 indices.</span></span> <span data-ttu-id="93c3f-184">Параметр *Hashing bitsize* меняется в диапазоне от 1 до 31.</span><span class="sxs-lookup"><span data-stu-id="93c3f-184">The parameter *Hashing bitsize* ranges from 1 to 31.</span></span> <span data-ttu-id="93c3f-185">Если для этого параметра установлено большее значение, менее вероятно, что слова или фразы будут хэшироваться по тому же индексу.</span><span class="sxs-lookup"><span data-stu-id="93c3f-185">If the parameter is set to a larger number, the words or phrases are less likely to be hashed into the same index.</span></span>
3. <span data-ttu-id="93c3f-186">Задайте для параметра *N-grams* значение 2.</span><span class="sxs-lookup"><span data-stu-id="93c3f-186">Set the parameter *N-grams* to 2.</span></span> <span data-ttu-id="93c3f-187">Этот параметр возвращает частоту вхождения униграмм (признаков для каждого отдельного слова) и биграмм (признаков для каждой пары смежных слов) во входном тексте.</span><span class="sxs-lookup"><span data-stu-id="93c3f-187">This retrieves the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</span></span> <span data-ttu-id="93c3f-188">Значение параметра *N-grams* меняется в диапазоне от 0 до 10, определяя максимальное количество последовательно идущих слов, которые включены в признак.</span><span class="sxs-lookup"><span data-stu-id="93c3f-188">The parameter *N-grams* ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</span></span>  

![Модуль хэширования признаков](./media/machine-learning-feature-selection-and-engineering/feature-Hashing1.png)

<span data-ttu-id="93c3f-190">На следующем рисунке показано, на что похожи эти новые признаки.</span><span class="sxs-lookup"><span data-stu-id="93c3f-190">The following figure shows what these new features look like.</span></span>

![Пример хэширования признаков](./media/machine-learning-feature-selection-and-engineering/feature-Hashing2.png)

## <a name="filtering-features-from-your-data--feature-selection"></a><span data-ttu-id="93c3f-192">Фильтрация признаков в данных. Выбор признаков</span><span class="sxs-lookup"><span data-stu-id="93c3f-192">Filtering features from your data--feature selection</span></span>
<span data-ttu-id="93c3f-193">*Выбор признаков* — это процесс, с помощью которого часто создаются наборы данных для обучения для задач прогнозирующего моделирования, например задач классификации или регрессии.</span><span class="sxs-lookup"><span data-stu-id="93c3f-193">*Feature selection* is a process that is commonly applied to the construction of training data sets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="93c3f-194">Целью этого является выбор подмножества признаков из исходного набора данных для уменьшения его размеров с помощью минимального набора признаков для представления максимального отклонения в данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-194">The goal is to select a subset of the features from the original data set that reduces its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</span></span> <span data-ttu-id="93c3f-195">Это подмножество признаков содержит только те признаки, которые должны быть включены в обучение модели.</span><span class="sxs-lookup"><span data-stu-id="93c3f-195">This subset of features contains the only features to be included to train the model.</span></span> <span data-ttu-id="93c3f-196">Выбор признаков служит двум основным целям.</span><span class="sxs-lookup"><span data-stu-id="93c3f-196">Feature selection serves two main purposes:</span></span>

* <span data-ttu-id="93c3f-197">Выбор признаков часто повышает точность классификации, исключая несоответствующие, избыточные или сильно коррелирующие признаки.</span><span class="sxs-lookup"><span data-stu-id="93c3f-197">Feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="93c3f-198">Выбор признаков сокращает число признаков, что повышает эффективность процесса обучения модели.</span><span class="sxs-lookup"><span data-stu-id="93c3f-198">Feature selection decreases the number of features, which makes the model training process more efficient.</span></span> <span data-ttu-id="93c3f-199">Это особенно важно для ученик, затратных в плане обучения, таких как вспомогательные векторные машины.</span><span class="sxs-lookup"><span data-stu-id="93c3f-199">This is particularly important for learners that are expensive to train such as support vector machines.</span></span>

<span data-ttu-id="93c3f-200">Несмотря на то что выбор признаков нацелен на сокращение числа признаков в наборе данных, используемых для обучения модели, он зачастую не связан с термином *сокращение размерности*.</span><span class="sxs-lookup"><span data-stu-id="93c3f-200">Although feature selection seeks to reduce the number of features in the data set used to train the model, it is not usually referred to by the term *dimensionality reduction.*</span></span> <span data-ttu-id="93c3f-201">Методы выбора признаков извлекают поднабор из исходных признаков в данных без каких-либо изменений.</span><span class="sxs-lookup"><span data-stu-id="93c3f-201">Feature selection methods extract a subset of original features in the data without changing them.</span></span>  <span data-ttu-id="93c3f-202">Методы сокращения размерности используют реконструированные признаки, которые могут преобразовывать исходные признаки и соответственно изменять их.</span><span class="sxs-lookup"><span data-stu-id="93c3f-202">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</span></span> <span data-ttu-id="93c3f-203">Примеры методов сокращения размерности: анализ главных компонентов, анализ канонических корреляций и сингулярная декомпозиция.</span><span class="sxs-lookup"><span data-stu-id="93c3f-203">Examples of dimensionality reduction methods include principal component analysis, canonical correlation analysis, and singular value decomposition.</span></span>

<span data-ttu-id="93c3f-204">Одна из распространенных категорий методов выбора признаков в контролируемом контексте называется "Выбор признаков на основе фильтра".</span><span class="sxs-lookup"><span data-stu-id="93c3f-204">One widely applied category of feature selection methods in a supervised context is filter-based feature selection.</span></span> <span data-ttu-id="93c3f-205">Эти методы применяют статистическую меру для назначения рейтинга каждому признаку путем вычисления корреляции между каждым признаком и целевым атрибутом.</span><span class="sxs-lookup"><span data-stu-id="93c3f-205">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</span></span> <span data-ttu-id="93c3f-206">Затем признаки ранжируются по рейтингу, который может использоваться, чтобы задать пороговое значение для сохранения или исключения конкретных признаков.</span><span class="sxs-lookup"><span data-stu-id="93c3f-206">The features are then ranked by the score, which you can use to set the threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="93c3f-207">Примеры статистических показателей, используемых в этих методах: корреляция Пирсона, взаимная информация и критерий хи-квадрат.</span><span class="sxs-lookup"><span data-stu-id="93c3f-207">Examples of the statistical measures used in these methods include Pearson Correlation, mutual information, and the Chi-squared test.</span></span>

<span data-ttu-id="93c3f-208">В студии машинного обучения Azure предусмотрены модули для выбора признаков.</span><span class="sxs-lookup"><span data-stu-id="93c3f-208">Azure Machine Learning Studio provides modules for feature selection.</span></span> <span data-ttu-id="93c3f-209">Как показано на следующем рисунке, это модули [Filter-Based Feature Selection][filter-based-feature-selection] (Выбор признаков на основе фильтра) и [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis] (Линейный дискриминант Фишера).</span><span class="sxs-lookup"><span data-stu-id="93c3f-209">As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![Пример выбора признаков](./media/machine-learning-feature-selection-and-engineering/feature-Selection.png)

<span data-ttu-id="93c3f-211">Например, используйте модуль [Filter-Based Feature Selection][filter-based-feature-selection] (Выбор признаков на основе фильтра) с примером интеллектуального анализа текста, описанным выше.</span><span class="sxs-lookup"><span data-stu-id="93c3f-211">For example, use the [Filter-Based Feature Selection][filter-based-feature-selection] module with the text mining example outlined previously.</span></span> <span data-ttu-id="93c3f-212">Допустим, вам необходимо построить регрессионную модель после создания набора из 256 признаков с помощью модуля [Feature Hashing][feature-hashing] (Хэширование признаков). При этом выходная переменная находится в столбце **Col1** и представляет собой рейтинг рецензий на книгу в диапазоне от 1 до 5.</span><span class="sxs-lookup"><span data-stu-id="93c3f-212">Assume that you want to build a regression model after a set of 256 features is created through the [Feature Hashing][feature-hashing] module, and that the response variable is **Col1** and represents a book review rating ranging from 1 to 5.</span></span> <span data-ttu-id="93c3f-213">Установите для параметра **Feature scoring method** (Метод оценки признаков) значение **Pearson Correlation** (Корреляция Пирсона), для параметра **Target column** (Целевой столбец) — **Col1**, а для параметра **Number of desired features** (Число необходимых признаков) — значение **50**.</span><span class="sxs-lookup"><span data-stu-id="93c3f-213">Set **Feature scoring method** to **Pearson Correlation**, **Target column** to **Col1**, and **Number of desired features** to **50**.</span></span> <span data-ttu-id="93c3f-214">Затем модуль [Filter-Based Feature Selection][filter-based-feature-selection] (Выбор признаков на основе фильтра) создает набор данных, содержащий 50 признаков с целевым атрибутом **Col1**.</span><span class="sxs-lookup"><span data-stu-id="93c3f-214">The module [Filter-Based Feature Selection][filter-based-feature-selection] then produces a data set containing 50 features together with the target attribute **Col1**.</span></span> <span data-ttu-id="93c3f-215">На следующем рисунке показан процесс этого эксперимента и входные параметры.</span><span class="sxs-lookup"><span data-stu-id="93c3f-215">The following figure shows the flow of this experiment and the input parameters.</span></span>

![Пример выбора признаков](./media/machine-learning-feature-selection-and-engineering/feature-Selection1.png)

<span data-ttu-id="93c3f-217">На следующем рисунке показаны итоговые наборы данных.</span><span class="sxs-lookup"><span data-stu-id="93c3f-217">The following figure shows the resulting data sets.</span></span> <span data-ttu-id="93c3f-218">Каждый признак оценивается на основе корреляции Пирсона между самим признаком и целевым атрибутом **Col1**.</span><span class="sxs-lookup"><span data-stu-id="93c3f-218">Each feature is scored based on the Pearson Correlation between itself and the target attribute **Col1**.</span></span> <span data-ttu-id="93c3f-219">Сохраняются признаки с наибольшей оценкой.</span><span class="sxs-lookup"><span data-stu-id="93c3f-219">The features with top scores are kept.</span></span>

![Наборы данных для выбора признаков на основе фильтра](./media/machine-learning-feature-selection-and-engineering/feature-Selection2.png)

<span data-ttu-id="93c3f-221">На следующем рисунке показаны соответствующие оценки для выбранных признаков.</span><span class="sxs-lookup"><span data-stu-id="93c3f-221">The following figure shows the corresponding scores of the selected features.</span></span>

![Оценки выбранных признаков](./media/machine-learning-feature-selection-and-engineering/feature-Selection3.png)

<span data-ttu-id="93c3f-223">Применяя этот модуль [Filter-Based Feature Selection][filter-based-feature-selection] (Выбор признаков на основе фильтра), будут выбраны 50 из 256 признаков, поскольку эти признаки максимально коррелируют с целевой переменной **Col1** на основе метода оценки **Корреляция Пирсона**.</span><span class="sxs-lookup"><span data-stu-id="93c3f-223">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most features correlated with the target variable **Col1** based on the scoring method **Pearson Correlation**.</span></span>

## <a name="conclusion"></a><span data-ttu-id="93c3f-224">Заключение</span><span class="sxs-lookup"><span data-stu-id="93c3f-224">Conclusion</span></span>
<span data-ttu-id="93c3f-225">Конструирование и выбор признаков — два часто выполняемых действия по подготовке обучающих данных при построении модели машинного обучения.</span><span class="sxs-lookup"><span data-stu-id="93c3f-225">Feature engineering and feature selection are two steps commonly performed to prepare the training data when building a machine learning model.</span></span> <span data-ttu-id="93c3f-226">Как правило, сначала применяется конструирование признаков для создания дополнительных признаков, а затем выполняется выбор признаков, чтобы исключить несоответствующие, избыточные или сильно коррелирующие признаки.</span><span class="sxs-lookup"><span data-stu-id="93c3f-226">Normally, feature engineering is applied first to generate additional features, and then the feature selection step is performed to eliminate irrelevant, redundant, or highly correlated features.</span></span>

<span data-ttu-id="93c3f-227">Не всегда обязательно выполнять реконструирование или выбор признаков.</span><span class="sxs-lookup"><span data-stu-id="93c3f-227">It is not always necessarily to perform feature engineering or feature selection.</span></span> <span data-ttu-id="93c3f-228">Необходимость в них зависит от существующих или собираемых данных, используемого алгоритма и цели эксперимента.</span><span class="sxs-lookup"><span data-stu-id="93c3f-228">Whether it is needed depends on the data you have or collect, the algorithm you pick, and the objective of the experiment.</span></span>

<!-- Module References -->
[execute-r-script]: https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/
