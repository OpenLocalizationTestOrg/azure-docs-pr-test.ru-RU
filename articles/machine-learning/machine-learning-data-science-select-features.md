---
title: "Выбор признаков в процессе обработки и анализа данных группы | Документация Майкрософт"
description: "Описывает цели реконструирования характеристик и содержит примеры, поясняющие его роль в совершенствовании данных в процессе машинного обучения."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 878541f5-1df8-4368-889a-ced6852aba47
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/24/2017
ms.author: zhangya;bradsev
ms.openlocfilehash: ab97ee8278be567ff46d9b0f762d3c5c6cafa412
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/11/2017
---
# <a name="feature-selection-in-the-team-data-science-process-tdsp"></a><span data-ttu-id="f581a-103">Выбор характеристик в процессе обработки и анализа данных группы (TDSP)</span><span class="sxs-lookup"><span data-stu-id="f581a-103">Feature selection in the Team Data Science Process (TDSP)</span></span>
<span data-ttu-id="f581a-104">В этой статье описаны цели выбора характеристик и приводятся примеры, поясняющие его роль в совершенствовании данных в процессе машинного обучения.</span><span class="sxs-lookup"><span data-stu-id="f581a-104">This article explains the purposes of feature selection and provides examples of its role in the data enhancement process of machine learning.</span></span> <span data-ttu-id="f581a-105">Эти примеры взяты из Студии машинного обучения Azure.</span><span class="sxs-lookup"><span data-stu-id="f581a-105">These examples are drawn from Azure Machine Learning Studio.</span></span> 

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="f581a-106">Проектирование и выбор признаков являются частью процесса TDSP, описанного в статье [Жизненный цикл процесса обработки и анализа данных группы](data-science-process-overview.md).</span><span class="sxs-lookup"><span data-stu-id="f581a-106">The engineering and selection of features is one part of the Team Data Science Process (TDSP) outlined in [What is the Team Data Science Process?](data-science-process-overview.md).</span></span> <span data-ttu-id="f581a-107">Проектирование и выбор характеристик входят в этап **разработки характеристик** процесса TDSP.</span><span class="sxs-lookup"><span data-stu-id="f581a-107">Feature engineering and selection are parts of the **Develop features** step of the TDSP.</span></span>

* <span data-ttu-id="f581a-108">**Реконструирование признаков**: этот процесс направлен на создание дополнительных признаков на основе соответствующих существующих необработанных признаков и повышение эффективности прогнозирования алгоритма обучения.</span><span class="sxs-lookup"><span data-stu-id="f581a-108">**feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data, and to increase predictive power to the learning algorithm.</span></span>
* <span data-ttu-id="f581a-109">**Выбор признаков**: в этом процессе выбирается ключевое подмножество исходных признаков с целью сокращения размерности задачи обучения.</span><span class="sxs-lookup"><span data-stu-id="f581a-109">**feature selection**: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem.</span></span>

<span data-ttu-id="f581a-110">Как правило, **проектирование признаков** сначала применяется для создания дополнительных признаков, а затем выполняется на этапе **выбора признаков**, чтобы исключить несоответствующие, избыточные или сильно коррелирующие признаки.</span><span class="sxs-lookup"><span data-stu-id="f581a-110">Normally **feature engineering** is applied first to generate additional features, and then the **feature selection** step is performed to eliminate irrelevant, redundant, or highly correlated features.</span></span>

## <a name="filtering-features-from-your-data---feature-selection"></a><span data-ttu-id="f581a-111">Фильтрация признаков в данных. Выбор признаков</span><span class="sxs-lookup"><span data-stu-id="f581a-111">Filtering Features from Your Data - Feature Selection</span></span>
<span data-ttu-id="f581a-112">Выбор признаков — это процесс, который часто применяется для создания обучающих наборов данных для задач прогнозирующего моделирования, например задач классификации или регрессии.</span><span class="sxs-lookup"><span data-stu-id="f581a-112">Feature selection is a process that is commonly applied for the construction of training datasets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="f581a-113">Целью этого является выбор подмножества признаков из исходного набора данных, уменьшение его размеров с помощью минимального набора признаков для представления максимального отклонения в данных.</span><span class="sxs-lookup"><span data-stu-id="f581a-113">The goal is to select a subset of the features from the original dataset that reduce its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</span></span> <span data-ttu-id="f581a-114">Это подмножество признаков является именно тем подмножеством, которое должно быть включено в обучение модели.</span><span class="sxs-lookup"><span data-stu-id="f581a-114">This subset of features are, then, the only features to be included to train the model.</span></span> <span data-ttu-id="f581a-115">Выбор признаков служит двум основным целям.</span><span class="sxs-lookup"><span data-stu-id="f581a-115">Feature selection serves two main purposes.</span></span>

* <span data-ttu-id="f581a-116">Во-первых, выбор признаков часто повышает точность классификации, исключая несоответствующие, избыточные или сильно коррелирующие признаки.</span><span class="sxs-lookup"><span data-stu-id="f581a-116">First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="f581a-117">Во-вторых, он сокращает число признаков, что повышает эффективность процесса обучения модели.</span><span class="sxs-lookup"><span data-stu-id="f581a-117">Second, it decreases the number of features which makes model training process more efficient.</span></span> <span data-ttu-id="f581a-118">Это особенно важно для ученик, затратных в плане обучения, таких как вспомогательные векторные машины.</span><span class="sxs-lookup"><span data-stu-id="f581a-118">This is particularly important for learners that are expensive to train such as support vector machines.</span></span>

<span data-ttu-id="f581a-119">Несмотря на то, что выбор признаков нацелен на сокращение числа признаков в наборе данных, используемых для обучения модели, он зачастую не связан с термином «сокращение размерности».</span><span class="sxs-lookup"><span data-stu-id="f581a-119">Although feature selection does seek to reduce the number of features in the dataset used to train the model, it is not usually referred to by the term "dimensionality reduction".</span></span> <span data-ttu-id="f581a-120">Методы выбора признаков извлекают поднабор из исходных признаков в данных без каких-либо изменений.</span><span class="sxs-lookup"><span data-stu-id="f581a-120">Feature selection methods extract a subset of original features in the data without changing them.</span></span>  <span data-ttu-id="f581a-121">Методы сокращения размерности используют реконструированные признаки, которые могут преобразовывать исходные признаки и соответственно изменять их.</span><span class="sxs-lookup"><span data-stu-id="f581a-121">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</span></span> <span data-ttu-id="f581a-122">Примеры методов сокращения размерности: анализ главных компонентов, анализ канонических корреляций и сингулярная декомпозиция.</span><span class="sxs-lookup"><span data-stu-id="f581a-122">Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</span></span>

<span data-ttu-id="f581a-123">В частности одна из распространенных категорий методов выбора признаков в контролируемом контексте называется «Выбор признаков на основе фильтра».</span><span class="sxs-lookup"><span data-stu-id="f581a-123">Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</span></span> <span data-ttu-id="f581a-124">Эти методы применяют статистическую меру для назначения рейтинга каждому признаку путем вычисления корреляции между каждым признаком и целевым атрибутом.</span><span class="sxs-lookup"><span data-stu-id="f581a-124">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</span></span> <span data-ttu-id="f581a-125">Затем признаки ранжируются по рейтингу, который может использоваться, чтобы задать пороговое значение для сохранения или исключения конкретных признаков.</span><span class="sxs-lookup"><span data-stu-id="f581a-125">The features are then ranked by the score, which may be used to help set the threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="f581a-126">Примеры статистических показателей, используемых в этих методах: корреляция Пирсона, взаимная информация и критерий хи-квадрат.</span><span class="sxs-lookup"><span data-stu-id="f581a-126">Examples of the statistical measures used in these methods include Person correlation, mutual information, and the Chi squared test.</span></span>

<span data-ttu-id="f581a-127">В Студии машинного обучения Azure предусмотрены модули для выбора признаков.</span><span class="sxs-lookup"><span data-stu-id="f581a-127">In Azure Machine Learning Studio, there are modules provided for feature selection.</span></span> <span data-ttu-id="f581a-128">Как показано на следующем рисунке, это модули [Filter-Based Feature Selection][filter-based-feature-selection] (Выбор признаков на основе фильтра) и [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis] (Линейный дискриминант Фишера).</span><span class="sxs-lookup"><span data-stu-id="f581a-128">As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![Пример выбора признаков](./media/machine-learning-data-science-select-features/feature-Selection.png)

<span data-ttu-id="f581a-130">Рассмотрим, например, использование модуля [Filter-Based Feature Selection][filter-based-feature-selection] (Выбор признаков на основе фильтра).</span><span class="sxs-lookup"><span data-stu-id="f581a-130">Consider, for example, the use of the [Filter-Based Feature Selection][filter-based-feature-selection] module.</span></span> <span data-ttu-id="f581a-131">Для удобства мы будем продолжать использовать описанный выше пример интеллектуального анализа текста.</span><span class="sxs-lookup"><span data-stu-id="f581a-131">For the purpose of convenience, we continue to use the text mining example outlined above.</span></span> <span data-ttu-id="f581a-132">Допустим, что мы хотим создать регрессионную модель после создания набора из 256 признаков с помощью модуля [Feature Hashing][feature-hashing] (Хэширование признаков) и что выходная переменная находится в столбце Col1 и представляет собой рейтинги рецензий на книги в диапазоне от 1 до 5.</span><span class="sxs-lookup"><span data-stu-id="f581a-132">Assume that we want to build a regression model after a set of 256 features are created through the [Feature Hashing][feature-hashing] module, and that the response variable is the "Col1" and represents a book review ratings ranging from 1 to 5.</span></span> <span data-ttu-id="f581a-133">Установив для параметра «Метод количественной оценки» значение «Корреляция Пирсона», параметра «Целевой столбец» — «Col1» и для параметра «Число необходимых признаков» — значение 50.</span><span class="sxs-lookup"><span data-stu-id="f581a-133">By setting "Feature scoring method" to be "Pearson Correlation", the "Target column" to be "Col1", and the "Number of desired features" to 50.</span></span> <span data-ttu-id="f581a-134">Затем модуль [Filter-Based Feature Selection][filter-based-feature-selection] (Выбор признаков на основе фильтра) создаст набор данных, содержащий 50 признаков с целевым атрибутом Col1.</span><span class="sxs-lookup"><span data-stu-id="f581a-134">Then the module [Filter-Based Feature Selection][filter-based-feature-selection] will produce a dataset containing 50 features together with the target attribute "Col1".</span></span> <span data-ttu-id="f581a-135">На следующем рисунке показан только что описанный процесс этого эксперимента и входные параметры.</span><span class="sxs-lookup"><span data-stu-id="f581a-135">The following figure shows the flow of this experiment and the input parameters we just described.</span></span>

![Пример выбора признаков](./media/machine-learning-data-science-select-features/feature-Selection1.png)

<span data-ttu-id="f581a-137">На следующем рисунке показаны результирующие наборы данных.</span><span class="sxs-lookup"><span data-stu-id="f581a-137">The following figure shows the resulting datasets.</span></span> <span data-ttu-id="f581a-138">Каждый признак оценивается на основе корреляции Пирсона между ним и целевым атрибутом «Col1».</span><span class="sxs-lookup"><span data-stu-id="f581a-138">Each feature is scored based on the Pearson Correlation between itself and the target attribute "Col1".</span></span> <span data-ttu-id="f581a-139">Сохраняются признаки с наибольшей оценкой.</span><span class="sxs-lookup"><span data-stu-id="f581a-139">The features with top scores are kept.</span></span>

![Пример выбора признаков](./media/machine-learning-data-science-select-features/feature-Selection2.png)

<span data-ttu-id="f581a-141">На следующем рисунке показаны соответствующие оценки для выбранных признаков.</span><span class="sxs-lookup"><span data-stu-id="f581a-141">The corresponding scores of the selected features are shown in the following figure.</span></span>

![Пример выбора признаков](./media/machine-learning-data-science-select-features/feature-Selection3.png)

<span data-ttu-id="f581a-143">Применяя модуль [Filter-Based Feature Selection][filter-based-feature-selection] (Выбор признаков на основе фильтра), будут выбраны 50 из 256 признаков, поскольку эти признаки максимально коррелируют с целевой переменной Col1 на основе метода оценки "Корреляция Пирсона".</span><span class="sxs-lookup"><span data-stu-id="f581a-143">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most correlated features with the target variable "Col1", based on the scoring method "Pearson Correlation".</span></span>

## <a name="conclusion"></a><span data-ttu-id="f581a-144">Заключение</span><span class="sxs-lookup"><span data-stu-id="f581a-144">Conclusion</span></span>
<span data-ttu-id="f581a-145">Реконструирование характеристик и выбор характеристик — два часто выполняемых действия. Реконструированные и выбранные характеристики повышают эффективность процесса обучения, который пытается извлечь ключевые сведения, содержащиеся в данных.</span><span class="sxs-lookup"><span data-stu-id="f581a-145">Feature engineering and feature selection are two commonly Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data.</span></span> <span data-ttu-id="f581a-146">Они также дают возможность повысить возможности этих моделей, чтобы точнее классифицировать входные данные и более надежно предсказывать нужные результаты.</span><span class="sxs-lookup"><span data-stu-id="f581a-146">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span></span> <span data-ttu-id="f581a-147">Реконструирование и выбор признаков можно также объединять, чтобы сделать процесс обучения более алгоритмизируемым.</span><span class="sxs-lookup"><span data-stu-id="f581a-147">Feature engineering and selection can also combine to make the learning more computationally tractable.</span></span> <span data-ttu-id="f581a-148">Этого можно достичь путем расширения и сокращения числа признаков, необходимых для калибровки или обучения модели.</span><span class="sxs-lookup"><span data-stu-id="f581a-148">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span></span> <span data-ttu-id="f581a-149">С точки зрения математики выбранные для обучения модели признаки являются минимальным набором независимых переменных, которые определяют структуры в данных и затем успешно прогнозируют результаты.</span><span class="sxs-lookup"><span data-stu-id="f581a-149">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span></span>

<span data-ttu-id="f581a-150">Обратите внимание, что он не всегда обязателен для выполнения реконструирования или выбора признаков.</span><span class="sxs-lookup"><span data-stu-id="f581a-150">Note that it is not always necessarily to perform feature engineering or feature selection.</span></span> <span data-ttu-id="f581a-151">Его необходимость зависит от собираемых данных, используемого алгоритма и цели эксперимента.</span><span class="sxs-lookup"><span data-stu-id="f581a-151">Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.</span></span>

<!-- Module References -->
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/

