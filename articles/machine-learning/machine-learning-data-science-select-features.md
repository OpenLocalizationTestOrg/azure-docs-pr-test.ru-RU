---
title: "Выбор aaaFeature в hello командного процесса обработки и анализа данных | Документы Microsoft"
description: "Объясняется назначение hello выбора компонентов и примеры их роли в процессе улучшения hello данных машинного обучения."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 878541f5-1df8-4368-889a-ced6852aba47
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/24/2017
ms.author: zhangya;bradsev
ms.openlocfilehash: 54af93c83e4cc6a3670b3ad62490e0f74082b4ee
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/06/2017
---
# <a name="feature-selection-in-hello-team-data-science-process-tdsp"></a><span data-ttu-id="a0441-103">Выбор компонентов в hello процесса обработки и анализа данных Team (TDSP)</span><span class="sxs-lookup"><span data-stu-id="a0441-103">Feature selection in hello Team Data Science Process (TDSP)</span></span>
<span data-ttu-id="a0441-104">В этой статье описываются целей hello выбора компонентов и приводятся примеры его роли в процессе улучшения hello данных машинного обучения.</span><span class="sxs-lookup"><span data-stu-id="a0441-104">This article explains hello purposes of feature selection and provides examples of its role in hello data enhancement process of machine learning.</span></span> <span data-ttu-id="a0441-105">Эти примеры взяты из Студии машинного обучения Azure.</span><span class="sxs-lookup"><span data-stu-id="a0441-105">These examples are drawn from Azure Machine Learning Studio.</span></span> 

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="a0441-106">Здравствуйте, проектирование и Выбор функций входит в состав hello данных обработки и анализа командного процесса (TDSP) появлялся [возможности hello командного процесса обработки и анализа данных?](data-science-process-overview.md).</span><span class="sxs-lookup"><span data-stu-id="a0441-106">hello engineering and selection of features is one part of hello Team Data Science Process (TDSP) outlined in [What is hello Team Data Science Process?](data-science-process-overview.md).</span></span> <span data-ttu-id="a0441-107">Конструируются и выбора — это части hello **разработки компонентов** шага hello TDSP.</span><span class="sxs-lookup"><span data-stu-id="a0441-107">Feature engineering and selection are parts of hello **Develop features** step of hello TDSP.</span></span>

* <span data-ttu-id="a0441-108">**функция engineering**: этот процесс пытается toocreate дополнительные соответствующие функции из существующих функций необработанные hello в данных hello и tooincrease прогнозируемой мощностью toohello обучающего алгоритма.</span><span class="sxs-lookup"><span data-stu-id="a0441-108">**feature engineering**: This process attempts toocreate additional relevant features from hello existing raw features in hello data, and tooincrease predictive power toohello learning algorithm.</span></span>
* <span data-ttu-id="a0441-109">**Выбор компонентов**: этот процесс выбирает hello ключа подмножество функций исходные данные в размерности hello попытки tooreduce hello обучения проблемы.</span><span class="sxs-lookup"><span data-stu-id="a0441-109">**feature selection**: This process selects hello key subset of original data features in an attempt tooreduce hello dimensionality of hello training problem.</span></span>

<span data-ttu-id="a0441-110">Обычно **компонентов engineering** примененных первый toogenerate дополнительные функции, а затем hello **Выбор компонентов** шаг — выполнено tooeliminate имеют значения, избыточные или высокой степенью корреляции функции.</span><span class="sxs-lookup"><span data-stu-id="a0441-110">Normally **feature engineering** is applied first toogenerate additional features, and then hello **feature selection** step is performed tooeliminate irrelevant, redundant, or highly correlated features.</span></span>

## <a name="filtering-features-from-your-data---feature-selection"></a><span data-ttu-id="a0441-111">Фильтрация признаков в данных. Выбор признаков</span><span class="sxs-lookup"><span data-stu-id="a0441-111">Filtering Features from Your Data - Feature Selection</span></span>
<span data-ttu-id="a0441-112">Выбор компонентов — это процесс, который часто применяется для создания hello обучающих наборах данных для прогнозирующего моделирования задач, таких как задачи классификации или регрессии.</span><span class="sxs-lookup"><span data-stu-id="a0441-112">Feature selection is a process that is commonly applied for hello construction of training datasets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="a0441-113">Задача Hello — tooselect подмножество функций hello hello исходный набор данных, которые уменьшить его размеры с помощью минимального набора функций toorepresent hello максимальное отклонение в данных hello.</span><span class="sxs-lookup"><span data-stu-id="a0441-113">hello goal is tooselect a subset of hello features from hello original dataset that reduce its dimensions by using a minimal set of features toorepresent hello maximum amount of variance in hello data.</span></span> <span data-ttu-id="a0441-114">— Это подмножество функций, то только toobe функции hello включены tootrain hello модели.</span><span class="sxs-lookup"><span data-stu-id="a0441-114">This subset of features are, then, hello only features toobe included tootrain hello model.</span></span> <span data-ttu-id="a0441-115">Выбор признаков служит двум основным целям.</span><span class="sxs-lookup"><span data-stu-id="a0441-115">Feature selection serves two main purposes.</span></span>

* <span data-ttu-id="a0441-116">Во-первых, выбор признаков часто повышает точность классификации, исключая несоответствующие, избыточные или сильно коррелирующие признаки.</span><span class="sxs-lookup"><span data-stu-id="a0441-116">First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="a0441-117">Во-вторых он уменьшает номер hello функций, который повышает эффективность процесса обучения модели.</span><span class="sxs-lookup"><span data-stu-id="a0441-117">Second, it decreases hello number of features which makes model training process more efficient.</span></span> <span data-ttu-id="a0441-118">Это особенно важно для учеников, ресурсоемкие tootrain, например опорных векторов.</span><span class="sxs-lookup"><span data-stu-id="a0441-118">This is particularly important for learners that are expensive tootrain such as support vector machines.</span></span>

<span data-ttu-id="a0441-119">Несмотря на то, что выбор компонентов поиска tooreduce hello количеством признаков в tootrain hello hello набора данных используется модель, не обычно ссылка tooby hello термин «сокращение размерности».</span><span class="sxs-lookup"><span data-stu-id="a0441-119">Although feature selection does seek tooreduce hello number of features in hello dataset used tootrain hello model, it is not usually referred tooby hello term "dimensionality reduction".</span></span> <span data-ttu-id="a0441-120">Методы выбора компонентов Извлеките подмножество исходного признаков в данных hello без изменений.</span><span class="sxs-lookup"><span data-stu-id="a0441-120">Feature selection methods extract a subset of original features in hello data without changing them.</span></span>  <span data-ttu-id="a0441-121">Методы сокращения размерности применяю социотехники функции, которые могут преобразовывать исходной функции hello и таким образом изменять их.</span><span class="sxs-lookup"><span data-stu-id="a0441-121">Dimensionality reduction methods employ engineered features that can transform hello original features and thus modify them.</span></span> <span data-ttu-id="a0441-122">Примеры методов сокращения размерности: анализ главных компонентов, анализ канонических корреляций и сингулярная декомпозиция.</span><span class="sxs-lookup"><span data-stu-id="a0441-122">Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</span></span>

<span data-ttu-id="a0441-123">В частности одна из распространенных категорий методов выбора признаков в контролируемом контексте называется «Выбор признаков на основе фильтра».</span><span class="sxs-lookup"><span data-stu-id="a0441-123">Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</span></span> <span data-ttu-id="a0441-124">Путем вычисления hello корреляции между каждого компонента и hello целевого атрибута, эти методы применяются статистические меры tooassign функция tooeach оценка.</span><span class="sxs-lookup"><span data-stu-id="a0441-124">By evaluating hello correlation between each feature and hello target attribute, these methods apply a statistical measure tooassign a score tooeach feature.</span></span> <span data-ttu-id="a0441-125">затем Hello компоненты ранжируются по оценке hello, которое может быть toohelp используется набор hello и пороговое значение для сохранения или удаления определенных компонентов.</span><span class="sxs-lookup"><span data-stu-id="a0441-125">hello features are then ranked by hello score, which may be used toohelp set hello threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="a0441-126">Hello статистические показатели, используемые в этих методах примеры корреляции лицо, взаимной информации и квадрат тест хи hello.</span><span class="sxs-lookup"><span data-stu-id="a0441-126">Examples of hello statistical measures used in these methods include Person correlation, mutual information, and hello Chi squared test.</span></span>

<span data-ttu-id="a0441-127">В Студии машинного обучения Azure предусмотрены модули для выбора признаков.</span><span class="sxs-lookup"><span data-stu-id="a0441-127">In Azure Machine Learning Studio, there are modules provided for feature selection.</span></span> <span data-ttu-id="a0441-128">Как показано на следующий рисунок hello, эти модули включают [Выбор компонентов на основе фильтра] [ filter-based-feature-selection] и [линейного Дискриминантного анализа Фишера] [ fisher-linear-discriminant-analysis].</span><span class="sxs-lookup"><span data-stu-id="a0441-128">As shown in hello following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![Пример выбора признаков](./media/machine-learning-data-science-select-features/feature-Selection.png)

<span data-ttu-id="a0441-130">Рассмотрим, например, использование hello hello [Выбор компонентов на основе фильтра] [ filter-based-feature-selection] модуля.</span><span class="sxs-lookup"><span data-stu-id="a0441-130">Consider, for example, hello use of hello [Filter-Based Feature Selection][filter-based-feature-selection] module.</span></span> <span data-ttu-id="a0441-131">В целях удобства hello мы продолжаем интеллектуального анализа текста hello toouse пример описанные выше.</span><span class="sxs-lookup"><span data-stu-id="a0441-131">For hello purpose of convenience, we continue toouse hello text mining example outlined above.</span></span> <span data-ttu-id="a0441-132">Предположим, что мы toobuild модель регрессии после набора функций 256 создаются с помощью hello [хэширование признаков] [ feature-hashing] модуль и этой переменной ответа hello hello «Col1» и представляет книги Проверьте оценки, в диапазоне от 1 too5.</span><span class="sxs-lookup"><span data-stu-id="a0441-132">Assume that we want toobuild a regression model after a set of 256 features are created through hello [Feature Hashing][feature-hashing] module, and that hello response variable is hello "Col1" and represents a book review ratings ranging from 1 too5.</span></span> <span data-ttu-id="a0441-133">Установив «Метод оценки компонентов» toobe «Корреляции Пирсона», Здравствуйте, «Целевой столбец» toobe «Col1» и too50 «Число необходимых компонентов» hello.</span><span class="sxs-lookup"><span data-stu-id="a0441-133">By setting "Feature scoring method" toobe "Pearson Correlation", hello "Target column" toobe "Col1", and hello "Number of desired features" too50.</span></span> <span data-ttu-id="a0441-134">Затем модуль hello [Выбор компонентов на основе фильтра] [ filter-based-feature-selection] создает набор данных, содержащий 50 возможности вместе с hello целевой атрибут «Col1».</span><span class="sxs-lookup"><span data-stu-id="a0441-134">Then hello module [Filter-Based Feature Selection][filter-based-feature-selection] will produce a dataset containing 50 features together with hello target attribute "Col1".</span></span> <span data-ttu-id="a0441-135">Hello ниже рисунке показан поток hello этого эксперимента и hello входных параметров, в которых мы писали выше.</span><span class="sxs-lookup"><span data-stu-id="a0441-135">hello following figure shows hello flow of this experiment and hello input parameters we just described.</span></span>

![Пример выбора признаков](./media/machine-learning-data-science-select-features/feature-Selection1.png)

<span data-ttu-id="a0441-137">Hello на этом рисунке показано hello результирующие наборы данных.</span><span class="sxs-lookup"><span data-stu-id="a0441-137">hello following figure shows hello resulting datasets.</span></span> <span data-ttu-id="a0441-138">Каждый компонент результат функции на основе на hello корреляции Пирсона между самой и hello целевой атрибут «Col1».</span><span class="sxs-lookup"><span data-stu-id="a0441-138">Each feature is scored based on hello Pearson Correlation between itself and hello target attribute "Col1".</span></span> <span data-ttu-id="a0441-139">функции Hello с верхних оценок сохраняются.</span><span class="sxs-lookup"><span data-stu-id="a0441-139">hello features with top scores are kept.</span></span>

![Пример выбора признаков](./media/machine-learning-data-science-select-features/feature-Selection2.png)

<span data-ttu-id="a0441-141">соответствующие оценки Hello hello выбранных компонентов показаны на hello следующий рисунок.</span><span class="sxs-lookup"><span data-stu-id="a0441-141">hello corresponding scores of hello selected features are shown in hello following figure.</span></span>

![Пример выбора признаков](./media/machine-learning-data-science-select-features/feature-Selection3.png)

<span data-ttu-id="a0441-143">Применяя это [Выбор компонентов на основе фильтра] [ filter-based-feature-selection] основании hello оценки модуль 50 из 256 компоненты выбраны, так как они имеют hello наиболее корреляцией признаков с hello целевой переменной «Col1» метод «Корреляция Пирсона».</span><span class="sxs-lookup"><span data-stu-id="a0441-143">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have hello most correlated features with hello target variable "Col1", based on hello scoring method "Pearson Correlation".</span></span>

## <a name="conclusion"></a><span data-ttu-id="a0441-144">Заключение</span><span class="sxs-lookup"><span data-stu-id="a0441-144">Conclusion</span></span>
<span data-ttu-id="a0441-145">Конструируются и Выбор компонентов — два часто разработан и выбранные компоненты повысить эффективность обучения процесса, который пытается tooextract hello ключевые сведения, содержащиеся в данных hello hello hello.</span><span class="sxs-lookup"><span data-stu-id="a0441-145">Feature engineering and feature selection are two commonly Engineered and selected features increase hello efficiency of hello training process which attempts tooextract hello key information contained in hello data.</span></span> <span data-ttu-id="a0441-146">Они также повышают степень hello эти tooclassify моделей hello входных данных точно и результаты toopredict интересуют более надежно.</span><span class="sxs-lookup"><span data-stu-id="a0441-146">They also improve hello power of these models tooclassify hello input data accurately and toopredict outcomes of interest more robustly.</span></span> <span data-ttu-id="a0441-147">Конструируются и выбора можно также объединять обучения hello toomake работы большего количества вычислений со.</span><span class="sxs-lookup"><span data-stu-id="a0441-147">Feature engineering and selection can also combine toomake hello learning more computationally tractable.</span></span> <span data-ttu-id="a0441-148">Это делается путем улучшение и то уменьшение hello количество компонентов требуется toocalibrate или обучения модели.</span><span class="sxs-lookup"><span data-stu-id="a0441-148">It does so by enhancing and then reducing hello number of features needed toocalibrate or train a model.</span></span> <span data-ttu-id="a0441-149">Математически говоря, модели hello выбранного tootrain функции hello являются минимальный набор независимых переменных, которые объясняют шаблоны hello в данных hello и затем успешно прогнозирования результатов.</span><span class="sxs-lookup"><span data-stu-id="a0441-149">Mathematically speaking, hello features selected tootrain hello model are a minimal set of independent variables that explain hello patterns in hello data and then predict outcomes successfully.</span></span>

<span data-ttu-id="a0441-150">Обратите внимание, что это не всегда обязательно tooperform Выбор компонентов engineering или компонента.</span><span class="sxs-lookup"><span data-stu-id="a0441-150">Note that it is not always necessarily tooperform feature engineering or feature selection.</span></span> <span data-ttu-id="a0441-151">Необходим ли он или нет в зависимости от данных hello мы или собирать hello алгоритма, который выбирается и цель эксперимента hello hello.</span><span class="sxs-lookup"><span data-stu-id="a0441-151">Whether it is needed or not depends on hello data we have or collect, hello algorithm we pick, and hello objective of hello experiment.</span></span>

<!-- Module References -->
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/

