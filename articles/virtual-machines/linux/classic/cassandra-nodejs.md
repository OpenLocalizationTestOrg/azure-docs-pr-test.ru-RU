---
title: "Запуск Cassandra с Linux в Azure | Документация Майкрософт"
description: "Как запустить кластер Cassandra под управлением Linux в Виртуальных машинах Azure из приложения Node.js."
services: virtual-machines-linux
documentationcenter: nodejs
author: tomarcher
manager: routlaw
editor: 
tags: azure-service-management
ms.assetid: 30de1f29-e97d-492f-ae34-41ec83488de0
ms.service: virtual-machines-linux
ms.workload: infrastructure-services
ms.tgt_pltfrm: vm-linux
ms.devlang: na
ms.topic: article
ms.date: 08/17/2017
ms.author: tarcher
ms.openlocfilehash: 1ff3d77ced6c9d90029b251490c05e52d9b43515
ms.sourcegitcommit: 6699c77dcbd5f8a1a2f21fba3d0a0005ac9ed6b7
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/11/2017
---
# <a name="running-cassandra-with-linux-on-azure-and-accessing-it-from-nodejs"></a>Запуск Cassandra под управлением Linux в Azure и доступ к нему из Node.js
> [!IMPORTANT] 
> В Azure предлагаются две модели развертывания для создания ресурсов и работы с ними: [модель диспетчера ресурсов и классическая модель](../../../resource-manager-deployment-model.md). В этой статье рассматривается использование классической модели развертывания. Для большинства новых развертываний Майкрософт рекомендует использовать модель диспетчера ресурсов. Ознакомьтесь с шаблонами Resource Manager для [Datastax Enterprise](https://azure.microsoft.com/documentation/templates/datastax) и [кластера Spark и Cassandra с дистрибутивом CentOS](https://azure.microsoft.com/documentation/templates/spark-and-cassandra-on-centos/).

## <a name="overview"></a>Обзор
Microsoft Azure — это открытая облачная платформа, на которой выполняются как программное обеспечение корпорации Майкрософт, так и сторонние программы, к которым относятся операционные системы, серверы приложений, ПО промежуточного слоя для обмена сообщениями, а также базы данных SQL и NoSQL из коммерческих моделей и моделей с открытым исходным кодом. При создании гибких служб в общедоступных облаках (в том числе Azure) требуется тщательное планирование и разработка архитектуры обоих серверов приложений, а также уровней хранилищ. Архитектура распределенного хранилища Cassandra помогает создать системы высокой доступности, которые отказоустойчивы к сбоям кластеров. Cassandra — это облачная база данных NoSQL, поддерживаемая организацией Apache Software Foundation на сайте cassandra.apache.org; решение Cassandra написано на языке Java и поэтому выполняется на платформах Windows и Linux.

Цель этой статьи — продемонстрировать процесс развертывания Cassandra на Ubuntu в качестве центра обработки данных с одним и несколькими кластерами с использованием Виртуальных машин Microsoft Azure и виртуальных сетей Microsoft Azure. Развертывание кластера для оптимизации рабочих нагрузок выходит за рамки этой статьи, поскольку для этого требуются конфигурация узла с несколькими дисками, соответствующая кольцевая топология и моделирование данных для выполнения требований к репликации, согласованности данных, пропускной способности и высокой доступности.

В этой статье рассматривается фундаментальный подход, который позволяет показать все компоненты, участвующие в создании сравнимого с кластером Cassandra объекта Docker, Chef или Puppet, который значительно упрощает развертывание инфраструктуры.  

## <a name="the-deployment-models"></a>Модели развертывания
Сеть Microsoft Azure обеспечивает развертывание изолированных частных кластеров, доступ к которым может ограничиваться из соображений сетевой безопасности.  Поскольку в этой статье рассматривается развертывание Cassandra на базовом уровне, мы не уделяем внимания уровню согласованности и оптимальной структуре хранения данных с точки зрения пропускной способности. Ниже приведен список требований к сетевым характеристикам нашего гипотетического кластера.

* Внешние системы не могут получить доступ к базе данных Cassandra из службы Azure или за пределами.
* Кластер Cassandra должен размещаться за подсистемой балансировки нагрузки для экономии трафика.
* Узлы Cassandra должны развертываться в двух группах в каждом центре обработки данных для улучшенной доступности кластера.
* Кластер следует заблокировать, чтобы прямой доступ к базе данных получила только ферма серверов приложений.
* Не допускаются открытые сетевые конечные точки, отличные от SSH.
* Каждый узел Cassandra должен иметь фиксированный внутренний IP-адрес.

Решение Cassandra можно развернуть в одном или нескольких регионах Azure в зависимости от характера распределения рабочей нагрузки. Модель развертывания в нескольких регионах можно использовать для обслуживания пользователей, расположенных ближе всего к конкретному географическому региону, с использованием аналогичной инфраструктуры Cassandra. При репликации встроенных узлов Cassandra отслеживается синхронизация операций записи в несколько источников, изначально выполненных из нескольких центров обработки данных, и предоставляется согласованное представление данных для приложений. Развертывание в нескольких регионах также позволяет устранить риски, связанные с простоями ряда служб Azure. Настраиваемая топология согласованности и репликации Cassandra обеспечит выполнение различных требований приложений к RPO.

### <a name="single-region-deployment"></a>Развертывание в одной области
Мы начнем с развертывания в одном регионе и используем полученные данные для создания модели развертывания в нескольких регионах. Виртуальные сети Azure будут использоваться для создания изолированных подсетей, что позволит выполнить упомянутые выше требования к сетевой безопасности.  При развертывании в одном регионе используются решения Ubuntu 14.04 LTS и Cassandra 2.08. Тем не менее этот процесс можно легко адаптировать для других вариантов ОС Linux. Ниже приведены некоторые системные характеристики развертывания в одном регионе.  

**Высокий уровень доступности.** Узлы Cassandra, изображенные на рис. 1, развертываются в двух группах доступности. Таким образом узлы распределяются между несколькими доменами сбоя для обеспечения высокой доступности. Виртуальные машины с заметками о каждой группе доступности сопоставляются с 2 доменами сбоев.  В службе Microsoft Azure для управления незапланированными простоями (например, из-за ошибок аппаратного или программного обеспечения) используется домен сбоя, а домен обновления (например, исправления либо обновления узла или гостевой ОС, обновления приложений) применяется для управления плановыми простоями. Дополнительную информацию о роли доменов сбоя и обновления в обеспечении высокого уровня доступности см. в статье [Аварийное восстановление и высокая доступность для приложений на платформе Azure](http://msdn.microsoft.com/library/dn251004.aspx).

![Развертывание в одной области](./media/cassandra-nodejs/cassandra-linux1.png)

Рисунок 1. Развертывание в одном регионе

Обратите внимание, что на момент написания этой статьи в службе Azure запрещено явное сопоставление группы виртуальных машин с определенным доменом сбоя. Следовательно, даже в модели развертывания, показанной на рисунке 1, статистически вероятно, что все виртуальные машины можно сопоставить с двумя доменами сбоя вместо четырех.

**Трафик Thrift с балансировкой нагрузки.** Клиентские библиотеки Thrift на веб-сервере подключаются к кластеру через внутреннюю подсистему балансировки нагрузки. Для этого требуется добавить внутреннюю подсистему балансировки нагрузки в подсеть "данных" (см. рисунок 1) в контексте облачной службы, в которой размещен кластер Cassandra. После определения внутренней подсистемы балансировки нагрузки на каждый узел необходимо добавить конечную точку со сбалансированной нагрузкой с заметками о наборе со сбалансированной нагрузкой и ранее определенным именем подсистемы балансировки нагрузки. Дополнительные сведения см. в статье [Обзор внутренней подсистемы балансировки нагрузки](../../../load-balancer/load-balancer-internal-overview.md).

**Начальные значения кластера.** Для начальных значений важно выбрать наиболее доступные узлы, так как новые узлы будут обмениваться данными с узлами начальных значений с целью обнаружения топологии кластера. Один узел из каждой группы доступности обозначается в качестве узлов начальных значений во избежание возникновения единой точки отказа.

**Коэффициент репликации и уровень согласованности.** Функции обеспечения высокой доступности и надежности данных, встроенные в решение Cassandra, имеют коэффициент репликации (количество копий каждой строки, хранящихся в кластере) и уровень согласованности (число реплик для чтения или записи перед возвратом результата вызывающему объекту). Коэффициент репликации указывается во время создания ПРОСТРАНСТВА КЛЮЧЕЙ (аналогично реляционной базе данных), в то время как уровень согласованности задается при подаче запроса CRUD. Дополнительные сведения о согласованности и формуле для вычисления кворума см. в документации к Cassandra в разделе [Configuring data consistency](http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html) (Настройка согласованности данных).

Cassandra поддерживает два типа моделей целостности данных — текущую и окончательную согласованность; коэффициент репликации и уровень согласованности вместе определяют, являются ли данные согласованными после завершения операции записи или станут таковыми в будущем. Например, задание КВОРУМА на уровне согласованности гарантирует согласованность данных при любом уровне согласованности, пока не достигнуто количество реплик, которые должны быть записаны для достижения КВОРУМА (например, ОДИН), что приведет к окончательному согласованию данных.

Показанный выше кластер из 8 узлов с коэффициентом репликации 3 и КВОРУМОМ (2 узла считываются или записываются для согласованности) на уровне согласованности чтения и записи теоретически может перенести потерю не более 1 узла на каждую группу репликации, прежде чем приложение заметит ошибку. Предполагается, что все пространства ключей содержат сбалансированные запросы на чтение и запись.  Ниже перечислены параметры, которые будут использоваться для развернутого кластера.

Конфигурация кластера Cassandra, развернутого в одном регионе

| Параметр кластера | Значение | Примечания |
| --- | --- | --- |
| Число узлов (N) |8 |Общее количество узлов в кластере |
| Коэффициент репликации (RF) |3 |Число реплик данной строки |
| Уровень согласованности (запись) |QUORUM [(RF/2) +1) = 2] Результат формулы округляется |Записывает максимум 2 реплики перед отправкой ответа вызывающему объекту; 3-я реплика записывается для обеспечения окончательной согласованности. |
| Уровень согласованности (чтение) |QUORUM [(RF/2) +1= 2] Результат формулы округляется |Считывается 2 реплики перед отправкой ответа вызывающему объекту. |
| Стратегия репликации |Дополнительную информацию о NetworkTopologyStrategy см. в разделе [Data Replication](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html) (Репликация данных) в документации по Cassandra |Понимает топологию развертывания и помещает реплики на узлы таким образом, чтобы все реплики не попадут в одну стойку. |
| Информатор |Дополнительную информацию о GossipingPropertyFileSnitch см. в разделе [Snitches](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html) (Информаторы) в документации по Cassandra |NetworkTopologyStrategy использует концепцию информатора для получения данных о топологии. GossipingPropertyFileSnitch обеспечивает лучший контроль при сопоставлении каждого узла с центром обработки данных и стойкой. Затем кластер использует полученные данные для распространения этих сведений. Процесс выполняется гораздо проще в параметре динамического IP-адреса относительно PropertyFileSnitch |

**Рекомендации по Azure для кластера Cassandra.** Виртуальные машины Microsoft Azure способны использовать хранилище BLOB-объектов Azure для сохранения данных на диске. В службе хранилища Azure сохраняется три реплики каждого диска для обеспечения высокой надежности. Таким образом, каждая строка данных, вставляемых в таблицу Cassandra, уже хранится в 3 репликах, поэтому требования к согласованности данных уже выполняются, даже если коэффициент репликации (RF) равен 1. Основная проблема с коэффициентом репликации 1 состоит в том, что в работе приложения будут наблюдаться простои даже в случае сбоя одного узла Cassandra. Однако если узел не работает из-за проблем (например, сбоев аппаратного или системного программного обеспечения), распознаваемых контроллером структуры Azure, он подготовит новый узел вместо себя, используя те же диски хранилища. Подготовка нового узла для замены старого может занять несколько минут.  Аналогичным образом, для действий по плановому обслуживанию, например внесения изменений в гостевую ОС или приложения либо обновления Cassandra, контроллер структуры Azure развертывает обновления узлов в кластере.  Кроме того, обновления могут развертываться одновременно на нескольких узлах, поэтому в работе кластера могут возникать небольшие простои в нескольких разделах. Тем не менее данные не будут потеряны благодаря встроенной избыточности службы хранилища Azure.  

Что касается систем, которые развернуты в Azure и не требуют высокого уровня доступности (например, на уровне 99,9, что соответствует 8,76 часа в год; дополнительную информацию см. в [статье о высоком уровне доступности](http://en.wikipedia.org/wiki/High_availability)), их можно запустить при коэффициенте репликации 1 и уровне согласованности "ОДИН".  Что касается приложений с требованиями к высокой доступности при коэффициенте репликации 3 и уровне согласованности, соответствующем КВОРУМУ, допускается простой одного из узлов одной из реплик. Коэффициент репликации 1 в традиционных развертываниях (например, локальных) не может использоваться из-за возможной потери данных вследствие подобных сбоев диска.   

## <a name="multi-region-deployment"></a>Развертывание в нескольких регионах
Описанная выше модель репликации и согласованности с учетом особенностей центра обработки данных Cassandra помогает сразу выполнить развертывание в нескольких регионах без использования внешних инструментов. Это значительно отличается от традиционных реляционных баз данных, которые могут отличаться весьма сложными настройками зеркального отображения баз данных при операциях записи в несколько источников. Cassandra при настройке в нескольких регионах обеспечивает выполнение сценариев использования, включая следующие.

**Развертывание с учетом расположения.** На работу мультитенантных приложений с явным сопоставлением пользователей клиентов с регионами могут положительно повлиять низкие задержки в работе кластера, развернутого в нескольких регионах. Например, системы управления обучением для образовательных учреждений способны развернуть распределенный кластер в регионах "Запад и восток США" для выполнения транзакций и аналитики в соответствующих учреждениях. Данные могут отличаться локальной согласованностью во время операций чтения и записи, а также окончательной согласованностью в обоих регионах. Имеются другие примеры, такие как распределение носителей и электронная торговля. Так, любые сферы, пользовательская база в которых сосредоточена в одном регионе, — хороший пример использования данной модели развертывания.

**Высокая доступность.** Избыточность — ключевой фактор обеспечения высокой доступности программного и аппаратного обеспечения. Дополнительные сведения см. в статье Building Reliable Cloud Systems on Microsoft Azure (Создание надежных облачных систем в Microsoft Azure). В Microsoft Azure единственным надежным способом обеспечения истинной избыточности является развертывание кластера в нескольких регионах. Приложения можно развернуть в режиме "активный-активный" или"активный-пассивный", и если один из регионов не работает, диспетчер трафика Azure может перенаправить трафик в активный регион.  Если кластер развернут в одном регионе, а уровень доступности составляет 99,9, то в случае развертывания в двух регионах можно обеспечить доступность на уровне 99,9999, которая вычисляется по следующей формуле: (1 – (1 – 0,999)*(1–0,999))*100). Дополнительные сведения см. в упомянутой выше статье.

**Аварийное восстановление.** Если кластер Cassandra, развернутый в нескольких регионах, разработан должным образом, он может выдерживать катастрофические сбои центра обработки данных. Если работа в одном регионе невозможна, приложение, развернутое в других регионах, может начать обслуживать пользователей. Как и в любых других примерах реализации непрерывности бизнес-процессов, приложение должно быть отказоустойчивым к потере некоторых данных в асинхронном конвейере. Тем не менее решение Cassandra выполняет восстановление намного быстрее по сравнению с традиционными процессами восстановления баз данных. На рисунке 2 показана типичная модель развертывания в нескольких регионах с восемью узлами в каждом из них. Оба региона — зеркальное отображение друг друга. Практические схемы определяются типом рабочей нагрузки (например, транзакционной или аналитической), показателями RPO и RTO, а также требованиями к согласованности и доступности данных.

![Развертывание в нескольких регионах](./media/cassandra-nodejs/cassandra-linux2.png)

Рисунок 2. Развертывание Cassandra в нескольких регионах

### <a name="network-integration"></a>Сетевая интеграция
Наборы виртуальных машин, развернутых в частных сетях, которые расположены в двух регионах, обменивается данными друг с другом через VPN-туннель. VPN-туннель соединяет два шлюза программного обеспечения, подготовленные в процессе развертывания сети. Оба региона имеют сходную архитектуру сети в терминах "веб-подсетей" и "подсетей данных"; сети Azure позволяют создавать необходимое количество подсетей и применять списки ACL, если это требуется из соображений сетевой безопасности. При проектировании топологии кластера необходимо учитывать внутреннюю задержку обмена данными в центре обработки данных и экономическое влияние сетевого трафика.

### <a name="data-consistency-for-multi-data-center-deployment"></a>Согласованность данных при развертывании в нескольких центрах обработки данных
При распределенных развертываниях необходимо учитывать влияние топологии кластера на пропускную способность и высокий уровень доступности. Коэффициент репликации и уровень согласованности необходимо выбрать таким образом, чтобы кворум не зависел от доступности во всех центрах обработки данных.
Что касается системы, требующей высокого уровня согласованности, LOCAL_QUORUM для уровня согласованности (для операций чтения и записи) обеспечит должное выполнение локальных операций чтения и записи из локальных узлов во время асинхронной репликации данных в удаленных центрах обработки данных.  В таблице 2 суммируются сведения о конфигурации кластера, развернутого в нескольких регионах (он описан ниже).

**Конфигурация кластера Cassandra, развернутого в двух регионах**

| Параметр кластера | Значение | Примечания |
| --- | --- | --- |
| Число узлов (N) |8 + 8 |Общее количество узлов в кластере |
| Коэффициент репликации (RF) |3 |Число реплик данной строки |
| Уровень согласованности (запись) |LOCAL_QUORUM [(sum(RF)/2) +1) = 4] Результат формулы округляется |Два узла будут синхронно записаны в первом центре обработки данных; дополнительные два узла, необходимые для кворума, будут асинхронно записаны во втором центре обработки данных. |
| Уровень согласованности (чтение) |LOCAL_QUORUM ((RF/2) +1) = 2 Результат формулы округляется |Запросы на чтение удовлетворяются только из одного региона. 2 узла считываются перед обратной отправкой ответа клиенту. |
| Стратегия репликации |Дополнительную информацию о NetworkTopologyStrategy см. в разделе [Data Replication](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html) (Репликация данных) в документации по Cassandra |Понимает топологию развертывания и помещает реплики на узлы таким образом, чтобы все реплики не попадут в одну стойку. |
| Информатор |Дополнительную информацию о GossipingPropertyFileSnitch см. в разделе [Snitches](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html) (Информаторы) в документации по Cassandra |NetworkTopologyStrategy использует концепцию информатора для получения данных о топологии. GossipingPropertyFileSnitch обеспечивает лучший контроль при сопоставлении каждого узла с центром обработки данных и стойкой. Затем кластер использует полученные данные для распространения этих сведений. Процесс выполняется гораздо проще в параметре динамического IP-адреса относительно PropertyFileSnitch |

## <a name="the-software-configuration"></a>КОНФИГУРАЦИЯ ПРОГРАММНОГО ОБЕСПЕЧЕНИЯ
Во время развертывания используются следующие версии программного обеспечения.

<table>
<tr><th>Программное обеспечение</th><th>Источник</th><th>Version (версия)</th></tr>
<tr><td>JRE    </td><td>[JRE 8](http://www.oracle.com/technetwork/java/javase/downloads/server-jre8-downloads-2133154.html) </td><td>8U5</td></tr>
<tr><td>JNA    </td><td>[JNA](https://github.com/twall/jna) </td><td> 3.2.7</td></tr>
<tr><td>Cassandra</td><td>[Apache Cassandra 2.0.8](http://www.apache.org/dist/cassandra/2.0.8/apache-cassandra-2.0.8-bin.tar.gz)</td><td> 2.0.8</td></tr>
<tr><td>Ubuntu    </td><td>[Microsoft Azure](https://azure.microsoft.com/) </td><td>14.04 LTS</td></tr>
</table>

Так как при загрузке среды выполнения Java требуется вручную принятия условия лицензионного соглашения Oracle, для упрощения развертывания загрузите все необходимое программное обеспечение на рабочий стол для последующей передачи в образ шаблона Ubuntu, который будет создан в качестве основы для развертывания кластера.

Скачайте приведенное выше программное обеспечение в известный каталог скачиваний (например, %TEMP%/downloads в ОС Windows или ~/downloads в Linux или Mac) на локальном компьютере.

### <a name="create-ubuntu-vm"></a>СОЗДАНИЕ ВИРТУАЛЬНОЙ МАШИНЫ UBUNTU
На этом этапе процесса мы создадим образ Ubuntu с необходимым программным обеспечением для повторного использования с целью подготовки нескольких узлов Cassandra.  

#### <a name="step-1-generate-ssh-key-pair"></a>Шаг 1. Создание пары ключей SSH
Azure требует открытый ключ X509, который во время подготовки был закодирован в формат PEM или DER. Создайте пару,состоящую из открытого и закрытого ключей, следуя инструкциям, приведенным в разделе "Использование SSH с Linux в Azure". Если вы планируете использовать putty.exe в качестве SSH-клиента в Windows или Linux, необходимо преобразовать закодированный в PEM закрытый ключ RSA в формат PPK с помощью средства puttygen.exe; соответствующие инструкции можно найти на приведенной выше веб-странице.

#### <a name="step-2-create-ubuntu-template-vm"></a>Шаг 2. Создание виртуальной машины шаблонов Ubuntu
Чтобы создать шаблонную виртуальную машину, войдите на классический портал Azure и выполните следующие действия: последовательно выберите «СОЗДАТЬ», «СРЕДА ВЫПОЛНЕНИЯ ПРИЛОЖЕНИЙ», «ВИРТУАЛЬНАЯ МАШИНА», «ИЗ КОЛЛЕКЦИИ», «UBUNTU», «Ubuntu Server 14.04 LTS», а затем щелкните стрелку вправо. Если вам нужен учебник по созданию виртуальной машины в Linux, см. разделе "Создание виртуальной машины под управлением Linux".

На экране № 1 ("Конфигурация виртуальной машины") введите следующие сведения.

<table>
<tr><th>ИМЯ ПОЛЯ              </td><td>       ЗНАЧЕНИЕ ПОЛЯ               </td><td>         ПРИМЕЧАНИЯ                </td><tr>
<tr><td>ДАТА ВЫПУСКА ВЕРСИИ    </td><td> Выберите дату в раскрывающемся списке.</td><td></td><tr>
<tr><td>ИМЯ ВИРТУАЛЬНОЙ МАШИНЫ    </td><td> cass-template                   </td><td> Это имя узла виртуальной машины </td><tr>
<tr><td>УРОВЕНЬ                     </td><td> STANDARD                           </td><td> Оставьте значение по умолчанию.              </td><tr>
<tr><td>РАЗМЕР                     </td><td> A1                              </td><td>Выберите виртуальную машину с учетом потребностей к вводу-выводу; для этого оставьте значение по умолчанию. </td><tr>
<tr><td> НОВОЕ ИМЯ ПОЛЬЗОВАТЕЛЯ             </td><td> localadmin                       </td><td> "admin" — это зарезервированное имя пользователя в Ubuntu 12.xx и более поздних версий.</td><tr>
<tr><td> ПРОВЕРКА ПОДЛИННОСТИ         </td><td> Щелкните флажок                 </td><td>Установите флажок, если хотите безопасно пользоваться ключом SSH </td><tr>
<tr><td> СЕРТИФИКАТ             </td><td> имя файла сертификата открытого ключа </td><td> Используйте ранее созданный открытый ключ.</td><tr>
<tr><td> Новый пароль    </td><td> надежный пароль </td><td> </td><tr>
<tr><td> Подтверждение пароля    </td><td> надежный пароль </td><td></td><tr>
</table>

На экране № 2 ("Конфигурация виртуальной машины") введите следующие сведения.

<table>
<tr><th>ИМЯ ПОЛЯ             </th><th> ЗНАЧЕНИЕ ПОЛЯ                       </th><th> ПРИМЕЧАНИЯ                                 </th></tr>
<tr><td> ОБЛАЧНАЯ СЛУЖБА    </td><td> Создать новую облачную службу.    </td><td>Облачная служба — это контейнер вычислительных ресурсов, таки как виртуальные машины.</td></tr>
<tr><td> DNS-ИМЯ ОБЛАЧНОЙ СЛУЖБЫ    </td><td>ubuntu-template.cloudapp.net    </td><td>Назначьте имя независимой подсистеме балансировки нагрузки</td></tr>
<tr><td> РЕГИОН/ТЕРРИТОРИАЛЬНАЯ ГРУППА/ВИРТУАЛЬНАЯ СЕТЬ </td><td>    Запад США    </td><td> Выберите регион, из которого веб-приложения осуществляют доступ к кластеру Cassandra</td></tr>
<tr><td>УЧЕТНАЯ ЗАПИСЬ ХРАНЕНИЯ </td><td>    Используйте значение по умолчанию.    </td><td>Используйте учетную запись хранения по умолчанию или учетную запись хранения, предварительно созданную в определенном регионе.</td></tr>
<tr><td>группа доступности; </td><td>    None </td><td>    Оставьте пустым.</td></tr>
<tr><td>КОНЕЧНЫЕ ТОЧКИ    </td><td>Используйте значение по умолчанию. </td><td>    Используйте конфигурацию SSH по умолчанию. </td></tr>
</table>

Щелкните стрелку вправо, оставьте настройки по умолчанию на экране № 3 и нажмите кнопку "Проверить", чтобы завершить процесс подготовки виртуальной машины. Через несколько минут виртуальная машина с именем "ubuntu-template" получит состояние "выполняется".

### <a name="install-the-necessary-software"></a>УСТАНОВКА НЕОБХОДИМОГО ПРОГРАММНОГО ОБЕСПЕЧЕНИЯ
#### <a name="step-1-upload-tarballs"></a>Шаг 1. Передача архивов в формате TAR
Используя scp или pscp, скопируйте ранее скачанное программное обеспечение в каталог ~/downloads, используя команду в следующем формате:

##### <a name="pscp-server-jre-8u5-linux-x64targz-localadminhk-cas-templatecloudappnethomelocaladmindownloadsserver-jre-8u5-linux-x64targz"></a>pscp server-jre-8u5-linux-x64.tar.gz localadmin@hk-cas-template.cloudapp.net:/home/localadmin/downloads/server-jre-8u5-linux-x64.tar.gz
Снова выполните приведенную выше команду для JRE и битов Cassandra.

#### <a name="step-2-prepare-the-directory-structure-and-extract-the-archives"></a>Шаг 2. Подготовка структуры каталогов и извлечение архивов
Войдите на виртуальную машину, создайте структуру каталогов и извлеките программное обеспечение в качестве суперпользователя с помощью указанного ниже сценария bash:

    #!/bin/bash
    CASS_INSTALL_DIR="/opt/cassandra"
    JRE_INSTALL_DIR="/opt/java"
    CASS_DATA_DIR="/var/lib/cassandra"
    CASS_LOG_DIR="/var/log/cassandra"
    DOWNLOADS_DIR="~/downloads"
    JRE_TARBALL="server-jre-8u5-linux-x64.tar.gz"
    CASS_TARBALL="apache-cassandra-2.0.8-bin.tar.gz"
    SVC_USER="localadmin"

    RESET_ERROR=1
    MKDIR_ERROR=2

    reset_installation ()
    {
       rm -rf $CASS_INSTALL_DIR 2> /dev/null
       rm -rf $JRE_INSTALL_DIR 2> /dev/null
       rm -rf $CASS_DATA_DIR 2> /dev/null
       rm -rf $CASS_LOG_DIR 2> /dev/null
    }
    make_dir ()
    {
       if [ -z "$1" ]
       then
          echo "make_dir: invalid directory name"
          exit $MKDIR_ERROR
       fi

       if [ -d "$1" ]
       then
          echo "make_dir: directory already exists"
          exit $MKDIR_ERROR
       fi

       mkdir $1 2>/dev/null
       if [ $? != 0 ]
       then
          echo "directory creation failed"
          exit $MKDIR_ERROR
       fi
    }

    unzip()
    {
       if [ $# == 2 ]
       then
          tar xzf $1 -C $2
       else
          echo "archive error"
       fi

    }

    if [ -n "$1" ]
    then
       SVC_USER=$1
    fi

    reset_installation
    make_dir $CASS_INSTALL_DIR
    make_dir $JRE_INSTALL_DIR
    make_dir $CASS_DATA_DIR
    make_dir $CASS_LOG_DIR

    #unzip JRE and Cassandra
    unzip $HOME/downloads/$JRE_TARBALL $JRE_INSTALL_DIR
    unzip $HOME/downloads/$CASS_TARBALL $CASS_INSTALL_DIR

    #Change the ownership to the service credentials

    chown -R $SVC_USER:$GROUP $CASS_DATA_DIR
    chown -R $SVC_USER:$GROUP $CASS_LOG_DIR
    echo "edit /etc/profile to add JRE to the PATH"
    echo "installation is complete"


Если вставить этот сценарий в окно vim, не забудьте удалить символ возврата каретки (‘\r”) с помощью следующей команды:

    tr -d '\r' <infile.sh >outfile.sh

#### <a name="step-3-edit-etcprofile"></a>Шаг 3. Изменение etc/profile
Добавьте следующие данные в конце.

    JAVA_HOME=/opt/java/jdk1.8.0_05
    CASS_HOME= /opt/cassandra/apache-cassandra-2.0.8
    PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$CASS_HOME/bin
    export JAVA_HOME
    export CASS_HOME
    export PATH

#### <a name="step-4-install-jna-for-production-systems"></a>Шаг 4. Установка JNA для производственных систем
Используйте следующую последовательность команд: следующая команда установит jna-3.2.7.jar и jna-platform-3.2.7.jar в каталог /usr/share.java directory sudo apt-get install libjna-java.

Создайте символические ссылки в каталоге $CASS_HOME/lib, чтобы сценарий запуска Cassandra смог найти эти JAR-файлы.

    ln -s /usr/share/java/jna-3.2.7.jar $CASS_HOME/lib/jna.jar

    ln -s /usr/share/java/jna-platform-3.2.7.jar $CASS_HOME/lib/jna-platform.jar

#### <a name="step-5-configure-cassandrayaml"></a>Шаг 5. Настройка cassandra.yaml
Измените cassandra.yaml на каждой виртуальной машине, чтобы отобразить конфигурацию, необходимую для всех виртуальных машин. (Соответствующие настройки будут внесены при фактической подготовке.)

<table>
<tr><th>Имя поля   </th><th> Значение  </th><th>    Примечания </th></tr>
<tr><td>cluster_name </td><td>    "CustomerService"    </td><td> Используйте имя, которое отражает развертывание.</td></tr>
<tr><td>listen_address    </td><td>[Оставьте пустым.]    </td><td> Удалите "localhost". </td></tr>
<tr><td>rpc_addres   </td><td>[Оставьте пустым.]    </td><td> Удалите "localhost". </td></tr>
<tr><td>начальные значения    </td><td>"10.1.2.4, 10.1.2.6, 10.1.2.8"    </td><td>Список всех IP-адресов, которые обозначены как начальные значения.</td></tr>
<tr><td>endpoint_snitch </td><td> org.apache.cassandra.locator.GossipingPropertyFileSnitch </td><td> Этот элемент используется NetworkTopologyStrateg для обозначения центра обработки данных и стойки виртуальной машины.</td></tr>
</table>

#### <a name="step-6-capture-the-vm-image"></a>Шаг 6. Запись образа виртуальной машины
Войдите на виртуальную машину, используя ранее созданные имя узла (hk-cas-template.cloudapp.net) и закрытый ключ SSH. Дополнительные сведения о том, как войти в систему с помощью команды ssh или средства putty.exe, см. в разделе "Использование SSH с Linux в Azure".

Выполните действия в следующей последовательности, чтобы записать образ.

##### <a name="1-deprovision"></a>1. Отзыв
Выполните команду "sudo waagent -deprovision + user", чтобы удалить сведения о конкретных экземплярах виртуальной машины. Дополнительную информацию о записи образа и использовании его в качестве шаблона см. в статье [Запись классической виртуальной машины Linux в виде образа](capture-image.md).

##### <a name="2-shutdown-the-vm"></a>2. Завершение работы виртуальной машины
Убедитесь, что виртуальная машина выделена, и щелкните ссылку "ЗАВЕРШЕНИЕ РАБОТЫ" в нижней панели команд.

##### <a name="3-capture-the-image"></a>3. Запись образа
Убедитесь, что виртуальная машина выделена, и щелкните ссылку "ЗАПИСЬ" в нижней панели команд. На следующем экране задайте ИМЯ ОБРАЗА (например, hk-cas-2-08-ub-14-04-2014071), соответствующее ОПИСАНИЕ ОБРАЗА и установите флажок "Проверить", чтобы завершить процесс записи.

Это может занять несколько секунд. Кроме того, образ должен быть доступен в разделе "МОИ ОБРАЗЫ" в коллекции образов. Исходная виртуальная машина будет автоматически удалена после успешной записи образа. 

## <a name="single-region-deployment-process"></a>Процесс развертывания в одном регионе
**Шаг 1. Создание виртуальной сети.** Войдите на портал Azure и создайте классическую виртуальную сеть с атрибутами, приведенными в следующей таблице. Подробное описание процесса см. в статье [Создание (классической) виртуальной сети с помощью портала предварительной версии Azure](../../../virtual-network/virtual-networks-create-vnet-classic-pportal.md).      

<table>
<tr><th>Имя атрибута виртуальной машины</th><th>Значение</th><th>Примечания</th></tr>
<tr><td>Имя</td><td>vnet-cass-west-us</td><td></td></tr>
<tr><td>Регион</td><td>Запад США</td><td></td></tr>
<tr><td>DNS-серверы</td><td>None</td><td>Пропустите этот атрибут, поскольку мы не используем DNS-сервер</td></tr>
<tr><td>Пространство адресов</td><td>10.1.0.0/16</td><td></td></tr>    
<tr><td>Начальный IP-адрес</td><td>10.1.0.0</td><td></td></tr>    
<tr><td>CIDR </td><td>/16 (65531)</td><td></td></tr>
</table>

Добавьте следующие подсети.

<table>
<tr><th>Name (Имя)</th><th>Начальный IP-адрес</th><th>CIDR</th><th>Примечания</th></tr>
<tr><td>web</td><td>10.1.1.0</td><td>/24 (251)</td><td>Подсеть для веб-фермы</td></tr>
<tr><td>data</td><td>10.1.2.0</td><td>/24 (251)</td><td>Подсеть для узлов базы данных</td></tr>
</table>

Подсети данных и веб-подсети можно защитить с помощью сетевых групп безопасности, которые не рассматриваются в этой статье.  

**Шаг 2. Подготовка виртуальных машин.** С помощью ранее сгенерированного образа мы создадим следующие виртуальные машины на облачном сервере hk-c-svc-west и привяжем их к соответствующим подсетям, как показано ниже.

<table>
<tr><th>Имя компьютера    </th><th>Подсеть    </th><th>IP-адрес    </th><th>группа доступности;</th><th>Контроллер домена/стойка</th><th>Начальное значение?</th></tr>
<tr><td>hk-c1-west-us    </td><td>data    </td><td>10.1.2.4    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack1 </td><td>Да</td></tr>
<tr><td>hk-c2-west-us    </td><td>data    </td><td>10.1.2.5    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack1    </td><td>Нет </td></tr>
<tr><td>hk-c3-west-us    </td><td>data    </td><td>10.1.2.6    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack2    </td><td>Да</td></tr>
<tr><td>hk-c4-west-us    </td><td>data    </td><td>10.1.2.7    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack2    </td><td>Нет </td></tr>
<tr><td>hk-c5-west-us    </td><td>data    </td><td>10.1.2.8    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack3    </td><td>Да</td></tr>
<tr><td>hk-c6-west-us    </td><td>data    </td><td>10.1.2.9    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack3    </td><td>Нет </td></tr>
<tr><td>hk-c7-west-us    </td><td>data    </td><td>10.1.2.10    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack4    </td><td>Да</td></tr>
<tr><td>hk-c8-west-us    </td><td>data    </td><td>10.1.2.11    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack4    </td><td>Нет </td></tr>
<tr><td>hk-w1-west-us    </td><td>web    </td><td>10.1.1.4    </td><td>hk-w-aset-1    </td><td>                       </td><td>Недоступно</td></tr>
<tr><td>hk-w2-west-us    </td><td>web    </td><td>10.1.1.5    </td><td>hk-w-aset-1    </td><td>                       </td><td>Недоступно</td></tr>
</table>

Для создания приведенного выше списка виртуальных машин необходимо выполнить следующие действия.

1. Создайте пустую облачную службу в определенном регионе.
2. Создайте виртуальную машину из ранее записанного образа и подключите ее к ранее созданной виртуальной сети; повторите это действие для всех виртуальных машин.
3. Добавьте внутреннюю подсистему балансировки нагрузки в облачную службу и подключите ее к подсети "данных".
4. Для каждой ранее созданной виртуальной машины добавьте конечную точку с балансировкой нагрузки для экономии трафика через набор балансировки нагрузки, подключенный к ранее созданной внутренней подсистеме балансировки нагрузки.

Описанные выше действия можно выполнить с помощью классического портала Azure. Используйте компьютер под управлением Windows (или виртуальную машину в Azure, если у вас нет доступа к компьютеру под управлением Windows), а также следующий сценарий PowerShell, чтобы автоматически подготовить все 8 виртуальных машин.

**Список 1. Сценарий PowerShell для подготовки виртуальных машин**

        #Tested with Azure Powershell - November 2014
        #This powershell script deployes a number of VMs from an existing image inside an Azure region
        #Import your Azure subscription into the current Powershell session before proceeding
        #The process: 1. create Azure Storage account, 2. create virtual network, 3.create the VM template, 2. crate a list of VMs from the template

        #fundamental variables - change these to reflect your subscription
        $country="us"; $region="west"; $vnetName = "your_vnet_name";$storageAccount="your_storage_account"
        $numVMs=8;$prefix = "hk-cass";$ilbIP="your_ilb_ip"
        $subscriptionName = "Azure_subscription_name";
        $vmSize="ExtraSmall"; $imageName="your_linux_image_name"
        $ilbName="ThriftInternalLB"; $thriftEndPoint="ThriftEndPoint"

        #generated variables
        $serviceName = "$prefix-svc-$region-$country"; $azureRegion = "$region $country"

        $vmNames = @()
        for ($i=0; $i -lt $numVMs; $i++)
        {
           $vmNames+=("$prefix-vm"+($i+1) + "-$region-$country" );
        }

        #select an Azure subscription already imported into Powershell session
        Select-AzureSubscription -SubscriptionName $subscriptionName -Current
        Set-AzureSubscription -SubscriptionName $subscriptionName -CurrentStorageAccountName $storageAccount

        #create an empty cloud service
        New-AzureService -ServiceName $serviceName -Label "hkcass$region" -Location $azureRegion
        Write-Host "Created $serviceName"

        $VMList= @()   # stores the list of azure vm configuration objects
        #create the list of VMs
        foreach($vmName in $vmNames)
        {
           $VMList += New-AzureVMConfig -Name $vmName -InstanceSize ExtraSmall -ImageName $imageName |
           Add-AzureProvisioningConfig -Linux -LinuxUser "localadmin" -Password "Local123" |
           Set-AzureSubnet "data"
        }

        New-AzureVM -ServiceName $serviceName -VNetName $vnetName -VMs $VMList

        #Create internal load balancer
        Add-AzureInternalLoadBalancer -ServiceName $serviceName -InternalLoadBalancerName $ilbName -SubnetName "data" -StaticVNetIPAddress "$ilbIP"
        Write-Host "Created $ilbName"
        #Add add the thrift endpoint to the internal load balancer for all the VMs
        foreach($vmName in $vmNames)
        {
            Get-AzureVM -ServiceName $serviceName -Name $vmName |
                Add-AzureEndpoint -Name $thriftEndPoint -LBSetName "ThriftLBSet" -Protocol tcp -LocalPort 9160 -PublicPort 9160 -ProbePort 9160 -ProbeProtocol tcp -ProbeIntervalInSeconds 10 -InternalLoadBalancerName $ilbName |
                Update-AzureVM

            Write-Host "created $vmName"     
        }

**Шаг 3. Настройка Cassandra на каждой виртуальной машине**

Войдите на виртуальную машину и выполните следующие действия:

* Внесите изменения в $CASS_HOME/conf/cassandra-rackdc.properties, чтобы указать свойства центра обработки данных и стойки.
  
       dc =EASTUS, rack =rack1
* Внесите изменения в cassandra.yaml, чтобы настроить узлы начальных значений, как показано ниже.
  
       Seeds: "10.1.2.4,10.1.2.6,10.1.2.8,10.1.2.10"

**Шаг 4. Запуск виртуальных машин и тестирование кластера**

Войдите на один из узлов (например, hk-c1-west-us) и выполните следующую команду, чтобы просмотреть состояние кластера.

       nodetool –h 10.1.2.4 –p 7199 status

На экране кластера из 8 узлов отобразятся данных, похожие на приведенные ниже.

<table>
<tr><th>Состояние</th><th>Адрес    </th><th>загрузить    </th><th>Маркеры    </th><th>Владение </th><th>Идентификатор узла    </th><th>Стойка</th></tr>
<tr><th>UN    </td><td>10.1.2.4     </td><td>87,81 КБ    </td><td>256    </td><td>38,0 %    </td><td>GUID (удален)</td><td>rack1</td></tr>
<tr><th>UN    </td><td>10.1.2.5     </td><td>41,08 КБ    </td><td>256    </td><td>68,9 %    </td><td>GUID (удален)</td><td>rack1</td></tr>
<tr><th>UN    </td><td>10.1.2.6     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack2</td></tr>
<tr><th>UN    </td><td>10.1.2.7     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack2</td></tr>
<tr><th>UN    </td><td>10.1.2.8     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack3</td></tr>
<tr><th>UN    </td><td>10.1.2.9     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack3</td></tr>
<tr><th>UN    </td><td>10.1.2.10     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack4</td></tr>
<tr><th>UN    </td><td>10.1.2.11     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack4</td></tr>
</table>

## <a name="test-the-single-region-cluster"></a>Тестирование отдельного кластера региона
Чтобы протестировать кластер, выполните следующие действия.

1. С помощью командлета PowerShell Get-AzureInternalLoadbalancer получите IP-адрес внутренней подсистемы балансировки нагрузки (например, 10.1.2.101). Синтаксис команды приведен ниже: Get-AzureLoadbalancer –ServiceName "hk-c-svc-west-us" (выводит на экран данные о внутренней подсистеме балансировки нагрузки и ее IP-адрес).
2. Войдите на виртуальную машину веб-фермы (например, hk-w1-west-us) с помощью Putty или ssh
3. Выполните команду $CASS_HOME/bin/cqlsh 10.1.2.101 9160
4. Проверьте работу кластера с помощью следующих команд CQL:
   
     CREATE KEYSPACE customers_ks WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };   USE customers_ks;   CREATE TABLE Customers(customer_id int PRIMARY KEY, firstname text, lastname text);   INSERT INTO Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');   INSERT INTO Customers(customer_id, firstname, lastname) VALUES (2, 'Jane', 'Doe');
   
     SELECT * FROM Customers;

На экране должно появиться примерно следующее:

<table>
  <tr><th> customer_id </th><th> firstname </th><th> Lastname </th></tr>
  <tr><td> 1 </td><td> Артем </td><td> Кузнецов </td></tr>
  <tr><td> 2 </td><td> Ольга </td><td> Кузнецов </td></tr>
</table>

Обратите внимание, что пространство ключей, созданное на шаге 4, использует SimpleStrategy со значением параметра replication_factor, равным 3. SimpleStrategy рекомендуется для развертываний с одним центром обработки данных, а NetworkTopologyStrategy — для развертываний с несколькими центрами. Значение параметра replication_factor, равное 3, обеспечивает отказоустойчивость узлов.

## <a id="tworegion"> </a>Процесс развертывания в нескольких регионах
Завершив развертывание в одном регионе, повторите этот процесс для установки второго региона. Основное отличие между развертываниями в одном и нескольких регионах состоит в настройке VPN-туннеля для связи между регионами. Начнем с установки сети, подготовим виртуальные машины и настроим Cassandra.

### <a name="step-1-create-the-virtual-network-at-the-2nd-region"></a>Шаг 1. Создание виртуальной сети во втором регионе
Войдите на классический портал Azure и создайте виртуальную сеть с атрибутами, приведенными в таблице. Подробное описание процесса см. в статье [Создание (классической) виртуальной сети с помощью портала предварительной версии Azure](../../../virtual-network/virtual-networks-create-vnet-classic-pportal.md).      

<table>
<tr><th>Имя атрибута    </th><th>Значение    </th><th>Примечания</th></tr>
<tr><td>Name (Имя)    </td><td>vnet-cass-east-us</td><td></td></tr>
<tr><td>Регион    </td><td>Восток США</td><td></td></tr>
<tr><td>DNS-серверы        </td><td></td><td>Пропустите этот атрибут, поскольку мы не используем DNS-сервер</td></tr>
<tr><td>Настройка VPN-подключения типа "точка-сеть"</td><td></td><td>        Пропустите этот атрибут</td></tr>
<tr><td>"Настроить VPN типа "сеть-сеть"</td><td></td><td>        Пропустите этот атрибут</td></tr>
<tr><td>Пространство адресов    </td><td>10.2.0.0/16</td><td></td></tr>
<tr><td>Начальный IP-адрес    </td><td>10.2.0.0    </td><td></td></tr>
<tr><td>CIDR    </td><td>/16 (65531)</td><td></td></tr>
</table>

Добавьте следующие подсети.

<table>
<tr><th>Name (Имя)    </th><th>Начальный IP-адрес    </th><th>CIDR    </th><th>Примечания</th></tr>
<tr><td>web    </td><td>10.2.1.0    </td><td>/24 (251)    </td><td>Подсеть для веб-фермы</td></tr>
<tr><td>data    </td><td>10.2.2.0    </td><td>/24 (251)    </td><td>Подсеть для узлов базы данных</td></tr>
</table>


### <a name="step-2-create-local-networks"></a>Шаг 2. Создание локальных сетей
Локальная сеть в виртуальной сети Azure — это пространство прокси-адресов, которое сопоставляется с удаленным сайтом, включая личное облако или другой регион Azure. Это пространство прокси-адресов привязывается к удаленному шлюзу для маршрутизации сети к нужным точкам назначения. Указания по установке подключения между виртуальными сетями см. в статье [Настройка подключения между виртуальными сетями для классической модели развертывания](../../../vpn-gateway/virtual-networks-configure-vnet-to-vnet-connection.md).

Создайте две локальных сети со следующими параметрами:

| Имя сети | Адрес VPN-шлюза | Пространство адресов | Примечания |
| --- | --- | --- | --- |
| hk-lnet-map-to-east-us |23.1.1.1 |10.2.0.0/16 |При создании локальной сети укажите замещающий адрес шлюза. Настоящий адрес указывается после создания шлюза. Убедитесь, что пространство адресов точно совпадает с соответствующей удаленной виртуальной сетью. В данном случае виртуальная сеть создается для восточного региона США. |
| hk-lnet-map-to-west-us |23.2.2.2 |10.1.0.0/16 |При создании локальной сети укажите замещающий адрес шлюза. Настоящий адрес указывается после создания шлюза. Убедитесь, что пространство адресов точно совпадает с соответствующей удаленной виртуальной сетью. В данном случае виртуальная сеть создается для западного региона США. |

### <a name="step-3-map-local-network-to-the-respective-vnets"></a>Шаг 3. Сопоставление "локальной" сети с соответствующими виртуальными сетями
На классическом портале Azure выберите каждую виртуальную сеть, щелкните «Настроить», установите флажок «Подключиться к локальной сети» и выберите локальные сети со следующими параметрами.

| Виртуальная сеть | Локальная сеть |
| --- | --- |
| hk-vnet-west-us |hk-lnet-map-to-east-us |
| hk-vnet-east-us |hk-lnet-map-to-west-us |

### <a name="step-4-create-gateways-on-vnet1-and-vnet2"></a>Шаг 4. Создание шлюзов для сетей VNET1 и VNET2
На панелях мониторинга обеих виртуальных сетей нажмите "СОЗДАТЬ ШЛЮЗ". Начнется процесс подготовки VPN-шлюза. Спустя несколько минут на панели мониторинга каждой виртуальной сети должен появиться настоящий адрес шлюза.

### <a name="step-5-update-local-networks-with-the-respective-gateway-addresses"></a>Шаг 5. Обновление "локальных" сетей соответствующими адресами шлюзов
Измените обе локальные сети, заменив подстановочные IP-адреса шлюза настоящим IP-адресами подготовленных шлюзов. Используйте следующие сопоставления:

<table>
<tr><th>Локальная сеть    </th><th>Шлюз виртуальной сети</th></tr>
<tr><td>hk-lnet-map-to-east-us </td><td>Шлюз сети hk-vnet-west-us</td></tr>
<tr><td>hk-lnet-map-to-west-us </td><td>Шлюз сети hk-vnet-east-us</td></tr>
</table>

### <a name="step-6-update-the-shared-key"></a>Шаг 6. Обновление общего ключа
Используйте следующий сценарий Powershell, чтобы обновить ключ IPSec каждого из VPN-шлюзов. Используйте один и тот же ключ для обоих шлюзов: Set-AzureVNetGatewayKey -VNetName hk-vnet-east-us -LocalNetworkSiteName hk-lnet-map-to-west-us -SharedKey D9E76BKK Set-AzureVNetGatewayKey -VNetName hk-vnet-west-us -LocalNetworkSiteName hk-lnet-map-to-east-us -SharedKey D9E76BKK.

### <a name="step-7-establish-the-vnet-to-vnet-connection"></a>Шаг 7. Установка подключения между виртуальными сетями
На классическом портале Azure используйте меню «ПАНЕЛЬ МОНИТОРИНГА» обеих виртуальных сетей, чтобы установить подключение типа «шлюз-шлюз». Используйте пункты меню "ПОДКЛЮЧЕНИЕ" в нижней панели инструментов. Спустя несколько минут на панели мониторинга должно появиться графическое представление сведений о подключении.

### <a name="step-8-create-the-virtual-machines-in-region-2"></a>Шаг 8. Создание виртуальных машин во втором регионе
Создайте образ Ubuntu, как при развертывании в первом регионе, выполнив те же действия, или скопируйте VHD-файл образа в учетную запись хранения Azure, расположенную во втором регионе, и создайте образ. Используя этот образ, создайте следующий список виртуальных машин в новой облачной службе hk-c-svc-east-us:

| Имя компьютера | Подсеть | IP-адрес | группа доступности; | Контроллер домена/стойка | Начальное значение? |
| --- | --- | --- | --- | --- | --- |
| hk-c1-east-us |data |10.2.2.4 |hk-c-aset-1 |dc =EASTUS rack =rack1 |Да |
| hk-c2-east-us |data |10.2.2.5 |hk-c-aset-1 |dc =EASTUS rack =rack1 |Нет |
| hk-c3-east-us |data |10.2.2.6 |hk-c-aset-1 |dc =EASTUS rack =rack2 |Да |
| hk-c5-east-us |data |10.2.2.8 |hk-c-aset-2 |dc =EASTUS rack =rack3 |Да |
| hk-c6-east-us |data |10.2.2.9 |hk-c-aset-2 |dc =EASTUS rack =rack3 |Нет |
| hk-c7-east-us |data |10.2.2.10 |hk-c-aset-2 |dc =EASTUS rack =rack4 |Да |
| hk-c8-east-us |data |10.2.2.11 |hk-c-aset-2 |dc =EASTUS rack =rack4 |Нет |
| hk-w1-east-us |web |10.2.1.4 |hk-w-aset-1 |Недоступно |Недоступно |
| hk-w2-east-us |web |10.2.1.5 |hk-w-aset-1 |Недоступно |Недоступно |

Выполните те же действия, что и в первом регионе, но используйте пространство адресов 10.2.xxx.xxx.

### <a name="step-9-configure-cassandra-on-each-vm"></a>Шаг 9. Настройка Cassandra на каждой виртуальной машине
Войдите на виртуальную машину и выполните следующие действия:

1. Отредактируйте файл $CASS_HOME/conf/cassandra-rackdc.properties, указав свойства центра обработки данных и стойки в следующем формате: dc =EASTUS rack =rack1.
2. Отредактируйте файл cassandra.yaml, чтобы настроить начальные узлы. Начальные значения: "10.1.2.4,10.1.2.6,10.1.2.8,10.1.2.10,10.2.2.4,10.2.2.6,10.2.2.8,10.2.2.10"

### <a name="step-10-start-cassandra"></a>Шаг 10. Запуск Cassandra
Войдите на каждую виртуальную машину и запустите Cassandra в фоновом режиме с помощью следующей команды: $CASS_HOME/bin/cassandra.

## <a name="test-the-multi-region-cluster"></a>Тестирование кластера с несколькими регионами
В данный момент выполнено развертывание Cassandra на 16 узлах: по 8 узлов в каждом регионе Azure. Эти узлы объединены в кластер с помощью общего имени кластера и конфигурации начальных узлов. Чтобы протестировать кластер, выполните следующие действия:

### <a name="step-1-get-the-internal-load-balancer-ip-for-both-the-regions-using-powershell"></a>Шаг 1. Получение IP-адреса внутренней подсистемы балансировки нагрузки для обоих регионов с помощью PowerShell
* Get-AzureInternalLoadbalancer -ServiceName "hk-c-svc-west-us"
* Get-AzureInternalLoadbalancer -ServiceName "hk-c-svc-east-us"  
  
    Обратите внимание на отображаемые IP-адреса (например, для западного региона — 10.1.2.101, а для восточного — 10.2.2.101).

### <a name="step-2-execute-the-following-in-the-west-region-after-logging-into-hk-w1-west-us"></a>Шаг 2. Выполнение команд в западном регионе после входа в hk-w1-west-us
1. Выполните команду $CASS_HOME/bin/cqlsh 10.1.2.101 9160
2. Выполните следующие команды CQL:
   
     CREATE KEYSPACE customers_ks   WITH REPLICATION = { 'class' : 'NetworkToplogyStrategy', 'WESTUS' : 3, 'EASTUS' : 3};   USE customers_ks;   CREATE TABLE Customers(customer_id int PRIMARY KEY, firstname text, lastname text);   INSERT INTO Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');   INSERT INTO Customers(customer_id, firstname, lastname) VALUES (2, 'Jane', 'Doe');   SELECT * FROM Customers;

На экране должно появиться примерно следующее:

| customer_id | firstname | Lastname |
| --- | --- | --- |
| 1 |Артем |Кузнецов |
| 2 |Ольга |Кузнецов |

### <a name="step-3-execute-the-following-in-the-east-region-after-logging-into-hk-w1-east-us"></a>Шаг 3. Выполнение команд в восточном регионе после входа в hk-w1-east-us
1. Выполните команду $CASS_HOME/bin/cqlsh 10.2.2.101 9160.
2. Выполните следующие команды CQL:
   
     USE customers_ks;   CREATE TABLE Customers(customer_id int PRIMARY KEY, firstname text, lastname text);   INSERT INTO Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');   INSERT INTO Customers(customer_id, firstname, lastname) VALUES (2, 'Jane', 'Doe');   SELECT * FROM Customers;

На экране должны появиться такие же данные, как и для западного региона:

| customer_id | firstname | Lastname |
| --- | --- | --- |
| 1 |Артем |Кузнецов |
| 2 |Ольга |Кузнецов |

Выполните еще несколько вставок и убедитесь, что они реплицируются в часть кластера west-us.

## <a name="test-cassandra-cluster-from-nodejs"></a>Тестирование кластера Cassandra из файла Node.js
Используя одну из виртуальных машин Linux, ранее созданных на уровне web, мы выполним простой сценарий Node.js, чтобы считать ранее вставленные данные

**Шаг 1. Установка Node.js и клиента Cassandra**

1. Установка Node.js и NPM
2. Установите пакет узлов "cassandra-client" с помощью NPM
3. Выполните следующий сценарий в окне командной строки, в которой отображается строка JSON полученных данных:
   
        var pooledCon = require('cassandra-client').PooledConnection;
        var ksName = "custsupport_ks";
        var cfName = "customers_cf";
        var hostList = ['internal_loadbalancer_ip:9160'];
        var ksConOptions = { hosts: hostList,
                             keyspace: ksName, use_bigints: false };
   
        function createKeyspace(callback){
           var cql = 'CREATE KEYSPACE ' + ksName + ' WITH strategy_class=SimpleStrategy AND strategy_options:replication_factor=1';
           var sysConOptions = { hosts: hostList,  
                                 keyspace: 'system', use_bigints: false };
           var con = new pooledCon(sysConOptions);
           con.execute(cql,[],function(err) {
           if (err) {
             console.log("Failed to create Keyspace: " + ksName);
             console.log(err);
           }
           else {
             console.log("Created Keyspace: " + ksName);
             callback(ksConOptions, populateCustomerData);
           }
           });
           con.shutdown();
        }
   
        function createColumnFamily(ksConOptions, callback){
          var params = ['customers_cf','custid','varint','custname',
                        'text','custaddress','text'];
          var cql = 'CREATE COLUMNFAMILY ? (? ? PRIMARY KEY,? ?, ? ?)';
        var con =  new pooledCon(ksConOptions);
          con.execute(cql,params,function(err) {
              if (err) {
                 console.log("Failed to create column family: " + params[0]);
                 console.log(err);
              }
              else {
                 console.log("Created column family: " + params[0]);
                 callback();
              }
          });
          con.shutdown();
        }
   
        //populate Data
        function populateCustomerData() {
           var params = ['John','Infinity Dr, TX', 1];
           updateCustomer(ksConOptions,params);
   
           params = ['Tom','Fermat Ln, WA', 2];
           updateCustomer(ksConOptions,params);
        }
   
        //update will also insert the record if none exists
        function updateCustomer(ksConOptions,params)
        {
          var cql = 'UPDATE customers_cf SET custname=?,custaddress=? where custid=?';
          var con = new pooledCon(ksConOptions);
          con.execute(cql,params,function(err) {
              if (err) console.log(err);
              else console.log("Inserted customer : " + params[0]);
          });
          con.shutdown();
        }
   
        //read the two rows inserted above
        function readCustomer(ksConOptions)
        {
          var cql = 'SELECT * FROM customers_cf WHERE custid IN (1,2)';
          var con = new pooledCon(ksConOptions);
          con.execute(cql,[],function(err,rows) {
              if (err)
                 console.log(err);
              else
                 for (var i=0; i<rows.length; i++)
                    console.log(JSON.stringify(rows[i]));
            });
           con.shutdown();
        }
   
        //exectue the code
        createKeyspace(createColumnFamily);
        readCustomer(ksConOptions)

## <a name="conclusion"></a>Заключение
Microsoft Azure — это гибкая платформа, которая позволяет запускать как программы Майкрософт, так и ПО с открытым кодом, как показано в этом упражнении. Кластеры Cassandra с высокой доступностью можно развернуть в одном центре обработки данных, распределяя узлы кластера между несколькими доменами сбоя. Кластеры Cassandra также можно развертывать в нескольких географически разделенных регионах Azure для отказоустойчивых систем. Совместное использование Azure и Cassandra позволяет создавать масштабируемые облачные службы с высокой доступностью и возможностью аварийного восстановления, необходимые для современных служб Интернета.  

## <a name="references"></a>Ссылки
* [http://cassandra.apache.org](http://cassandra.apache.org)
* [http://www.datastax.com](http://www.datastax.com)
* [http://www.nodejs.org](http://www.nodejs.org)

