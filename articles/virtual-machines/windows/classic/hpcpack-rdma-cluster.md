---
title: "Настройка кластера Windows RDMA для запуска приложений MPI | Документация Майкрософт"
description: "Создание кластера Windows HPC Pack с виртуальными машинами размеров H16r, H16mr, A8 или A9, чтобы использовать сеть Azure RDMA для запуска приложений MPI."
services: virtual-machines-windows
documentationcenter: 
author: dlepow
manager: timlt
editor: 
tags: azure-service-management,hpc-pack
ms.assetid: 7d9f5bc8-012f-48dd-b290-db81c7592215
ms.service: virtual-machines-windows
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: vm-windows
ms.workload: big-compute
ms.date: 06/01/2017
ms.author: danlep
ms.openlocfilehash: 19be1d693fe13af0f6c1ab0cb6f7bc829b9fad5a
ms.sourcegitcommit: 6699c77dcbd5f8a1a2f21fba3d0a0005ac9ed6b7
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/11/2017
---
# <a name="set-up-a-windows-rdma-cluster-with-hpc-pack-to-run-mpi-applications"></a>Настройка кластера RDMA в Windows с помощью пакета HPC для запуска приложений MPI
Настройте кластер Linux RDMA в Azure с [пакетом Microsoft HPC](https://technet.microsoft.com/library/cc514029) и [виртуальными машинами серии H или серии A для ресурсоемких вычислений](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json) для параллельного выполнения приложений с интерфейсом MPI. Если настроить узлы с поддержкой RDMA под управлением Windows Server в кластере пакета HPC, приложения MPI будут эффективно взаимодействовать по сети с низкой задержкой и высокой пропускной способностью в Azure, основанной на технологии удаленного доступа к памяти (RDMA).

Если требуется применить рабочие нагрузки MPI на виртуальных машинах под управлением Linux, получающих доступ к сети RDMA в Azure, см. раздел [Настройка кластера Linux RDMA для запуска приложений MPI](../../linux/classic/rdma-cluster.md).

## <a name="hpc-pack-cluster-deployment-options"></a>Варианты развертывания кластера пакета HPC
Пакет Microsoft HPC — это бесплатное средство для создания кластеров HPC в локальной сети или в Azure, в которых могут выполняться приложения HPC для Windows или Linux. В пакет HPC входит среда выполнения для реализации интерфейса передачи сообщений для Windows (MS-MPI). При использовании пакета HPC с экземплярами с поддержкой RDMA под управлением поддерживаемой операционной системы Windows Server этот пакет позволяет эффективно выполнять приложения MPI для Windows с доступом к сети Azure RDMA. 

В этой статье представлены два скрипта и ссылки на подробные руководства по настройке кластера Windows RDMA с помощью пакета Microsoft HPC. 

* Сценарий 1. Развертывание экземпляров рабочих ролей для ресурсоемких вычислений (PaaS)
* Сценарий 2. Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)

Общие предварительные требования для использования ресурсоемких экземпляров с Windows описаны в статье [Размеры виртуальных машин, оптимизированных для высокопроизводительных вычислений](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).

## <a name="scenario-1-deploy-compute-intensive-worker-role-instances-paas"></a>Сценарий 1. Развертывание экземпляров рабочих ролей для ресурсоемких вычислений (PaaS)
Из существующего кластера HPC Pack добавьте дополнительные вычислительные ресурсы в экземплярах рабочих ролей Azure (узлах Azure), запускаемых в облачной службе (PaaS). Эта функция, называемая также «ускорением в Azure» из пакета HPC, поддерживает определенный диапазон размеров для экземпляров рабочих ролей. При добавлении узлов Azure укажите один из размеров с поддержкой RDMA.

Ниже описаны рекомендации и действия для расширения существующего (как правило, локального) кластера с помощью экземпляров Azure с поддержкой RDMA. Аналогичные процедуры позволяют добавлять экземпляры рабочих ролей в головной узел HPC Pack, который развертывается на виртуальной машине Azure.

> [!NOTE]
> Учебник по ускорению в Azure с помощью пакета HPC см. в разделе [Настройка гибридного кластера с пакетом HPC](../../../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md). Обратите внимание на рекомендации в описанных ниже шагах, которые применяются к узлам Azure с поддержкой RDMA.
> 
> 

![Ускорение в Azure][burst]

### <a name="steps"></a>Действия
1. **Развертывание и настройка головного узла HPC Pack 2012 R2.**
   
    Загрузите новейшую версию пакета установки HPC Pack из [Центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922). Требования и инструкции для подготовки к расширению Azure см. в статье [Burst to Azure Worker Instances with Microsoft HPC Pack](https://technet.microsoft.com/library/gg481749.aspx) (Расширение рабочих экземпляров Azure с помощью пакета Microsoft HPC).
2. **Настройка сертификата управления в подписке Azure.**
   
    Настройте сертификат для обеспечения безопасного соединения между головным узлом и Azure. Параметры и процедуры см. в разделе [Сценарии настройки сертификата управления Azure для пакета HPC](http://technet.microsoft.com/library/gg481759.aspx). Для тестового развертывания пакет HPC устанавливает сертификат управления Microsoft HPC Azure по умолчанию, который вы можете быстро отправить в подписку Azure.
3. **Создание новой облачной службы и учетной записи хранения.**
   
    С помощью портала Azure создайте облачную службу и учетную запись хранения для развертывания в регионе, в котором доступны экземпляры с поддержкой RDMA.
4. **Создание шаблона узла Azure.**
   
    Используйте мастер создания шаблона узла в диспетчере кластеров HPC. Пошаговые инструкции см. в разделе [Создание шаблона узла Azure](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) статьи "Шаги по развертыванию узлов Azure с помощью пакета Microsoft HPC".
   
    Для первоначальных тестов рекомендуется настроить ручную политику доступности в шаблоне.
5. **Добавление узлов в кластер.**
   
    Используйте мастер добавления узла в диспетчере кластеров HPC. Дополнительные сведения см. в разделе [Добавление узлов Azure в кластер HPC Windows](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).
   
    При указании размера узлов выберите один из размеров экземпляров с поддержкой RDMA.
   
   > [!NOTE]
   > При каждом расширении развертывания Azure с помощью вычислений пакет HPC автоматически развертывает не менее 2 экземпляров с поддержкой RDMA (например A8) в качестве прокси-узлов (в дополнение к указанным вами экземплярам рабочих ролей Azure). Прокси-узлы используют ядра, привязанные к подписке, за которые взимается плата на тех же условиях, что и за экземпляры рабочих ролей Azure.
   > 
   > 
6. **Запуск (подготовка) узлов и их подключение к сети для выполнения заданий.**
   
    Выберите узлы и используйте действие **Запуск** в диспетчере кластеров HPC. По завершении подготовки выберите узлы и используйте действие **Перевести в оперативный режим** в диспетчере кластеров HPC. После этого узлы будут готовы к выполнению заданий.
7. **Отправка заданий в кластер.**
   
   Используйте средства отправки заданий HPC Pack для выполнения заданий кластера. См. раздел [Пакет Microsoft HPC: управление заданиями](http://technet.microsoft.com/library/jj899585.aspx).
8. **Остановка (отзыв) узлов**
   
   После завершения выполнения заданий отключите узлы и используйте действие **Остановить** в диспетчере кластеров HPC.

## <a name="scenario-2-deploy-compute-nodes-in-compute-intensive-vms-iaas"></a>Сценарий 2. Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)
В этом сценарии головной узел и вычислительные узлы кластера пакета HPC развертываются на виртуальных машинах в виртуальной сети Azure. Пакет HPC предоставляет несколько [вариантов развертывания на виртуальных машинах Azure](../../linux/hpcpack-cluster-options.md), в том числе сценарии автоматизированного развертывания и шаблоны быстрого запуска Azure. Следующие рекомендации и действия помогут автоматизировать развертывание кластера пакета HPC 2012 R2 в Azure с помощью [сценария развертывания IaaS пакета HPC](hpcpack-cluster-powershell-script.md).

![Кластер на виртуальных машинах Azure][iaas]

### <a name="steps"></a>Действия
1. **Создание головного узла кластера и виртуальных машин вычислительных узлов путем выполнения скрипта развертывания IaaS с пакетом HPC на клиентском компьютере.**
   
    Загрузите пакет скрипта развертывания IaaS для пакета HPC Pack из [Центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922).
   
    Чтобы подготовить клиентский компьютер, создайте файл конфигурации скрипта и запустите скрипт. См. раздел [Создание кластера HPC с помощью скрипта развертывания IaaS с пакетом HPC](hpcpack-cluster-powershell-script.md). 
   
    При развертывании вычислительных узлов с поддержкой RDMA обратите внимание на следующие дополнительные рекомендации.
   
   * **Виртуальная сеть.** Укажите новую виртуальную сеть в регионе, в котором доступны нужные размеры экземпляров с поддержкой RDMA.
   * **Операционная система Windows Server.** Для поддержки соединения RDMA укажите операционную систему Windows Server 2012 R2 или Windows Server 2012 для виртуальных машин вычислительных узлов.
   * **Облачные службы.** Мы рекомендуем развертывать головной узел в одной облачной службе, а вычислительные узлы — в другой облачной службе.
   * **Размер головного узла.** Для нашего сценария головной узел должен иметь по меньшей мере размер A4 (очень крупный).
   * **Расширение HpcVmDrivers.** Сценарий развертывания автоматически устанавливает агент ВМ Azure и расширение HpcVmDrivers, когда вы развертываете вычислительные узлы размера A8 или A9, в которых используется ОС Windows Server. HpcVmDrivers устанавливает драйверы на виртуальные машины вычислительного узла, чтобы они могли подключаться к сети RDMA. На виртуальных машинах серии H с поддержкой RDMA необходимо вручную установить расширение HpcVmDrivers. См. [Размеры виртуальных машин, оптимизированных для высокопроизводительных вычислений](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).
   * **Конфигурация сети кластера.** Сценарий развертывания автоматически настраивает кластер пакета HPC с топологией 5 (все узлы в корпоративной сети). Данная топология является обязательной для всех развертываний кластера пакета HPC на виртуальных машинах. В дальнейшем не следует изменять топологию сети кластера.
2. **Перевод вычислительных узлов в оперативный режим для запуска заданий.**
   
    Выберите узлы и используйте действие **Перевести в оперативный режим** в диспетчере кластеров HPC. После этого узлы будут готовы к выполнению заданий.
3. **Отправка заданий в кластер.**
   
    Подключитесь к головному узлу для отправки заданий или настройте для этого локальный компьютер. Сведения см. в разделе [Отправка заданий в кластер HPC в Azure](../../virtual-machines-windows-hpcpack-cluster-submit-jobs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).
4. **Отключение узлов и их остановка (отзыв).**
   
    После выполнения заданий отключите узлы в диспетчере кластеров HPC. Затем с помощью средств управления Azure завершите их работу.

## <a name="run-mpi-applications-on-the-cluster"></a>Запуск приложений MPI в кластере
### <a name="example-run-mpipingpong-on-an-hpc-pack-cluster"></a>Пример. Выполнение команды mpipingpong в кластере HPC Pack
Чтобы проверить развертывание экземпляров с поддержкой RDMA, которое выполнялось с помощью пакета HPC, запустите в кластере команду **mpipingpong** из этого пакета. **mpipingpong** многократно отправляет пакеты данных из одного спаренного узла в другой, чтобы вычислить задержки и изменить пропускную способность и статистику для сети приложений с поддержкой RDMA. В этом примере показана типичная схема запуска задания MPI (в данном случае **mpipingpong**) с помощью команды кластера **mpiexec**.

В этом примере предполагается, что вы добавили узлы Azure в конфигурации "расширение в Azure" ([Сценарий 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-\(PaaS\) in this article)). Если пакет HPC Pack развернут в кластере виртуальных машин Azure, необходимо изменить синтаксис команды, чтобы указать другую группу узлов и задать дополнительные переменные среды для направления сетевого трафика по сети RDMA.

Чтобы выполнить команду mpipingpong в кластере:

1. На головном узле или на соответствующим образом настроенном клиентском компьютере откройте командную строку.
2. Чтобы оценить задержку между парами узлов в развертывании ускорения Azure, состоящем из четырех узлов, запустите задание выполнения команды mpipingpong с небольшим размером пакетов и большим количеством итераций. Для этого введите такую команду:
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 1:100000 -op -s nul
    ```
   
    Команда вернет идентификатор запущенного задания.
   
    Если кластер HPC Pack развернут на виртуальных машинах Azure, укажите группу узлов, которая содержит виртуальные машины вычислительных узлов, развернутые в одной облачной службе, и измените команду **mpiexec** следующим образом:
   
    ```Command
    job submit /nodegroup:vmcomputenodes /numnodes:4 mpiexec -c 1 -affinity -env MSMPI_DISABLE_SOCK 1 -env MSMPI_PRECONNECT all -env MPICH_NETMASK 172.16.0.0/255.255.0.0 mpipingpong -p 1:100000 -op -s nul
    ```
3. Чтобы после завершения задания просмотреть выходные данные (в данном случае — выходные данные задачи 1 задания), введите следующую команду
   
    ```Command
    task view <JobID>.1
    ```
   
    где &lt;*JobID*&gt; — идентификатор запущенного задания.
   
    Выходные данные будут содержать сведения о задержке, подобные следующим.
   
    ![Задержка проверки связи][pingpong1]
4. Чтобы оценить пропускную способность между парами узлов расширения Azure, запустите задание выполнения команды **mpipingpong** с большим размером пакетов и небольшим количеством итераций. Для этого введите следующую команду:
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 4000000:1000 -op -s nul
    ```
   
    Команда вернет идентификатор запущенного задания.
   
    В кластере HPC Pack, развернутом на виртуальных машинах Azure, измените команду, как указано в шаге 2.
5. Чтобы после завершения задания просмотреть выходные данные (в данном случае — выходные данные задачи 1 задания), введите следующую команду:
   
    ```Command
    task view <JobID>.1
    ```
   
   Выходные данные будут содержать сведения о пропускной способности, подобные следующим.
   
   ![Пропускная способность при «пинг-понговой» передаче данных][pingpong2]

### <a name="mpi-application-considerations"></a>Рекомендации по приложениям MPI
Ниже приведены рекомендации по запуску в Azure приложений MPI с пакетом HPC. Некоторые из них применимы только к развертыванию узлов Azure (экземпляров рабочих ролей, добавленных в конфигурации «ускорение в Azure»).

* Экземпляры рабочих ролей в облачной службе периодически повторно подготавливаются без уведомления системой Azure (например, для обслуживания системы или в случае сбоя экземпляра). Если экземпляр повторно подготавливается в то время, когда он выполняет задание MPI, экземпляр теряет все свои данные и возвращается в исходное состояние, что может привести к сбою задания MPI. Чем больше узлов используется для одного задания MPI и чем дольше задание выполняется, тем вероятнее, что один из экземпляров будет подготовлен повторно во время выполнения задания. Это следует учитывать также в случае назначения одного узла в развертывании в качестве файлового сервера.
* Для запуска заданий MPI в Azure не обязательно использовать экземпляры с поддержкой RDMA. Вы можете использовать экземпляр любого размера, поддерживаемый пакетом HPC. Тем не менее экземпляры с поддержкой RDMA рекомендуются для относительно крупных заданий MPI, чувствительных к задержке и пропускной способности сети, соединяющей узлы. Если для запуска заданий MPI, чувствительных к задержке и пропускной способности, вы будете использовать другие размеры экземпляров, рекомендуем запускать мелкие задания, в которых каждая отдельная задача выполняется только на нескольких узлах.
* Приложения, развернутые в экземплярах Azure, подлежат условиям лицензирования, связанным с приложением. Проконсультируйтесь с поставщиками всех коммерческих приложений насчет лицензирования или иных ограничений на запуск приложений в облаке. Не все поставщики предлагают лицензирование с оплатой по мере использования.
* Экземплярам Azure требуется дополнительная настройка для доступа к локальным узлам, общим ресурсам и серверам лицензий. Например, чтобы включить для узлов Azure доступ к локальному серверу лицензий, вы можете настроить виртуальную сеть Azure «сеть — сеть».
* Чтобы запустить приложение MPI в экземплярах Azure, следует зарегистрировать приложение MPI в брандмауэре Windows в экземплярах, запустив команду **hpcfwutil** . Это позволит осуществлять обмен данными MPI через порт, который динамически назначается брандмауэром.
  
  > [!NOTE]
  > Для развертываний ускорения в Azure можно также настроить команду исключения брандмауэра с целью автоматического запуска на всех новых узлах Azure, которые добавляются к кластеру. Запустив команду **hpcfwutil** и убедившись в том, что ваше приложение работает, добавьте команду в скрипт запуска для узлов Azure. Дополнительные сведения см. в статье [Использование скрипта запуска для узлов Azure](https://technet.microsoft.com/library/jj899632.aspx).
  > 
  > 
* Пакет HPC использует переменную среды кластера CCP_MPI_NETMASK для указания диапазона допустимых адресов для связи MPI. Начиная с пакета HPC Pack 2012 R2, переменная среды кластера CCP_MPI_NETMASK влияет только на связь MPI между вычислительными узлами кластера, присоединенными к домену (локально или на виртуальных машинах Azure). Переменная игнорируется узлами, добавленными в рамках ускорения в конфигурацию Azure.
* Задания MPI не могут выполняться между экземплярами Azure, развернутыми в разных облачных службах (например, в развертываниях ускорения в Azure с разными шаблонами узлов или на вычислительных узлах виртуальных машин Azure, развернутых в нескольких облачных службах). При наличии нескольких развертываний узлов Azure, запущенных с разными шаблонами узлов, задание MPI должно выполняться только на одном наборе узлов Azure.
* При добавлении узлов Azure в кластер и переводе их в оперативный режим служба планировщика заданий HPC немедленно пытается запустить задания на узлах. Если в Azure можно запустить только часть имеющейся рабочей нагрузки, не забудьте обновить или создать шаблоны заданий, чтобы таким образом определить типы заданий, которые можно выполнять в Azure. Например, чтобы на узлах Azure выполнялись только те задания, которые были отправлены с использованием шаблона задания, добавьте свойство «Группы узлов» в шаблон задания и выберите AzureNodes в качестве обязательного значения. Для создания настраиваемых групп узлов Azure можно использовать командлет PowerShell Add-HpcGroup HPC.

## <a name="next-steps"></a>Дальнейшие действия
* В качестве альтернативы пакету HPC для выполнения приложений MPI в управляемых пулах вычислительных узлов в Azure можно использовать пакетную службу Azure. Ознакомьтесь со статьей [Использование задач с несколькими экземплярами для запуска приложений с интерфейсом передачи сообщений в пакетной службе Azure](../../../batch/batch-mpi.md).
* Если требуется запускать приложения MPI для Linux, получающие доступ к сети RDMA в Azure, см. раздел [Настройка кластера Linux RDMA для запуска приложений MPI](../../linux/classic/rdma-cluster.md).

<!--Image references-->
[burst]:media/hpcpack-rdma-cluster/burst.png
[iaas]:media/hpcpack-rdma-cluster/iaas.png
[pingpong1]:media/hpcpack-rdma-cluster/pingpong1.png
[pingpong2]:media/hpcpack-rdma-cluster/pingpong2.png
