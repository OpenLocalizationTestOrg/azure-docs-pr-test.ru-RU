---
title: "Настройка кластера Windows RDMA для запуска приложений MPI | Документация Майкрософт"
description: "Создание кластера Windows HPC Pack с виртуальными машинами размеров H16r, H16mr, A8 или A9, чтобы использовать сеть Azure RDMA для запуска приложений MPI."
services: virtual-machines-windows
documentationcenter: 
author: dlepow
manager: timlt
editor: 
tags: azure-service-management,hpc-pack
ms.assetid: 7d9f5bc8-012f-48dd-b290-db81c7592215
ms.service: virtual-machines-windows
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: vm-windows
ms.workload: big-compute
ms.date: 06/01/2017
ms.author: danlep
ms.openlocfilehash: 19be1d693fe13af0f6c1ab0cb6f7bc829b9fad5a
ms.sourcegitcommit: 02e69c4a9d17645633357fe3d46677c2ff22c85a
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/03/2017
---
# <a name="set-up-a-windows-rdma-cluster-with-hpc-pack-to-run-mpi-applications"></a><span data-ttu-id="20789-103">Настройка кластера RDMA в Windows с помощью пакета HPC для запуска приложений MPI</span><span class="sxs-lookup"><span data-stu-id="20789-103">Set up a Windows RDMA cluster with HPC Pack to run MPI applications</span></span>
<span data-ttu-id="20789-104">Настройте кластер Linux RDMA в Azure с [пакетом Microsoft HPC](https://technet.microsoft.com/library/cc514029) и [виртуальными машинами серии H или серии A для ресурсоемких вычислений](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json) для параллельного выполнения приложений с интерфейсом MPI.</span><span class="sxs-lookup"><span data-stu-id="20789-104">Set up a Windows RDMA cluster in Azure with [Microsoft HPC Pack](https://technet.microsoft.com/library/cc514029) and [High performance compute VM sizes](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json) to run parallel Message Passing Interface (MPI) applications.</span></span> <span data-ttu-id="20789-105">Если настроить узлы с поддержкой RDMA под управлением Windows Server в кластере пакета HPC, приложения MPI будут эффективно взаимодействовать по сети с низкой задержкой и высокой пропускной способностью в Azure, основанной на технологии удаленного доступа к памяти (RDMA).</span><span class="sxs-lookup"><span data-stu-id="20789-105">When you set up RDMA-capable, Windows Server-based nodes in an HPC Pack cluster, MPI applications communicate efficiently over a low latency, high throughput network in Azure that is based on remote direct memory access (RDMA) technology.</span></span>

<span data-ttu-id="20789-106">Если требуется применить рабочие нагрузки MPI на виртуальных машинах под управлением Linux, получающих доступ к сети RDMA в Azure, см. раздел [Настройка кластера Linux RDMA для запуска приложений MPI](../../linux/classic/rdma-cluster.md).</span><span class="sxs-lookup"><span data-stu-id="20789-106">If you want to run MPI workloads on Linux VMs that access the Azure RDMA network, see [Set up a Linux RDMA cluster to run MPI applications](../../linux/classic/rdma-cluster.md).</span></span>

## <a name="hpc-pack-cluster-deployment-options"></a><span data-ttu-id="20789-107">Варианты развертывания кластера пакета HPC</span><span class="sxs-lookup"><span data-stu-id="20789-107">HPC Pack cluster deployment options</span></span>
<span data-ttu-id="20789-108">Пакет Microsoft HPC — это бесплатное средство для создания кластеров HPC в локальной сети или в Azure, в которых могут выполняться приложения HPC для Windows или Linux.</span><span class="sxs-lookup"><span data-stu-id="20789-108">Microsoft HPC Pack is a tool provided at no additional cost to create HPC clusters on-premises or in Azure to run Windows or Linux HPC applications.</span></span> <span data-ttu-id="20789-109">В пакет HPC входит среда выполнения для реализации интерфейса передачи сообщений для Windows (MS-MPI).</span><span class="sxs-lookup"><span data-stu-id="20789-109">HPC Pack includes a runtime environment for the Microsoft implementation of the Message Passing Interface for Windows (MS-MPI).</span></span> <span data-ttu-id="20789-110">При использовании пакета HPC с экземплярами с поддержкой RDMA под управлением поддерживаемой операционной системы Windows Server этот пакет позволяет эффективно выполнять приложения MPI для Windows с доступом к сети Azure RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-110">When used with RDMA-capable instances running a supported Windows Server operating system, HPC Pack provides an efficient option to run Windows MPI applications that access the Azure RDMA network.</span></span> 

<span data-ttu-id="20789-111">В этой статье представлены два скрипта и ссылки на подробные руководства по настройке кластера Windows RDMA с помощью пакета Microsoft HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-111">This article introduces two scenarios and links to detailed guidance to set up a Windows RDMA cluster with Microsoft HPC Pack.</span></span> 

* <span data-ttu-id="20789-112">Сценарий 1.</span><span class="sxs-lookup"><span data-stu-id="20789-112">Scenario 1.</span></span> <span data-ttu-id="20789-113">Развертывание экземпляров рабочих ролей для ресурсоемких вычислений (PaaS)</span><span class="sxs-lookup"><span data-stu-id="20789-113">Deploy compute-intensive worker role instances (PaaS)</span></span>
* <span data-ttu-id="20789-114">Сценарий 2.</span><span class="sxs-lookup"><span data-stu-id="20789-114">Scenario 2.</span></span> <span data-ttu-id="20789-115">Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)</span><span class="sxs-lookup"><span data-stu-id="20789-115">Deploy compute nodes in compute-intensive VMs (IaaS)</span></span>

<span data-ttu-id="20789-116">Общие предварительные требования для использования ресурсоемких экземпляров с Windows описаны в статье [Размеры виртуальных машин, оптимизированных для высокопроизводительных вычислений](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span><span class="sxs-lookup"><span data-stu-id="20789-116">For general prerequisites to use compute-intensive instances with Windows, see [High performance compute VM sizes](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>

## <a name="scenario-1-deploy-compute-intensive-worker-role-instances-paas"></a><span data-ttu-id="20789-117">Сценарий 1. Развертывание экземпляров рабочих ролей для ресурсоемких вычислений (PaaS)</span><span class="sxs-lookup"><span data-stu-id="20789-117">Scenario 1: Deploy compute-intensive worker role instances (PaaS)</span></span>
<span data-ttu-id="20789-118">Из существующего кластера HPC Pack добавьте дополнительные вычислительные ресурсы в экземплярах рабочих ролей Azure (узлах Azure), запускаемых в облачной службе (PaaS).</span><span class="sxs-lookup"><span data-stu-id="20789-118">From an existing HPC Pack cluster, add extra compute resources in Azure worker role instances (Azure nodes) running in a cloud service (PaaS).</span></span> <span data-ttu-id="20789-119">Эта функция, называемая также «ускорением в Azure» из пакета HPC, поддерживает определенный диапазон размеров для экземпляров рабочих ролей.</span><span class="sxs-lookup"><span data-stu-id="20789-119">This feature, also called “burst to Azure” from HPC Pack, supports a range of sizes for the worker role instances.</span></span> <span data-ttu-id="20789-120">При добавлении узлов Azure укажите один из размеров с поддержкой RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-120">When adding the Azure nodes, specify one of the RDMA-capable sizes.</span></span>

<span data-ttu-id="20789-121">Ниже описаны рекомендации и действия для расширения существующего (как правило, локального) кластера с помощью экземпляров Azure с поддержкой RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-121">Following are considerations and steps to burst to RDMA-capable Azure instances from an existing (typically on-premises) cluster.</span></span> <span data-ttu-id="20789-122">Аналогичные процедуры позволяют добавлять экземпляры рабочих ролей в головной узел HPC Pack, который развертывается на виртуальной машине Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-122">Use similar procedures to add worker role instances to an HPC Pack head node that is deployed in an Azure VM.</span></span>

> [!NOTE]
> <span data-ttu-id="20789-123">Учебник по ускорению в Azure с помощью пакета HPC см. в разделе [Настройка гибридного кластера с пакетом HPC](../../../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md).</span><span class="sxs-lookup"><span data-stu-id="20789-123">For a tutorial to burst to Azure with HPC Pack, see [Set up a hybrid cluster with HPC Pack](../../../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md).</span></span> <span data-ttu-id="20789-124">Обратите внимание на рекомендации в описанных ниже шагах, которые применяются к узлам Azure с поддержкой RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-124">Note the considerations in the following steps that apply specifically to RDMA-capable Azure nodes.</span></span>
> 
> 

![Ускорение в Azure][burst]

### <a name="steps"></a><span data-ttu-id="20789-126">Действия</span><span class="sxs-lookup"><span data-stu-id="20789-126">Steps</span></span>
1. <span data-ttu-id="20789-127">**Развертывание и настройка головного узла HPC Pack 2012 R2.**</span><span class="sxs-lookup"><span data-stu-id="20789-127">**Deploy and configure an HPC Pack 2012 R2 head node**</span></span>
   
    <span data-ttu-id="20789-128">Загрузите новейшую версию пакета установки HPC Pack из [Центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922).</span><span class="sxs-lookup"><span data-stu-id="20789-128">Download the latest HPC Pack installation package from the [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span></span> <span data-ttu-id="20789-129">Требования и инструкции для подготовки к расширению Azure см. в статье [Burst to Azure Worker Instances with Microsoft HPC Pack](https://technet.microsoft.com/library/gg481749.aspx) (Расширение рабочих экземпляров Azure с помощью пакета Microsoft HPC).</span><span class="sxs-lookup"><span data-stu-id="20789-129">For requirements and instructions to prepare for an Azure burst deployment, see [Burst to Azure Worker Instances with Microsoft HPC Pack](https://technet.microsoft.com/library/gg481749.aspx).</span></span>
2. <span data-ttu-id="20789-130">**Настройка сертификата управления в подписке Azure.**</span><span class="sxs-lookup"><span data-stu-id="20789-130">**Configure a management certificate in the Azure subscription**</span></span>
   
    <span data-ttu-id="20789-131">Настройте сертификат для обеспечения безопасного соединения между головным узлом и Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-131">Configure a certificate to secure the connection between the head node and Azure.</span></span> <span data-ttu-id="20789-132">Параметры и процедуры см. в разделе [Сценарии настройки сертификата управления Azure для пакета HPC](http://technet.microsoft.com/library/gg481759.aspx).</span><span class="sxs-lookup"><span data-stu-id="20789-132">For options and procedures, see [Scenarios to Configure the Azure Management Certificate for HPC Pack](http://technet.microsoft.com/library/gg481759.aspx).</span></span> <span data-ttu-id="20789-133">Для тестового развертывания пакет HPC устанавливает сертификат управления Microsoft HPC Azure по умолчанию, который вы можете быстро отправить в подписку Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-133">For test deployments, HPC Pack installs a Default Microsoft HPC Azure Management Certificate you can quickly upload to your Azure subscription.</span></span>
3. <span data-ttu-id="20789-134">**Создание новой облачной службы и учетной записи хранения.**</span><span class="sxs-lookup"><span data-stu-id="20789-134">**Create a new cloud service and a storage account**</span></span>
   
    <span data-ttu-id="20789-135">С помощью портала Azure создайте облачную службу и учетную запись хранения для развертывания в регионе, в котором доступны экземпляры с поддержкой RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-135">Use the Azure portal to create a cloud service and a storage account for the deployment in a region where the RDMA-capable instances are available.</span></span>
4. <span data-ttu-id="20789-136">**Создание шаблона узла Azure.**</span><span class="sxs-lookup"><span data-stu-id="20789-136">**Create an Azure node template**</span></span>
   
    <span data-ttu-id="20789-137">Используйте мастер создания шаблона узла в диспетчере кластеров HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-137">Use the Create Node Template Wizard in HPC Cluster Manager.</span></span> <span data-ttu-id="20789-138">Пошаговые инструкции см. в разделе [Создание шаблона узла Azure](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) статьи "Шаги по развертыванию узлов Azure с помощью пакета Microsoft HPC".</span><span class="sxs-lookup"><span data-stu-id="20789-138">For steps, see [Create an Azure node template](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) in “Steps to Deploy Azure Nodes with Microsoft HPC Pack”.</span></span>
   
    <span data-ttu-id="20789-139">Для первоначальных тестов рекомендуется настроить ручную политику доступности в шаблоне.</span><span class="sxs-lookup"><span data-stu-id="20789-139">For initial tests, we suggest configuring a manual availability policy in the template.</span></span>
5. <span data-ttu-id="20789-140">**Добавление узлов в кластер.**</span><span class="sxs-lookup"><span data-stu-id="20789-140">**Add nodes to the cluster**</span></span>
   
    <span data-ttu-id="20789-141">Используйте мастер добавления узла в диспетчере кластеров HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-141">Use the Add Node Wizard in HPC Cluster Manager.</span></span> <span data-ttu-id="20789-142">Дополнительные сведения см. в разделе [Добавление узлов Azure в кластер HPC Windows](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).</span><span class="sxs-lookup"><span data-stu-id="20789-142">For more information, see [Add Azure Nodes to the Windows HPC Cluster](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).</span></span>
   
    <span data-ttu-id="20789-143">При указании размера узлов выберите один из размеров экземпляров с поддержкой RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-143">When specifying the size of the nodes, select one of the RDMA-capable instance sizes.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="20789-144">При каждом расширении развертывания Azure с помощью вычислений пакет HPC автоматически развертывает не менее 2 экземпляров с поддержкой RDMA (например A8) в качестве прокси-узлов (в дополнение к указанным вами экземплярам рабочих ролей Azure).</span><span class="sxs-lookup"><span data-stu-id="20789-144">In each burst to Azure deployment with the compute-intensive instances, HPC Pack automatically deploys a minimum of two RDMA-capable instances (such as A8) as proxy nodes, in addition to the Azure worker role instances you specify.</span></span> <span data-ttu-id="20789-145">Прокси-узлы используют ядра, привязанные к подписке, за которые взимается плата на тех же условиях, что и за экземпляры рабочих ролей Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-145">The proxy nodes use cores that are allocated to the subscription and incur charges along with the Azure worker role instances.</span></span>
   > 
   > 
6. <span data-ttu-id="20789-146">**Запуск (подготовка) узлов и их подключение к сети для выполнения заданий.**</span><span class="sxs-lookup"><span data-stu-id="20789-146">**Start (provision) the nodes and bring them online to run jobs**</span></span>
   
    <span data-ttu-id="20789-147">Выберите узлы и используйте действие **Запуск** в диспетчере кластеров HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-147">Select the nodes and use the **Start** action in HPC Cluster Manager.</span></span> <span data-ttu-id="20789-148">По завершении подготовки выберите узлы и используйте действие **Перевести в оперативный режим** в диспетчере кластеров HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-148">When provisioning is complete, select the nodes and use the **Bring Online** action in HPC Cluster Manager.</span></span> <span data-ttu-id="20789-149">После этого узлы будут готовы к выполнению заданий.</span><span class="sxs-lookup"><span data-stu-id="20789-149">The nodes are ready to run jobs.</span></span>
7. <span data-ttu-id="20789-150">**Отправка заданий в кластер.**</span><span class="sxs-lookup"><span data-stu-id="20789-150">**Submit jobs to the cluster**</span></span>
   
   <span data-ttu-id="20789-151">Используйте средства отправки заданий HPC Pack для выполнения заданий кластера.</span><span class="sxs-lookup"><span data-stu-id="20789-151">Use HPC Pack job submission tools to run cluster jobs.</span></span> <span data-ttu-id="20789-152">См. раздел [Пакет Microsoft HPC: управление заданиями](http://technet.microsoft.com/library/jj899585.aspx).</span><span class="sxs-lookup"><span data-stu-id="20789-152">See [Microsoft HPC Pack: Job Management](http://technet.microsoft.com/library/jj899585.aspx).</span></span>
8. <span data-ttu-id="20789-153">**Остановка (отзыв) узлов**</span><span class="sxs-lookup"><span data-stu-id="20789-153">**Stop (deprovision) the nodes**</span></span>
   
   <span data-ttu-id="20789-154">После завершения выполнения заданий отключите узлы и используйте действие **Остановить** в диспетчере кластеров HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-154">When you are done running jobs, take the nodes offline and use the **Stop** action in HPC Cluster Manager.</span></span>

## <a name="scenario-2-deploy-compute-nodes-in-compute-intensive-vms-iaas"></a><span data-ttu-id="20789-155">Сценарий 2. Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)</span><span class="sxs-lookup"><span data-stu-id="20789-155">Scenario 2: Deploy compute nodes in compute-intensive VMs (IaaS)</span></span>
<span data-ttu-id="20789-156">В этом сценарии головной узел и вычислительные узлы кластера пакета HPC развертываются на виртуальных машинах в виртуальной сети Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-156">In this scenario, you deploy the HPC Pack head node and cluster compute nodes on VMs in an Azure virtual network.</span></span> <span data-ttu-id="20789-157">Пакет HPC предоставляет несколько [вариантов развертывания на виртуальных машинах Azure](../../linux/hpcpack-cluster-options.md), в том числе сценарии автоматизированного развертывания и шаблоны быстрого запуска Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-157">HPC Pack provides several [deployment options in Azure VMs](../../linux/hpcpack-cluster-options.md), including automated deployment scripts and Azure quickstart templates.</span></span> <span data-ttu-id="20789-158">Следующие рекомендации и действия помогут автоматизировать развертывание кластера пакета HPC 2012 R2 в Azure с помощью [сценария развертывания IaaS пакета HPC](hpcpack-cluster-powershell-script.md).</span><span class="sxs-lookup"><span data-stu-id="20789-158">As an example, the following considerations and steps guide you to use the [HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md) to automate the deployment of an HPC Pack 2012 R2 cluster in Azure.</span></span>

![Кластер на виртуальных машинах Azure][iaas]

### <a name="steps"></a><span data-ttu-id="20789-160">Действия</span><span class="sxs-lookup"><span data-stu-id="20789-160">Steps</span></span>
1. <span data-ttu-id="20789-161">**Создание головного узла кластера и виртуальных машин вычислительных узлов путем выполнения скрипта развертывания IaaS с пакетом HPC на клиентском компьютере.**</span><span class="sxs-lookup"><span data-stu-id="20789-161">**Create a cluster head node and compute node VMs by running the HPC Pack IaaS deployment script on a client computer**</span></span>
   
    <span data-ttu-id="20789-162">Загрузите пакет скрипта развертывания IaaS для пакета HPC Pack из [Центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922).</span><span class="sxs-lookup"><span data-stu-id="20789-162">Download the HPC Pack IaaS Deployment Script package from the [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span></span>
   
    <span data-ttu-id="20789-163">Чтобы подготовить клиентский компьютер, создайте файл конфигурации скрипта и запустите скрипт. См. раздел [Создание кластера HPC с помощью скрипта развертывания IaaS с пакетом HPC](hpcpack-cluster-powershell-script.md).</span><span class="sxs-lookup"><span data-stu-id="20789-163">To prepare the client computer, create the script configuration file, and run the script, see [Create an HPC Cluster with the HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md).</span></span> 
   
    <span data-ttu-id="20789-164">При развертывании вычислительных узлов с поддержкой RDMA обратите внимание на следующие дополнительные рекомендации.</span><span class="sxs-lookup"><span data-stu-id="20789-164">To deploy RDMA-capable compute nodes, note the following additional considerations:</span></span>
   
   * <span data-ttu-id="20789-165">**Виртуальная сеть.** Укажите новую виртуальную сеть в регионе, в котором доступны нужные размеры экземпляров с поддержкой RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-165">**Virtual network**: Specify a new virtual network in a region in which the RDMA-capable instance size you want to use is available.</span></span>
   * <span data-ttu-id="20789-166">**Операционная система Windows Server.** Для поддержки соединения RDMA укажите операционную систему Windows Server 2012 R2 или Windows Server 2012 для виртуальных машин вычислительных узлов.</span><span class="sxs-lookup"><span data-stu-id="20789-166">**Windows Server operating system**: To support RDMA connectivity, specify a Windows Server 2012 R2 or Windows Server 2012 operating system for the compute node VMs.</span></span>
   * <span data-ttu-id="20789-167">**Облачные службы.** Мы рекомендуем развертывать головной узел в одной облачной службе, а вычислительные узлы — в другой облачной службе.</span><span class="sxs-lookup"><span data-stu-id="20789-167">**Cloud services**: We recommend deploying your head node in one cloud service and your compute nodes in a different cloud service.</span></span>
   * <span data-ttu-id="20789-168">**Размер головного узла.** Для нашего сценария головной узел должен иметь по меньшей мере размер A4 (очень крупный).</span><span class="sxs-lookup"><span data-stu-id="20789-168">**Head node size**: For this scenario, consider a size of at least A4 (Extra Large) for the head node.</span></span>
   * <span data-ttu-id="20789-169">**Расширение HpcVmDrivers.** Сценарий развертывания автоматически устанавливает агент ВМ Azure и расширение HpcVmDrivers, когда вы развертываете вычислительные узлы размера A8 или A9, в которых используется ОС Windows Server.</span><span class="sxs-lookup"><span data-stu-id="20789-169">**HpcVmDrivers extension**: The deployment script installs the Azure VM Agent and the HpcVmDrivers extension automatically when you deploy size A8 or A9 compute nodes with a Windows Server operating system.</span></span> <span data-ttu-id="20789-170">HpcVmDrivers устанавливает драйверы на виртуальные машины вычислительного узла, чтобы они могли подключаться к сети RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-170">HpcVmDrivers installs drivers on the compute node VMs so they can connect to the RDMA network.</span></span> <span data-ttu-id="20789-171">На виртуальных машинах серии H с поддержкой RDMA необходимо вручную установить расширение HpcVmDrivers.</span><span class="sxs-lookup"><span data-stu-id="20789-171">On RDMA-capable H-series VMs, you must manually install the HpcVmDrivers extension.</span></span> <span data-ttu-id="20789-172">См. [Размеры виртуальных машин, оптимизированных для высокопроизводительных вычислений](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span><span class="sxs-lookup"><span data-stu-id="20789-172">See [High performance compute VM sizes](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>
   * <span data-ttu-id="20789-173">**Конфигурация сети кластера.** Сценарий развертывания автоматически настраивает кластер пакета HPC с топологией 5 (все узлы в корпоративной сети).</span><span class="sxs-lookup"><span data-stu-id="20789-173">**Cluster network configuration**: The deployment script automatically sets up the HPC Pack cluster in Topology 5 (all nodes on the Enterprise network).</span></span> <span data-ttu-id="20789-174">Данная топология является обязательной для всех развертываний кластера пакета HPC на виртуальных машинах.</span><span class="sxs-lookup"><span data-stu-id="20789-174">This topology is required for all HPC Pack cluster deployments in VMs.</span></span> <span data-ttu-id="20789-175">В дальнейшем не следует изменять топологию сети кластера.</span><span class="sxs-lookup"><span data-stu-id="20789-175">Do not change the cluster network topology later.</span></span>
2. <span data-ttu-id="20789-176">**Перевод вычислительных узлов в оперативный режим для запуска заданий.**</span><span class="sxs-lookup"><span data-stu-id="20789-176">**Bring the compute nodes online to run jobs**</span></span>
   
    <span data-ttu-id="20789-177">Выберите узлы и используйте действие **Перевести в оперативный режим** в диспетчере кластеров HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-177">Select the nodes and use the **Bring Online** action in HPC Cluster Manager.</span></span> <span data-ttu-id="20789-178">После этого узлы будут готовы к выполнению заданий.</span><span class="sxs-lookup"><span data-stu-id="20789-178">The nodes are ready to run jobs.</span></span>
3. <span data-ttu-id="20789-179">**Отправка заданий в кластер.**</span><span class="sxs-lookup"><span data-stu-id="20789-179">**Submit jobs to the cluster**</span></span>
   
    <span data-ttu-id="20789-180">Подключитесь к головному узлу для отправки заданий или настройте для этого локальный компьютер.</span><span class="sxs-lookup"><span data-stu-id="20789-180">Connect to the head node to submit jobs, or set up an on-premises computer to do this.</span></span> <span data-ttu-id="20789-181">Сведения см. в разделе [Отправка заданий в кластер HPC в Azure](../../virtual-machines-windows-hpcpack-cluster-submit-jobs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span><span class="sxs-lookup"><span data-stu-id="20789-181">For information, see [Submit Jobs to an HPC cluster in Azure](../../virtual-machines-windows-hpcpack-cluster-submit-jobs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>
4. <span data-ttu-id="20789-182">**Отключение узлов и их остановка (отзыв).**</span><span class="sxs-lookup"><span data-stu-id="20789-182">**Take the nodes offline and stop (deallocate) them**</span></span>
   
    <span data-ttu-id="20789-183">После выполнения заданий отключите узлы в диспетчере кластеров HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-183">When you are done running jobs, take the nodes offline in HPC Cluster Manager.</span></span> <span data-ttu-id="20789-184">Затем с помощью средств управления Azure завершите их работу.</span><span class="sxs-lookup"><span data-stu-id="20789-184">Then, use Azure management tools to shut them down.</span></span>

## <a name="run-mpi-applications-on-the-cluster"></a><span data-ttu-id="20789-185">Запуск приложений MPI в кластере</span><span class="sxs-lookup"><span data-stu-id="20789-185">Run MPI applications on the cluster</span></span>
### <a name="example-run-mpipingpong-on-an-hpc-pack-cluster"></a><span data-ttu-id="20789-186">Пример. Выполнение команды mpipingpong в кластере HPC Pack</span><span class="sxs-lookup"><span data-stu-id="20789-186">Example: Run mpipingpong on an HPC Pack cluster</span></span>
<span data-ttu-id="20789-187">Чтобы проверить развертывание экземпляров с поддержкой RDMA, которое выполнялось с помощью пакета HPC, запустите в кластере команду **mpipingpong** из этого пакета.</span><span class="sxs-lookup"><span data-stu-id="20789-187">To verify an HPC Pack deployment of the RDMA-capable instances, run the HPC Pack **mpipingpong** command on the cluster.</span></span> <span data-ttu-id="20789-188">**mpipingpong** многократно отправляет пакеты данных из одного спаренного узла в другой, чтобы вычислить задержки и изменить пропускную способность и статистику для сети приложений с поддержкой RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-188">**mpipingpong** sends packets of data between paired nodes repeatedly to calculate latency and throughput measurements and statistics for the RDMA-enabled application network.</span></span> <span data-ttu-id="20789-189">В этом примере показана типичная схема запуска задания MPI (в данном случае **mpipingpong**) с помощью команды кластера **mpiexec**.</span><span class="sxs-lookup"><span data-stu-id="20789-189">This example shows a typical pattern for running an MPI job (in this case, **mpipingpong**) by using the cluster **mpiexec** command.</span></span>

<span data-ttu-id="20789-190">В этом примере предполагается, что вы добавили узлы Azure в конфигурации "расширение в Azure" ([Сценарий 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-\(PaaS\) in this article)).</span><span class="sxs-lookup"><span data-stu-id="20789-190">This example assumes you added Azure nodes in a “burst to Azure” configuration ([Scenario 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-\(PaaS\) in this article).</span></span> <span data-ttu-id="20789-191">Если пакет HPC Pack развернут в кластере виртуальных машин Azure, необходимо изменить синтаксис команды, чтобы указать другую группу узлов и задать дополнительные переменные среды для направления сетевого трафика по сети RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-191">If you deployed HPC Pack on a cluster of Azure VMs, you’ll need to modify the command syntax to specify a different node group and set additional environment variables to direct network traffic to the RDMA network.</span></span>

<span data-ttu-id="20789-192">Чтобы выполнить команду mpipingpong в кластере:</span><span class="sxs-lookup"><span data-stu-id="20789-192">To run mpipingpong on the cluster:</span></span>

1. <span data-ttu-id="20789-193">На головном узле или на соответствующим образом настроенном клиентском компьютере откройте командную строку.</span><span class="sxs-lookup"><span data-stu-id="20789-193">On the head node or on a properly configured client computer, open a Command Prompt.</span></span>
2. <span data-ttu-id="20789-194">Чтобы оценить задержку между парами узлов в развертывании ускорения Azure, состоящем из четырех узлов, запустите задание выполнения команды mpipingpong с небольшим размером пакетов и большим количеством итераций. Для этого введите такую команду:</span><span class="sxs-lookup"><span data-stu-id="20789-194">To estimate latency between pairs of nodes in an Azure burst deployment of four nodes, type the following command to submit a job to run mpipingpong with a small packet size and many iterations:</span></span>
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 1:100000 -op -s nul
    ```
   
    <span data-ttu-id="20789-195">Команда вернет идентификатор запущенного задания.</span><span class="sxs-lookup"><span data-stu-id="20789-195">The command returns the ID of the job that is submitted.</span></span>
   
    <span data-ttu-id="20789-196">Если кластер HPC Pack развернут на виртуальных машинах Azure, укажите группу узлов, которая содержит виртуальные машины вычислительных узлов, развернутые в одной облачной службе, и измените команду **mpiexec** следующим образом:</span><span class="sxs-lookup"><span data-stu-id="20789-196">If you deployed the HPC Pack cluster deployed on Azure VMs, specify a node group that contains compute node VMs deployed in a single cloud service, and modify the **mpiexec** command as follows:</span></span>
   
    ```Command
    job submit /nodegroup:vmcomputenodes /numnodes:4 mpiexec -c 1 -affinity -env MSMPI_DISABLE_SOCK 1 -env MSMPI_PRECONNECT all -env MPICH_NETMASK 172.16.0.0/255.255.0.0 mpipingpong -p 1:100000 -op -s nul
    ```
3. <span data-ttu-id="20789-197">Чтобы после завершения задания просмотреть выходные данные (в данном случае — выходные данные задачи 1 задания), введите следующую команду</span><span class="sxs-lookup"><span data-stu-id="20789-197">When the job completes, to view the output (in this case, the output of task 1 of the job), type the following</span></span>
   
    ```Command
    task view <JobID>.1
    ```
   
    <span data-ttu-id="20789-198">где &lt;*JobID*&gt; — идентификатор запущенного задания.</span><span class="sxs-lookup"><span data-stu-id="20789-198">where &lt;*JobID*&gt; is the ID of the job that was submitted.</span></span>
   
    <span data-ttu-id="20789-199">Выходные данные будут содержать сведения о задержке, подобные следующим.</span><span class="sxs-lookup"><span data-stu-id="20789-199">The output includes latency results similar to the following.</span></span>
   
    ![Задержка проверки связи][pingpong1]
4. <span data-ttu-id="20789-201">Чтобы оценить пропускную способность между парами узлов расширения Azure, запустите задание выполнения команды **mpipingpong** с большим размером пакетов и небольшим количеством итераций. Для этого введите следующую команду:</span><span class="sxs-lookup"><span data-stu-id="20789-201">To estimate throughput between pairs of Azure burst nodes, type the following command to submit a job to run **mpipingpong** with a large packet size and a few iterations:</span></span>
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 4000000:1000 -op -s nul
    ```
   
    <span data-ttu-id="20789-202">Команда вернет идентификатор запущенного задания.</span><span class="sxs-lookup"><span data-stu-id="20789-202">The command returns the ID of the job that is submitted.</span></span>
   
    <span data-ttu-id="20789-203">В кластере HPC Pack, развернутом на виртуальных машинах Azure, измените команду, как указано в шаге 2.</span><span class="sxs-lookup"><span data-stu-id="20789-203">On an HPC Pack cluster deployed on Azure VMs, modify the command as noted in step 2.</span></span>
5. <span data-ttu-id="20789-204">Чтобы после завершения задания просмотреть выходные данные (в данном случае — выходные данные задачи 1 задания), введите следующую команду:</span><span class="sxs-lookup"><span data-stu-id="20789-204">When the job completes, to view the output (in this case, the output of task 1 of the job), type the following:</span></span>
   
    ```Command
    task view <JobID>.1
    ```
   
   <span data-ttu-id="20789-205">Выходные данные будут содержать сведения о пропускной способности, подобные следующим.</span><span class="sxs-lookup"><span data-stu-id="20789-205">The output includes throughput results similar to the following.</span></span>
   
   ![Пропускная способность при «пинг-понговой» передаче данных][pingpong2]

### <a name="mpi-application-considerations"></a><span data-ttu-id="20789-207">Рекомендации по приложениям MPI</span><span class="sxs-lookup"><span data-stu-id="20789-207">MPI application considerations</span></span>
<span data-ttu-id="20789-208">Ниже приведены рекомендации по запуску в Azure приложений MPI с пакетом HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-208">Following are considerations for running MPI applications with HPC Pack in Azure.</span></span> <span data-ttu-id="20789-209">Некоторые из них применимы только к развертыванию узлов Azure (экземпляров рабочих ролей, добавленных в конфигурации «ускорение в Azure»).</span><span class="sxs-lookup"><span data-stu-id="20789-209">Some apply only to deployments of Azure nodes (worker role instances added in a “burst to Azure” configuration).</span></span>

* <span data-ttu-id="20789-210">Экземпляры рабочих ролей в облачной службе периодически повторно подготавливаются без уведомления системой Azure (например, для обслуживания системы или в случае сбоя экземпляра).</span><span class="sxs-lookup"><span data-stu-id="20789-210">Worker role instances in a cloud service are periodically reprovisioned without notice by Azure (for example, for system maintenance, or in case an instance fails).</span></span> <span data-ttu-id="20789-211">Если экземпляр повторно подготавливается в то время, когда он выполняет задание MPI, экземпляр теряет все свои данные и возвращается в исходное состояние, что может привести к сбою задания MPI.</span><span class="sxs-lookup"><span data-stu-id="20789-211">If an instance is reprovisioned while it is running an MPI job, the instance loses its data and returns to the state when it was first deployed, which can cause the MPI job to fail.</span></span> <span data-ttu-id="20789-212">Чем больше узлов используется для одного задания MPI и чем дольше задание выполняется, тем вероятнее, что один из экземпляров будет подготовлен повторно во время выполнения задания.</span><span class="sxs-lookup"><span data-stu-id="20789-212">The more nodes that you use for a single MPI job, and the longer the job runs, the more likely that one of the instances is reprovisioned while a job is running.</span></span> <span data-ttu-id="20789-213">Это следует учитывать также в случае назначения одного узла в развертывании в качестве файлового сервера.</span><span class="sxs-lookup"><span data-stu-id="20789-213">Also consider this if you designate a single node in the deployment as a file server.</span></span>
* <span data-ttu-id="20789-214">Для запуска заданий MPI в Azure не обязательно использовать экземпляры с поддержкой RDMA.</span><span class="sxs-lookup"><span data-stu-id="20789-214">To run MPI jobs in Azure, you don't have to use the RDMA-capable instances.</span></span> <span data-ttu-id="20789-215">Вы можете использовать экземпляр любого размера, поддерживаемый пакетом HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-215">You can use any instance size that is supported by HPC Pack.</span></span> <span data-ttu-id="20789-216">Тем не менее экземпляры с поддержкой RDMA рекомендуются для относительно крупных заданий MPI, чувствительных к задержке и пропускной способности сети, соединяющей узлы.</span><span class="sxs-lookup"><span data-stu-id="20789-216">However, the RDMA-capable instances are recommended for running relatively large-scale MPI jobs that are sensitive to the latency and the bandwidth of the network that connects the nodes.</span></span> <span data-ttu-id="20789-217">Если для запуска заданий MPI, чувствительных к задержке и пропускной способности, вы будете использовать другие размеры экземпляров, рекомендуем запускать мелкие задания, в которых каждая отдельная задача выполняется только на нескольких узлах.</span><span class="sxs-lookup"><span data-stu-id="20789-217">If you use other sizes to run latency- and bandwidth-sensitive MPI jobs, we recommend running small jobs, in which a single task runs on only a few nodes.</span></span>
* <span data-ttu-id="20789-218">Приложения, развернутые в экземплярах Azure, подлежат условиям лицензирования, связанным с приложением.</span><span class="sxs-lookup"><span data-stu-id="20789-218">Applications deployed to Azure instances are subject to the licensing terms associated with the application.</span></span> <span data-ttu-id="20789-219">Проконсультируйтесь с поставщиками всех коммерческих приложений насчет лицензирования или иных ограничений на запуск приложений в облаке.</span><span class="sxs-lookup"><span data-stu-id="20789-219">Check with the vendor of any commercial application for licensing or other restrictions for running in the cloud.</span></span> <span data-ttu-id="20789-220">Не все поставщики предлагают лицензирование с оплатой по мере использования.</span><span class="sxs-lookup"><span data-stu-id="20789-220">Not all vendors offer pay-as-you-go licensing.</span></span>
* <span data-ttu-id="20789-221">Экземплярам Azure требуется дополнительная настройка для доступа к локальным узлам, общим ресурсам и серверам лицензий.</span><span class="sxs-lookup"><span data-stu-id="20789-221">Azure instances need further setup to access on-premises nodes, shares, and license servers.</span></span> <span data-ttu-id="20789-222">Например, чтобы включить для узлов Azure доступ к локальному серверу лицензий, вы можете настроить виртуальную сеть Azure «сеть — сеть».</span><span class="sxs-lookup"><span data-stu-id="20789-222">For example, to enable the Azure nodes to access an on-premises license server, you can configure a site-to-site Azure virtual network.</span></span>
* <span data-ttu-id="20789-223">Чтобы запустить приложение MPI в экземплярах Azure, следует зарегистрировать приложение MPI в брандмауэре Windows в экземплярах, запустив команду **hpcfwutil** .</span><span class="sxs-lookup"><span data-stu-id="20789-223">To run MPI applications on Azure instances, register each MPI application with Windows Firewall on the instances by running the **hpcfwutil** command.</span></span> <span data-ttu-id="20789-224">Это позволит осуществлять обмен данными MPI через порт, который динамически назначается брандмауэром.</span><span class="sxs-lookup"><span data-stu-id="20789-224">This allows MPI communications to take place on a port that is assigned dynamically by the firewall.</span></span>
  
  > [!NOTE]
  > <span data-ttu-id="20789-225">Для развертываний ускорения в Azure можно также настроить команду исключения брандмауэра с целью автоматического запуска на всех новых узлах Azure, которые добавляются к кластеру.</span><span class="sxs-lookup"><span data-stu-id="20789-225">For burst to Azure deployments, you can also configure a firewall exception command to run automatically on all new Azure nodes that are added to your cluster.</span></span> <span data-ttu-id="20789-226">Запустив команду **hpcfwutil** и убедившись в том, что ваше приложение работает, добавьте команду в скрипт запуска для узлов Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-226">After you run the **hpcfwutil** command and verify that your application works, add the command to a startup script for your Azure nodes.</span></span> <span data-ttu-id="20789-227">Дополнительные сведения см. в статье [Использование скрипта запуска для узлов Azure](https://technet.microsoft.com/library/jj899632.aspx).</span><span class="sxs-lookup"><span data-stu-id="20789-227">For more information, see [Use a Startup Script for Azure Nodes](https://technet.microsoft.com/library/jj899632.aspx).</span></span>
  > 
  > 
* <span data-ttu-id="20789-228">Пакет HPC использует переменную среды кластера CCP_MPI_NETMASK для указания диапазона допустимых адресов для связи MPI.</span><span class="sxs-lookup"><span data-stu-id="20789-228">HPC Pack uses the CCP_MPI_NETMASK cluster environment variable to specify a range of acceptable addresses for MPI communication.</span></span> <span data-ttu-id="20789-229">Начиная с пакета HPC Pack 2012 R2, переменная среды кластера CCP_MPI_NETMASK влияет только на связь MPI между вычислительными узлами кластера, присоединенными к домену (локально или на виртуальных машинах Azure).</span><span class="sxs-lookup"><span data-stu-id="20789-229">Starting in HPC Pack 2012 R2, the CCP_MPI_NETMASK cluster environment variable only affects MPI communication between domain-joined cluster compute nodes (either on-premises or in Azure VMs).</span></span> <span data-ttu-id="20789-230">Переменная игнорируется узлами, добавленными в рамках ускорения в конфигурацию Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-230">The variable is ignored by nodes added in a burst to Azure configuration.</span></span>
* <span data-ttu-id="20789-231">Задания MPI не могут выполняться между экземплярами Azure, развернутыми в разных облачных службах (например, в развертываниях ускорения в Azure с разными шаблонами узлов или на вычислительных узлах виртуальных машин Azure, развернутых в нескольких облачных службах).</span><span class="sxs-lookup"><span data-stu-id="20789-231">MPI jobs can't run across Azure instances that are deployed in different cloud services (for example, in burst to Azure deployments with different node templates, or Azure VM compute nodes deployed in multiple cloud services).</span></span> <span data-ttu-id="20789-232">При наличии нескольких развертываний узлов Azure, запущенных с разными шаблонами узлов, задание MPI должно выполняться только на одном наборе узлов Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-232">If you have multiple Azure node deployments that are started with different node templates, the MPI job must run on only one set of Azure nodes.</span></span>
* <span data-ttu-id="20789-233">При добавлении узлов Azure в кластер и переводе их в оперативный режим служба планировщика заданий HPC немедленно пытается запустить задания на узлах.</span><span class="sxs-lookup"><span data-stu-id="20789-233">When you add Azure nodes to your cluster and bring them online, the HPC Job Scheduler Service immediately tries to start jobs on the nodes.</span></span> <span data-ttu-id="20789-234">Если в Azure можно запустить только часть имеющейся рабочей нагрузки, не забудьте обновить или создать шаблоны заданий, чтобы таким образом определить типы заданий, которые можно выполнять в Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-234">If only a portion of your workload can run on Azure, ensure that you update or create job templates to define what job types can run on Azure.</span></span> <span data-ttu-id="20789-235">Например, чтобы на узлах Azure выполнялись только те задания, которые были отправлены с использованием шаблона задания, добавьте свойство «Группы узлов» в шаблон задания и выберите AzureNodes в качестве обязательного значения.</span><span class="sxs-lookup"><span data-stu-id="20789-235">For example, to ensure that jobs submitted with a job template only run on Azure nodes, add the Node Groups property to the job template and select AzureNodes as the required value.</span></span> <span data-ttu-id="20789-236">Для создания настраиваемых групп узлов Azure можно использовать командлет PowerShell Add-HpcGroup HPC.</span><span class="sxs-lookup"><span data-stu-id="20789-236">To create custom groups for your Azure nodes, use the Add-HpcGroup HPC PowerShell cmdlet.</span></span>

## <a name="next-steps"></a><span data-ttu-id="20789-237">Дальнейшие действия</span><span class="sxs-lookup"><span data-stu-id="20789-237">Next steps</span></span>
* <span data-ttu-id="20789-238">В качестве альтернативы пакету HPC для выполнения приложений MPI в управляемых пулах вычислительных узлов в Azure можно использовать пакетную службу Azure.</span><span class="sxs-lookup"><span data-stu-id="20789-238">As an alternative to using HPC Pack, develop with the Azure Batch service to run MPI applications on managed pools of compute nodes in Azure.</span></span> <span data-ttu-id="20789-239">Ознакомьтесь со статьей [Использование задач с несколькими экземплярами для запуска приложений с интерфейсом передачи сообщений в пакетной службе Azure](../../../batch/batch-mpi.md).</span><span class="sxs-lookup"><span data-stu-id="20789-239">See [Use multi-instance tasks to run Message Passing Interface (MPI) applications in Azure Batch](../../../batch/batch-mpi.md).</span></span>
* <span data-ttu-id="20789-240">Если требуется запускать приложения MPI для Linux, получающие доступ к сети RDMA в Azure, см. раздел [Настройка кластера Linux RDMA для запуска приложений MPI](../../linux/classic/rdma-cluster.md).</span><span class="sxs-lookup"><span data-stu-id="20789-240">If you want to run Linux MPI applications that access the Azure RDMA network, see [Set up a Linux RDMA cluster to run MPI applications](../../linux/classic/rdma-cluster.md).</span></span>

<!--Image references-->
[burst]:media/hpcpack-rdma-cluster/burst.png
[iaas]:media/hpcpack-rdma-cluster/iaas.png
[pingpong1]:media/hpcpack-rdma-cluster/pingpong1.png
[pingpong2]:media/hpcpack-rdma-cluster/pingpong2.png
