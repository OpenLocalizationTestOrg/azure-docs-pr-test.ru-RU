---
title: "aaaSet копирование приложений MPI toorun кластера Windows RDMA | Документы Microsoft"
description: "Узнайте, как toocreate кластера Windows HPC Pack с toouse H16r, H16mr, A8 или A9 ВМ размера hello приложений MPI toorun сети Azure RDMA."
services: virtual-machines-windows
documentationcenter: 
author: dlepow
manager: timlt
editor: 
tags: azure-service-management,hpc-pack
ms.assetid: 7d9f5bc8-012f-48dd-b290-db81c7592215
ms.service: virtual-machines-windows
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: vm-windows
ms.workload: big-compute
ms.date: 06/01/2017
ms.author: danlep
ms.openlocfilehash: 23bc8740dbd05a7c7ab3f998489a41d0df4520a2
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/06/2017
---
# <a name="set-up-a-windows-rdma-cluster-with-hpc-pack-toorun-mpi-applications"></a><span data-ttu-id="8e2b9-103">Настройка кластера Windows RDMA с помощью приложений MPI toorun пакета HPC</span><span class="sxs-lookup"><span data-stu-id="8e2b9-103">Set up a Windows RDMA cluster with HPC Pack toorun MPI applications</span></span>
<span data-ttu-id="8e2b9-104">Настройка кластера Windows RDMA в Azure с помощью [пакета Microsoft HPC](https://technet.microsoft.com/library/cc514029) и [высокопроизводительных вычислений размеры виртуальных Машин](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json) toorun параллельных приложений интерфейса передачи сообщений (MPI).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-104">Set up a Windows RDMA cluster in Azure with [Microsoft HPC Pack](https://technet.microsoft.com/library/cc514029) and [High performance compute VM sizes](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json) toorun parallel Message Passing Interface (MPI) applications.</span></span> <span data-ttu-id="8e2b9-105">Если настроить узлы с поддержкой RDMA под управлением Windows Server в кластере пакета HPC, приложения MPI будут эффективно взаимодействовать по сети с низкой задержкой и высокой пропускной способностью в Azure, основанной на технологии удаленного доступа к памяти (RDMA).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-105">When you set up RDMA-capable, Windows Server-based nodes in an HPC Pack cluster, MPI applications communicate efficiently over a low latency, high throughput network in Azure that is based on remote direct memory access (RDMA) technology.</span></span>

<span data-ttu-id="8e2b9-106">Toorun рабочих нагрузок MPI на виртуальных машинах Linux сети Azure RDMA hello, доступа к статье [настройки приложений MPI toorun кластеров Linux RDMA](../../linux/classic/rdma-cluster.md).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-106">If you want toorun MPI workloads on Linux VMs that access hello Azure RDMA network, see [Set up a Linux RDMA cluster toorun MPI applications](../../linux/classic/rdma-cluster.md).</span></span>

## <a name="hpc-pack-cluster-deployment-options"></a><span data-ttu-id="8e2b9-107">Варианты развертывания кластера пакета HPC</span><span class="sxs-lookup"><span data-stu-id="8e2b9-107">HPC Pack cluster deployment options</span></span>
<span data-ttu-id="8e2b9-108">Пакет Microsoft HPC — это средство, предоставляемых в toocreate без дополнительных затрат HPC кластеров в локальной или в Azure toorun Windows или Linux HPC приложений.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-108">Microsoft HPC Pack is a tool provided at no additional cost toocreate HPC clusters on-premises or in Azure toorun Windows or Linux HPC applications.</span></span> <span data-ttu-id="8e2b9-109">Пакет HPC включает среду выполнения для реализации Майкрософт hello hello сообщение передача интерфейса для Windows (MS-MPI).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-109">HPC Pack includes a runtime environment for hello Microsoft implementation of hello Message Passing Interface for Windows (MS-MPI).</span></span> <span data-ttu-id="8e2b9-110">При использовании с функцией RDMA экземпляры, поддерживаемой операционной системы Windows Server, HPC Pack предоставляет эффективный параметр toorun Windows приложений MPI этой сети Azure RDMA hello доступа.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-110">When used with RDMA-capable instances running a supported Windows Server operating system, HPC Pack provides an efficient option toorun Windows MPI applications that access hello Azure RDMA network.</span></span> 

<span data-ttu-id="8e2b9-111">В этой статье рассматриваются два сценария и ссылки tooset руководство toodetailed кластера Windows RDMA с помощью пакета Microsoft HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-111">This article introduces two scenarios and links toodetailed guidance tooset up a Windows RDMA cluster with Microsoft HPC Pack.</span></span> 

* <span data-ttu-id="8e2b9-112">Сценарий 1.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-112">Scenario 1.</span></span> <span data-ttu-id="8e2b9-113">Развертывание экземпляров рабочих ролей для ресурсоемких вычислений (PaaS)</span><span class="sxs-lookup"><span data-stu-id="8e2b9-113">Deploy compute-intensive worker role instances (PaaS)</span></span>
* <span data-ttu-id="8e2b9-114">Сценарий 2.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-114">Scenario 2.</span></span> <span data-ttu-id="8e2b9-115">Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)</span><span class="sxs-lookup"><span data-stu-id="8e2b9-115">Deploy compute nodes in compute-intensive VMs (IaaS)</span></span>

<span data-ttu-id="8e2b9-116">Основные обязательные требования toouse экземпляры большим объемом вычислений с Windows, см. [высокопроизводительных вычислений размеры виртуальных Машин](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-116">For general prerequisites toouse compute-intensive instances with Windows, see [High performance compute VM sizes](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>

## <a name="scenario-1-deploy-compute-intensive-worker-role-instances-paas"></a><span data-ttu-id="8e2b9-117">Сценарий 1. Развертывание экземпляров рабочих ролей для ресурсоемких вычислений (PaaS)</span><span class="sxs-lookup"><span data-stu-id="8e2b9-117">Scenario 1: Deploy compute-intensive worker role instances (PaaS)</span></span>
<span data-ttu-id="8e2b9-118">Из существующего кластера HPC Pack добавьте дополнительные вычислительные ресурсы в экземплярах рабочих ролей Azure (узлах Azure), запускаемых в облачной службе (PaaS).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-118">From an existing HPC Pack cluster, add extra compute resources in Azure worker role instances (Azure nodes) running in a cloud service (PaaS).</span></span> <span data-ttu-id="8e2b9-119">Эта функция также называется «прорыв tooAzure» из пакета HPC, поддерживает диапазон размеров экземпляров рабочей роли hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-119">This feature, also called “burst tooAzure” from HPC Pack, supports a range of sizes for hello worker role instances.</span></span> <span data-ttu-id="8e2b9-120">При добавлении hello узлов Azure, укажите один из размеров hello функцией RDMA.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-120">When adding hello Azure nodes, specify one of hello RDMA-capable sizes.</span></span>

<span data-ttu-id="8e2b9-121">Ниже приведены рекомендации и инструкции tooburst поддержкой tooRDMA экземплярами Azure из существующего (обычно локального) кластера.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-121">Following are considerations and steps tooburst tooRDMA-capable Azure instances from an existing (typically on-premises) cluster.</span></span> <span data-ttu-id="8e2b9-122">Используйте аналогичные процедуры tooadd рабочей роли экземпляров tooan головной узел пакета HPC, развернутого в Виртуальной машине Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-122">Use similar procedures tooadd worker role instances tooan HPC Pack head node that is deployed in an Azure VM.</span></span>

> [!NOTE]
> <span data-ttu-id="8e2b9-123">Учебника tooburst tooAzure, с помощью пакета HPC, в разделе [Настройка гибридного кластера с помощью пакета HPC](../../../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-123">For a tutorial tooburst tooAzure with HPC Pack, see [Set up a hybrid cluster with HPC Pack](../../../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md).</span></span> <span data-ttu-id="8e2b9-124">Обратите внимание особенности hello в hello, выполните действия, применяемые в частности поддержкой tooRDMA узлов Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-124">Note hello considerations in hello following steps that apply specifically tooRDMA-capable Azure nodes.</span></span>
> 
> 

![Повышение tooAzure][burst]

### <a name="steps"></a><span data-ttu-id="8e2b9-126">Действия</span><span class="sxs-lookup"><span data-stu-id="8e2b9-126">Steps</span></span>
1. <span data-ttu-id="8e2b9-127">**Развертывание и настройка головного узла HPC Pack 2012 R2.**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-127">**Deploy and configure an HPC Pack 2012 R2 head node**</span></span>
   
    <span data-ttu-id="8e2b9-128">Загрузить hello последнюю версию пакета установки пакета HPC hello [центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-128">Download hello latest HPC Pack installation package from hello [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span></span> <span data-ttu-id="8e2b9-129">Требования и инструкции по tooprepare развертыванию прорыва в Azure, в разделе [прорыв экземпляров рабочей tooAzure с пакетом Microsoft HPC](https://technet.microsoft.com/library/gg481749.aspx).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-129">For requirements and instructions tooprepare for an Azure burst deployment, see [Burst tooAzure Worker Instances with Microsoft HPC Pack](https://technet.microsoft.com/library/gg481749.aspx).</span></span>
2. <span data-ttu-id="8e2b9-130">**Настройка сертификата управления в подписку Azure hello**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-130">**Configure a management certificate in hello Azure subscription**</span></span>
   
    <span data-ttu-id="8e2b9-131">Настройте сертификат toosecure hello соединение между головным узлом hello и Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-131">Configure a certificate toosecure hello connection between hello head node and Azure.</span></span> <span data-ttu-id="8e2b9-132">Параметры и процедуры см. в разделе [сценариев tooConfigure hello сертификата управления Azure для пакета HPC](http://technet.microsoft.com/library/gg481759.aspx).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-132">For options and procedures, see [Scenarios tooConfigure hello Azure Management Certificate for HPC Pack](http://technet.microsoft.com/library/gg481759.aspx).</span></span> <span data-ttu-id="8e2b9-133">В тестовых развертываниях пакет HPC устанавливается по умолчанию Microsoft HPC Azure сертификат управления можно быстро отправить tooyour подписки Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-133">For test deployments, HPC Pack installs a Default Microsoft HPC Azure Management Certificate you can quickly upload tooyour Azure subscription.</span></span>
3. <span data-ttu-id="8e2b9-134">**Создание новой облачной службы и учетной записи хранения.**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-134">**Create a new cloud service and a storage account**</span></span>
   
    <span data-ttu-id="8e2b9-135">Используйте hello Azure toocreate портала облачной службы и учетной записи хранилища для развертывания hello в регионе, где доступны экземпляры с поддержкой RDMA hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-135">Use hello Azure portal toocreate a cloud service and a storage account for hello deployment in a region where hello RDMA-capable instances are available.</span></span>
4. <span data-ttu-id="8e2b9-136">**Создание шаблона узла Azure.**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-136">**Create an Azure node template**</span></span>
   
    <span data-ttu-id="8e2b9-137">Используйте приветствия мастера шаблона узла в диспетчере кластера HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-137">Use hello Create Node Template Wizard in HPC Cluster Manager.</span></span> <span data-ttu-id="8e2b9-138">Пошаговые инструкции см. в разделе [создания шаблона узла Azure](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) в «TooDeploy действия узлов Azure с пакетом Microsoft HPC».</span><span class="sxs-lookup"><span data-stu-id="8e2b9-138">For steps, see [Create an Azure node template](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) in “Steps tooDeploy Azure Nodes with Microsoft HPC Pack”.</span></span>
   
    <span data-ttu-id="8e2b9-139">Для первоначальных тестов рекомендуется Настройка политики вручную доступности в шаблоне hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-139">For initial tests, we suggest configuring a manual availability policy in hello template.</span></span>
5. <span data-ttu-id="8e2b9-140">**Добавление узлов кластера toohello**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-140">**Add nodes toohello cluster**</span></span>
   
    <span data-ttu-id="8e2b9-141">Здравствуйте, используйте мастер добавления узла в диспетчере кластера HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-141">Use hello Add Node Wizard in HPC Cluster Manager.</span></span> <span data-ttu-id="8e2b9-142">Дополнительные сведения см. в разделе [toohello Добавление узлов Azure кластер Windows HPC](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-142">For more information, see [Add Azure Nodes toohello Windows HPC Cluster](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).</span></span>
   
    <span data-ttu-id="8e2b9-143">При указании размера hello hello узлов, выберите один из размеров экземпляров функцией RDMA hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-143">When specifying hello size of hello nodes, select one of hello RDMA-capable instance sizes.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="8e2b9-144">В каждой пакетное развертывание tooAzure с большим объемом вычислений экземплярами hello HPC Pack автоматически разворачивает не менее двух экземпляров, имеющих функцию RDMA (таких как A8) в качестве узлов прокси-сервера, кроме toohello экземпляров рабочей роли Azure можно указать.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-144">In each burst tooAzure deployment with hello compute-intensive instances, HPC Pack automatically deploys a minimum of two RDMA-capable instances (such as A8) as proxy nodes, in addition toohello Azure worker role instances you specify.</span></span> <span data-ttu-id="8e2b9-145">Hello узлов прокси-сервера используют ядра, выделяются toohello подписки и взимается вместе с hello экземпляров рабочей роли Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-145">hello proxy nodes use cores that are allocated toohello subscription and incur charges along with hello Azure worker role instances.</span></span>
   > 
   > 
6. <span data-ttu-id="8e2b9-146">**Запуск (провизионирование) узлов hello и их подключением к сети toorun заданий**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-146">**Start (provision) hello nodes and bring them online toorun jobs**</span></span>
   
    <span data-ttu-id="8e2b9-147">Выбор узлов hello и использовать hello **запустить** в диспетчере кластера HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-147">Select hello nodes and use hello **Start** action in HPC Cluster Manager.</span></span> <span data-ttu-id="8e2b9-148">По завершении подготовки выберите узлы hello и с помощью hello **перевести в оперативный режим** в диспетчере кластера HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-148">When provisioning is complete, select hello nodes and use hello **Bring Online** action in HPC Cluster Manager.</span></span> <span data-ttu-id="8e2b9-149">узлы Hello — Готово toorun заданий.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-149">hello nodes are ready toorun jobs.</span></span>
7. <span data-ttu-id="8e2b9-150">**Отправка задания toohello кластера**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-150">**Submit jobs toohello cluster**</span></span>
   
   <span data-ttu-id="8e2b9-151">Используйте кластер HPC Pack задания отправки средства toorun задания.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-151">Use HPC Pack job submission tools toorun cluster jobs.</span></span> <span data-ttu-id="8e2b9-152">См. раздел [Пакет Microsoft HPC: управление заданиями](http://technet.microsoft.com/library/jj899585.aspx).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-152">See [Microsoft HPC Pack: Job Management](http://technet.microsoft.com/library/jj899585.aspx).</span></span>
8. <span data-ttu-id="8e2b9-153">**Остановка (отзыв) узлов hello**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-153">**Stop (deprovision) hello nodes**</span></span>
   
   <span data-ttu-id="8e2b9-154">После завершения выполнения заданий отключите узлы hello и использовать hello **остановить** в диспетчере кластера HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-154">When you are done running jobs, take hello nodes offline and use hello **Stop** action in HPC Cluster Manager.</span></span>

## <a name="scenario-2-deploy-compute-nodes-in-compute-intensive-vms-iaas"></a><span data-ttu-id="8e2b9-155">Сценарий 2. Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)</span><span class="sxs-lookup"><span data-stu-id="8e2b9-155">Scenario 2: Deploy compute nodes in compute-intensive VMs (IaaS)</span></span>
<span data-ttu-id="8e2b9-156">В этом сценарии развертывания головного узла HPC Pack hello и вычислительные узлы кластера на виртуальных машинах в виртуальной сети Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-156">In this scenario, you deploy hello HPC Pack head node and cluster compute nodes on VMs in an Azure virtual network.</span></span> <span data-ttu-id="8e2b9-157">Пакет HPC предоставляет несколько [вариантов развертывания на виртуальных машинах Azure](../../linux/hpcpack-cluster-options.md), в том числе сценарии автоматизированного развертывания и шаблоны быстрого запуска Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-157">HPC Pack provides several [deployment options in Azure VMs](../../linux/hpcpack-cluster-options.md), including automated deployment scripts and Azure quickstart templates.</span></span> <span data-ttu-id="8e2b9-158">Пример: hello следующие рекомендации и инструкции toouse hello [скрипт развертывания IaaS пакета HPC](hpcpack-cluster-powershell-script.md) для автоматизации развертывания hello кластера HPC Pack 2012 R2 в Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-158">As an example, hello following considerations and steps guide you toouse hello [HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md) to automate hello deployment of an HPC Pack 2012 R2 cluster in Azure.</span></span>

![Кластер на виртуальных машинах Azure][iaas]

### <a name="steps"></a><span data-ttu-id="8e2b9-160">Действия</span><span class="sxs-lookup"><span data-stu-id="8e2b9-160">Steps</span></span>
1. <span data-ttu-id="8e2b9-161">**Создание головного узла кластера и виртуальных машин вычислительных узлов, выполнив скрипт развертывания IaaS пакета HPC hello на клиентском компьютере**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-161">**Create a cluster head node and compute node VMs by running hello HPC Pack IaaS deployment script on a client computer**</span></span>
   
    <span data-ttu-id="8e2b9-162">Загрузить пакет скрипта развертывания IaaS пакета HPC hello с hello [центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-162">Download hello HPC Pack IaaS Deployment Script package from hello [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span></span>
   
    <span data-ttu-id="8e2b9-163">tooprepare hello клиентского компьютера, создайте файл конфигурации скрипта hello и hello выполнения сценария см. в разделе [создание кластера HPC с помощью скрипта развертывания IaaS пакета HPC hello](hpcpack-cluster-powershell-script.md).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-163">tooprepare hello client computer, create hello script configuration file, and run hello script, see [Create an HPC Cluster with hello HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md).</span></span> 
   
    <span data-ttu-id="8e2b9-164">toodeploy функцией RDMA вычислительные узлы hello Примечание следующие дополнительные вопросы:</span><span class="sxs-lookup"><span data-stu-id="8e2b9-164">toodeploy RDMA-capable compute nodes, note hello following additional considerations:</span></span>
   
   * <span data-ttu-id="8e2b9-165">**Виртуальная сеть**: укажите новую виртуальную сеть в регионе, в которых hello функцией RDMA размер экземпляра требуется toouse доступен.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-165">**Virtual network**: Specify a new virtual network in a region in which hello RDMA-capable instance size you want toouse is available.</span></span>
   * <span data-ttu-id="8e2b9-166">**Операционная система Windows Server**: toosupport подключения RDMA, укажите операционную систему Windows Server 2012 R2 или Windows Server 2012 для hello ВМ вычислительных узлов.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-166">**Windows Server operating system**: toosupport RDMA connectivity, specify a Windows Server 2012 R2 or Windows Server 2012 operating system for hello compute node VMs.</span></span>
   * <span data-ttu-id="8e2b9-167">**Облачные службы.** Мы рекомендуем развертывать головной узел в одной облачной службе, а вычислительные узлы — в другой облачной службе.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-167">**Cloud services**: We recommend deploying your head node in one cloud service and your compute nodes in a different cloud service.</span></span>
   * <span data-ttu-id="8e2b9-168">**Размер головного узла**: для этого сценария рекомендуется выбрать размер хотя бы A4 (очень крупный) для головного узла hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-168">**Head node size**: For this scenario, consider a size of at least A4 (Extra Large) for hello head node.</span></span>
   * <span data-ttu-id="8e2b9-169">**Расширение HpcVmDrivers**: hello скрипт развертывания hello агента ВМ Azure и hello расширение HpcVmDrivers автоматически устанавливается при развертывании размер A8 или A9 вычислительных узлов с операционной системой Windows Server.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-169">**HpcVmDrivers extension**: hello deployment script installs hello Azure VM Agent and hello HpcVmDrivers extension automatically when you deploy size A8 or A9 compute nodes with a Windows Server operating system.</span></span> <span data-ttu-id="8e2b9-170">HpcVmDrivers устанавливает драйверы на hello ВМ вычислительных узлов, чтобы они могли подключаться к сети RDMA toohello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-170">HpcVmDrivers installs drivers on hello compute node VMs so they can connect toohello RDMA network.</span></span> <span data-ttu-id="8e2b9-171">На виртуальных машинах серии H функцией RDMA необходимо вручную установить расширение HpcVmDrivers hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-171">On RDMA-capable H-series VMs, you must manually install hello HpcVmDrivers extension.</span></span> <span data-ttu-id="8e2b9-172">См. [Размеры виртуальных машин, оптимизированных для высокопроизводительных вычислений](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-172">See [High performance compute VM sizes](../sizes-hpc.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>
   * <span data-ttu-id="8e2b9-173">**Конфигурация сети кластера**: hello сценарий развертывания автоматически настраивает кластер HPC Pack hello с топологией 5 (все узлы в корпоративной сети hello).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-173">**Cluster network configuration**: hello deployment script automatically sets up hello HPC Pack cluster in Topology 5 (all nodes on hello Enterprise network).</span></span> <span data-ttu-id="8e2b9-174">Данная топология является обязательной для всех развертываний кластера пакета HPC на виртуальных машинах.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-174">This topology is required for all HPC Pack cluster deployments in VMs.</span></span> <span data-ttu-id="8e2b9-175">Не изменяйте топологию сети кластера hello позже.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-175">Do not change hello cluster network topology later.</span></span>
2. <span data-ttu-id="8e2b9-176">**Переведите hello вычислительных узлов сети toorun заданий**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-176">**Bring hello compute nodes online toorun jobs**</span></span>
   
    <span data-ttu-id="8e2b9-177">Выбор узлов hello и использовать hello **перевести в оперативный режим** в диспетчере кластера HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-177">Select hello nodes and use hello **Bring Online** action in HPC Cluster Manager.</span></span> <span data-ttu-id="8e2b9-178">узлы Hello — Готово toorun заданий.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-178">hello nodes are ready toorun jobs.</span></span>
3. <span data-ttu-id="8e2b9-179">**Отправка задания toohello кластера**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-179">**Submit jobs toohello cluster**</span></span>
   
    <span data-ttu-id="8e2b9-180">Подключение toohello головного узла toosubmit заданий или настройте toodo на локальном компьютере это.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-180">Connect toohello head node toosubmit jobs, or set up an on-premises computer toodo this.</span></span> <span data-ttu-id="8e2b9-181">Сведения см. в разделе [кластера tooan отправки заданий HPC в Azure](../../virtual-machines-windows-hpcpack-cluster-submit-jobs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-181">For information, see [Submit Jobs tooan HPC cluster in Azure](../../virtual-machines-windows-hpcpack-cluster-submit-jobs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>
4. <span data-ttu-id="8e2b9-182">**Take hello узлов вне сети и остановки (освобождения) их**</span><span class="sxs-lookup"><span data-stu-id="8e2b9-182">**Take hello nodes offline and stop (deallocate) them**</span></span>
   
    <span data-ttu-id="8e2b9-183">После завершения выполняющихся заданий, перевести узлы hello в автономном режиме в диспетчере кластера HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-183">When you are done running jobs, take hello nodes offline in HPC Cluster Manager.</span></span> <span data-ttu-id="8e2b9-184">Затем с помощью tooshut средства управления Azure их работу.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-184">Then, use Azure management tools tooshut them down.</span></span>

## <a name="run-mpi-applications-on-hello-cluster"></a><span data-ttu-id="8e2b9-185">Выполнение приложений MPI в кластере hello</span><span class="sxs-lookup"><span data-stu-id="8e2b9-185">Run MPI applications on hello cluster</span></span>
### <a name="example-run-mpipingpong-on-an-hpc-pack-cluster"></a><span data-ttu-id="8e2b9-186">Пример. Выполнение команды mpipingpong в кластере HPC Pack</span><span class="sxs-lookup"><span data-stu-id="8e2b9-186">Example: Run mpipingpong on an HPC Pack cluster</span></span>
<span data-ttu-id="8e2b9-187">tooverify развертывания пакета HPC экземпляров hello функцией RDMA, запускаются hello HPC Pack **mpipingpong** на кластере hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-187">tooverify an HPC Pack deployment of hello RDMA-capable instances, run hello HPC Pack **mpipingpong** command on hello cluster.</span></span> <span data-ttu-id="8e2b9-188">**MPIPingPong** постоянно отправляет пакеты данных между спаренными узлами toocalculate задержки и пропускной способности и статистики для сети приложений с поддержкой RDMA hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-188">**mpipingpong** sends packets of data between paired nodes repeatedly toocalculate latency and throughput measurements and statistics for hello RDMA-enabled application network.</span></span> <span data-ttu-id="8e2b9-189">В этом примере показан типичный шаблон для выполнения задания MPI (в этом случае **mpipingpong**) с помощью кластера hello **mpiexec** команды.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-189">This example shows a typical pattern for running an MPI job (in this case, **mpipingpong**) by using hello cluster **mpiexec** command.</span></span>

<span data-ttu-id="8e2b9-190">В этом примере предполагается, вы добавили узлы Azure в конфигурации «повышение tooAzure» ([сценария 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-\(PaaS\) in this article).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-190">This example assumes you added Azure nodes in a “burst tooAzure” configuration ([Scenario 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-\(PaaS\) in this article).</span></span> <span data-ttu-id="8e2b9-191">При развертывании пакета HPC в кластере виртуальных машин Azure будет требуется toomodify hello команда синтаксис toospecify другую группу узлов и задать переменные среды дополнительных toodirect трафика сети toohello сети RDMA.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-191">If you deployed HPC Pack on a cluster of Azure VMs, you’ll need toomodify hello command syntax toospecify a different node group and set additional environment variables toodirect network traffic toohello RDMA network.</span></span>

<span data-ttu-id="8e2b9-192">toorun mpipingpong в кластере hello:</span><span class="sxs-lookup"><span data-stu-id="8e2b9-192">toorun mpipingpong on hello cluster:</span></span>

1. <span data-ttu-id="8e2b9-193">На головном узле hello или на правильно настроенном клиентском компьютере откройте командную строку.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-193">On hello head node or on a properly configured client computer, open a Command Prompt.</span></span>
2. <span data-ttu-id="8e2b9-194">tooestimate задержки между парами узлов в развертыванию прорыва в Azure из четырех узлов типа hello, следующая команда toosubmit mpipingpong toorun задания с пакетами небольшого размера и много итераций:</span><span class="sxs-lookup"><span data-stu-id="8e2b9-194">tooestimate latency between pairs of nodes in an Azure burst deployment of four nodes, type hello following command toosubmit a job toorun mpipingpong with a small packet size and many iterations:</span></span>
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 1:100000 -op -s nul
    ```
   
    <span data-ttu-id="8e2b9-195">Hello команда возвращает идентификатор hello hello задания.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-195">hello command returns hello ID of hello job that is submitted.</span></span>
   
    <span data-ttu-id="8e2b9-196">Если вы развернули кластера HPC Pack hello, развернутом на виртуальных машинах Azure, укажите группу узлов, которая содержит ВМ вычислительных узлов развернуты в одной облачной службе и изменения hello **mpiexec** следующим образом:</span><span class="sxs-lookup"><span data-stu-id="8e2b9-196">If you deployed hello HPC Pack cluster deployed on Azure VMs, specify a node group that contains compute node VMs deployed in a single cloud service, and modify hello **mpiexec** command as follows:</span></span>
   
    ```Command
    job submit /nodegroup:vmcomputenodes /numnodes:4 mpiexec -c 1 -affinity -env MSMPI_DISABLE_SOCK 1 -env MSMPI_PRECONNECT all -env MPICH_NETMASK 172.16.0.0/255.255.0.0 mpipingpong -p 1:100000 -op -s nul
    ```
3. <span data-ttu-id="8e2b9-197">По завершении заданий hello tooview hello выходных данных (в данном случае hello выходных данных задачи 1 задания hello), тип hello после</span><span class="sxs-lookup"><span data-stu-id="8e2b9-197">When hello job completes, tooview hello output (in this case, hello output of task 1 of hello job), type hello following</span></span>
   
    ```Command
    task view <JobID>.1
    ```
   
    <span data-ttu-id="8e2b9-198">где &lt; *JobID* &gt; hello идентификатор отправленного задания hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-198">where &lt;*JobID*&gt; is hello ID of hello job that was submitted.</span></span>
   
    <span data-ttu-id="8e2b9-199">Hello выходные данные содержат следующие toohello аналогичные результаты задержки.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-199">hello output includes latency results similar toohello following.</span></span>
   
    ![Задержка проверки связи][pingpong1]
4. <span data-ttu-id="8e2b9-201">tooestimate пропускной способности между парами Azure burst узлы, тип hello следующая команда toosubmit toorun задания **mpipingpong** с пакетами большого размера и несколько итераций:</span><span class="sxs-lookup"><span data-stu-id="8e2b9-201">tooestimate throughput between pairs of Azure burst nodes, type hello following command toosubmit a job toorun **mpipingpong** with a large packet size and a few iterations:</span></span>
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 4000000:1000 -op -s nul
    ```
   
    <span data-ttu-id="8e2b9-202">Hello команда возвращает идентификатор hello hello задания.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-202">hello command returns hello ID of hello job that is submitted.</span></span>
   
    <span data-ttu-id="8e2b9-203">На кластере HPC Pack, развернутом на виртуальных машинах Azure измените команду hello, как указано в шаге 2.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-203">On an HPC Pack cluster deployed on Azure VMs, modify hello command as noted in step 2.</span></span>
5. <span data-ttu-id="8e2b9-204">По завершении заданий hello tooview hello выходных данных (в данном случае hello выходных данных задачи 1 задания hello), тип hello следующие:</span><span class="sxs-lookup"><span data-stu-id="8e2b9-204">When hello job completes, tooview hello output (in this case, hello output of task 1 of hello job), type hello following:</span></span>
   
    ```Command
    task view <JobID>.1
    ```
   
   <span data-ttu-id="8e2b9-205">Hello выходные данные содержат следующие toohello аналогичные результаты пропускной способности.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-205">hello output includes throughput results similar toohello following.</span></span>
   
   ![Пропускная способность при «пинг-понговой» передаче данных][pingpong2]

### <a name="mpi-application-considerations"></a><span data-ttu-id="8e2b9-207">Рекомендации по приложениям MPI</span><span class="sxs-lookup"><span data-stu-id="8e2b9-207">MPI application considerations</span></span>
<span data-ttu-id="8e2b9-208">Ниже приведены рекомендации по запуску в Azure приложений MPI с пакетом HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-208">Following are considerations for running MPI applications with HPC Pack in Azure.</span></span> <span data-ttu-id="8e2b9-209">Некоторые применимы только toodeployments узлов Azure (экземпляры рабочих ролей добавлены в конфигурацию «повышение tooAzure»).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-209">Some apply only toodeployments of Azure nodes (worker role instances added in a “burst tooAzure” configuration).</span></span>

* <span data-ttu-id="8e2b9-210">Экземпляры рабочих ролей в облачной службе периодически повторно подготавливаются без уведомления системой Azure (например, для обслуживания системы или в случае сбоя экземпляра).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-210">Worker role instances in a cloud service are periodically reprovisioned without notice by Azure (for example, for system maintenance, or in case an instance fails).</span></span> <span data-ttu-id="8e2b9-211">Если экземпляр провизионируется повторно при выполнении задания MPI, экземпляр hello теряет свои данные и возвращает состояние toohello при ее первоначального развертывания, что может привести к toofail задания MPI hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-211">If an instance is reprovisioned while it is running an MPI job, hello instance loses its data and returns toohello state when it was first deployed, which can cause hello MPI job toofail.</span></span> <span data-ttu-id="8e2b9-212">Hello дополнительные узлы, используется для выполнения одного задания MPI и hello больше hello задание запускается, hello, скорее всего, что один из экземпляров hello провизионируется повторно во время выполнения задания.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-212">hello more nodes that you use for a single MPI job, and hello longer hello job runs, hello more likely that one of hello instances is reprovisioned while a job is running.</span></span> <span data-ttu-id="8e2b9-213">Также учитывать при назначении одного узла в развертывании hello как файловый сервер.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-213">Also consider this if you designate a single node in hello deployment as a file server.</span></span>
* <span data-ttu-id="8e2b9-214">toorun заданий MPI в Azure, у вас нет toouse hello функцией RDMA экземпляры.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-214">toorun MPI jobs in Azure, you don't have toouse hello RDMA-capable instances.</span></span> <span data-ttu-id="8e2b9-215">Вы можете использовать экземпляр любого размера, поддерживаемый пакетом HPC.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-215">You can use any instance size that is supported by HPC Pack.</span></span> <span data-ttu-id="8e2b9-216">Тем не менее имеющих функцию RDMA экземпляры hello рекомендуется использовать для выполнения относительно крупномасштабных заданий MPI, которые являются конфиденциальных toohello задержка и пропускная способность hello hello сети, соединяющей узлы hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-216">However, hello RDMA-capable instances are recommended for running relatively large-scale MPI jobs that are sensitive toohello latency and hello bandwidth of hello network that connects hello nodes.</span></span> <span data-ttu-id="8e2b9-217">При использовании других заданий MPI размеры toorun задержками и пропускной способности, рекомендуется выполнять мелкие задания, в которых каждая отдельная задача работает только на нескольких узлах.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-217">If you use other sizes toorun latency- and bandwidth-sensitive MPI jobs, we recommend running small jobs, in which a single task runs on only a few nodes.</span></span>
* <span data-ttu-id="8e2b9-218">Приложения, развернутые tooAzure экземпляры являются toohello субъекта, связанных с приложением hello условия лицензии.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-218">Applications deployed tooAzure instances are subject toohello licensing terms associated with hello application.</span></span> <span data-ttu-id="8e2b9-219">Проверьте у поставщика hello коммерческих приложений для лицензирования или других ограничений для выполнения в облаке hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-219">Check with hello vendor of any commercial application for licensing or other restrictions for running in hello cloud.</span></span> <span data-ttu-id="8e2b9-220">Не все поставщики предлагают лицензирование с оплатой по мере использования.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-220">Not all vendors offer pay-as-you-go licensing.</span></span>
* <span data-ttu-id="8e2b9-221">Экземпляры Azure требуется дальнейшей установки tooaccess локальных узлов, общие ресурсы и серверы лицензирования.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-221">Azure instances need further setup tooaccess on-premises nodes, shares, and license servers.</span></span> <span data-ttu-id="8e2b9-222">В примере hello tooenable tooaccess узлов Azure к серверу лицензий в локальной среде, можно настроить сайт сайт виртуальной сети Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-222">For example, tooenable hello Azure nodes tooaccess an on-premises license server, you can configure a site-to-site Azure virtual network.</span></span>
* <span data-ttu-id="8e2b9-223">toorun приложений MPI на экземплярах Azure зарегистрировать каждое приложение MPI в брандмауэре Windows на экземплярах hello, запустив hello **hpcfwutil** команды.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-223">toorun MPI applications on Azure instances, register each MPI application with Windows Firewall on hello instances by running hello **hpcfwutil** command.</span></span> <span data-ttu-id="8e2b9-224">Это позволяет месте tootake MPI связь через порт, динамически назначаемый брандмауэром hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-224">This allows MPI communications tootake place on a port that is assigned dynamically by hello firewall.</span></span>
  
  > [!NOTE]
  > <span data-ttu-id="8e2b9-225">Для развертываний повышения tooAzure можно также настроить toorun команду исключения брандмауэра автоматически на всех новых узлах Azure, которые добавляются tooyour кластера.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-225">For burst tooAzure deployments, you can also configure a firewall exception command toorun automatically on all new Azure nodes that are added tooyour cluster.</span></span> <span data-ttu-id="8e2b9-226">После запуска hello **hpcfwutil** команды и убедитесь, что работает вашего приложения, добавить hello команды скрипта запуска tooa для узлов Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-226">After you run hello **hpcfwutil** command and verify that your application works, add hello command tooa startup script for your Azure nodes.</span></span> <span data-ttu-id="8e2b9-227">Дополнительные сведения см. в статье [Использование скрипта запуска для узлов Azure](https://technet.microsoft.com/library/jj899632.aspx).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-227">For more information, see [Use a Startup Script for Azure Nodes](https://technet.microsoft.com/library/jj899632.aspx).</span></span>
  > 
  > 
* <span data-ttu-id="8e2b9-228">Пакет HPC использует hello CCP_MPI_NETMASK кластера среды переменной toospecify диапазона допустимых адресов для связи MPI.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-228">HPC Pack uses hello CCP_MPI_NETMASK cluster environment variable toospecify a range of acceptable addresses for MPI communication.</span></span> <span data-ttu-id="8e2b9-229">Начиная с HPC Pack 2012 R2, переменная среды кластера CCP_MPI_NETMASK hello влияет только на связь MPI между вычислительными узлами кластера, присоединенных к домену (локально или на виртуальных машинах Azure).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-229">Starting in HPC Pack 2012 R2, hello CCP_MPI_NETMASK cluster environment variable only affects MPI communication between domain-joined cluster compute nodes (either on-premises or in Azure VMs).</span></span> <span data-ttu-id="8e2b9-230">переменная Hello обрабатывается узлами, добавленными в конфигурацию tooAzure повышение.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-230">hello variable is ignored by nodes added in a burst tooAzure configuration.</span></span>
* <span data-ttu-id="8e2b9-231">Задания MPI не могут выполняться между экземплярами Azure, развернутых в разных облачных службах (например, в развертываниях tooAzure пакетов с разными шаблонами узлов или вычислительные узлы виртуальной Машины Azure развернут в нескольких облачных службах).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-231">MPI jobs can't run across Azure instances that are deployed in different cloud services (for example, in burst tooAzure deployments with different node templates, or Azure VM compute nodes deployed in multiple cloud services).</span></span> <span data-ttu-id="8e2b9-232">При наличии нескольких развертываний узлов Azure, которые запускаются на разных шаблонах узлов, задание MPI hello должно выполняться только на одном наборе узлов Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-232">If you have multiple Azure node deployments that are started with different node templates, hello MPI job must run on only one set of Azure nodes.</span></span>
* <span data-ttu-id="8e2b9-233">Добавление узлов Azure tooyour кластера при их подключением к сети, hello служба планировщика заданий HPC немедленно предпринимает toostart заданий на узлах hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-233">When you add Azure nodes tooyour cluster and bring them online, hello HPC Job Scheduler Service immediately tries toostart jobs on hello nodes.</span></span> <span data-ttu-id="8e2b9-234">Если только часть рабочей нагрузки можно запустить в Azure, убедитесь, что обновление или создание задания шаблоны toodefine задания, какие типы можно запустить в Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-234">If only a portion of your workload can run on Azure, ensure that you update or create job templates toodefine what job types can run on Azure.</span></span> <span data-ttu-id="8e2b9-235">Например tooensure, заданий, отправленные с помощью шаблона задания выполняться только на узлах Azure, добавьте шаблон задания toohello свойство группы узлов hello — AzureNodes — hello необходимое значение.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-235">For example, tooensure that jobs submitted with a job template only run on Azure nodes, add hello Node Groups property toohello job template and select AzureNodes as hello required value.</span></span> <span data-ttu-id="8e2b9-236">toocreate настраиваемых групп для своих узлов Azure с помощью командлета HPC PowerShell Add-HpcGroup hello.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-236">toocreate custom groups for your Azure nodes, use hello Add-HpcGroup HPC PowerShell cmdlet.</span></span>

## <a name="next-steps"></a><span data-ttu-id="8e2b9-237">Дальнейшие действия</span><span class="sxs-lookup"><span data-stu-id="8e2b9-237">Next steps</span></span>
* <span data-ttu-id="8e2b9-238">Как альтернативный toousing HPC Pack разрабатывать с toorun служба пакетной службы Azure hello приложений MPI на управляемых пулов вычислительных узлов в Azure.</span><span class="sxs-lookup"><span data-stu-id="8e2b9-238">As an alternative toousing HPC Pack, develop with hello Azure Batch service toorun MPI applications on managed pools of compute nodes in Azure.</span></span> <span data-ttu-id="8e2b9-239">В разделе [несколькими экземплярами использования задачи toorun приложений интерфейса передачи сообщений (MPI) в пакете Azure](../../../batch/batch-mpi.md).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-239">See [Use multi-instance tasks toorun Message Passing Interface (MPI) applications in Azure Batch](../../../batch/batch-mpi.md).</span></span>
* <span data-ttu-id="8e2b9-240">Если требуется toorun Linux MPI. в разделе приложений, обращающихся к сети Azure RDMA hello, [настройки приложений MPI toorun кластеров Linux RDMA](../../linux/classic/rdma-cluster.md).</span><span class="sxs-lookup"><span data-stu-id="8e2b9-240">If you want toorun Linux MPI applications that access hello Azure RDMA network, see [Set up a Linux RDMA cluster toorun MPI applications](../../linux/classic/rdma-cluster.md).</span></span>

<!--Image references-->
[burst]:media/hpcpack-rdma-cluster/burst.png
[iaas]:media/hpcpack-rdma-cluster/iaas.png
[pingpong1]:media/hpcpack-rdma-cluster/pingpong1.png
[pingpong2]:media/hpcpack-rdma-cluster/pingpong2.png
