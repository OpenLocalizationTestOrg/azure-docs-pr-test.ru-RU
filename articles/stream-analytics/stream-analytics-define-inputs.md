---
title: "Подключение данных: потоки входных данных из потока событий | Документация Майкрософт"
description: "Узнайте больше о настройке подключения данных к Stream Analytics, которые называются входными. К входным данным относятся поток данных из событий, а также справочные данные."
keywords: "поток данных, подключение данных, поток событий"
services: stream-analytics
documentationcenter: 
author: SnehaGunda
manager: kfile
editor: cgronlun
ms.assetid: 8155823c-9dd8-4a6b-8393-34452d299b68
ms.service: stream-analytics
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: data-services
ms.date: 12/11/2017
ms.author: sngun
ms.openlocfilehash: e8b55269e861dc010c911491d52973b674dd50ca
ms.sourcegitcommit: aaba209b9cea87cb983e6f498e7a820616a77471
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/12/2017
---
# <a name="data-connection-learn-about-data-stream-inputs-from-events-to-stream-analytics"></a>Подключение данных: узнайте о потоках входных данных из событий в Stream Analytics
Подключение данных к заданию Stream Analytics — это поток событий из источника данных, который называется *входными данными* задания. Stream Analytics полностью интегрируется с источниками потоков данных Azure, включая [Концентраторы событий Azure](https://azure.microsoft.com/services/event-hubs/), [Центр Интернета вещей Azure](https://azure.microsoft.com/services/iot-hub/) и [хранилище BLOB-объектов Azure](https://azure.microsoft.com/services/storage/blobs/). Эти источники входных данных могут быть из той же подписки Azure, что и задание аналитики, или из другой подписки.

## <a name="data-input-types-data-stream-and-reference-data"></a>Типы входных данных: поток данных и справочные данные
Данные, отправляемые в источник данных, принимаются заданием Stream Analytics и обрабатываются в режиме реального времени. Входные данные делятся на два типа: входные потоковые данные и входные справочные данные.

### <a name="data-stream-inputs"></a>Входные потоковые данные
Поток данных — это несвязанная последовательность событий в динамике по времени. Задания Stream Analytics должны включать хотя бы один входной поток данных. В качестве источников входных потоковых данных могут выступать хранилища больших двоичных объектов, концентраторы событий и центры Интернета вещей. Концентраторы событий используются для сбора потоков событий из нескольких устройств и служб. Это могут быть ленты новостей социальных сетей, сведения о торговле акциями или данные датчиков. Центры Интернета вещей оптимизированы для сбора данных с подключенных устройств в сценариях Интернета вещей.  Хранилище больших двоичных объектов может использоваться в качестве источника входных данных при сборе массовых данных в виде потока, например файлов журналов.  

### <a name="reference-data"></a>Ссылочные данные
Stream Analytics также поддерживает входные данные, называемые *ссылочными данными*. Это вспомогательные данные, которые статичны или медленно изменяются. Они обычно используются для корреляции и поисков. Например, можно соединить входные потоковые данные со ссылочными данными так же, как вы бы выполнили соединение SQL для поиска статических значений. Хранилище больших двоичных объектов Azure — единственный поддерживаемый источник входных эталонных данных. Максимальный размер больших двоичных объектов в источнике ссылочных данных — 100 МБ.

Сведения о том, как создать входные ссылочные данные, см. в статье [Использование ссылочных данных и таблиц подстановки во входном потоке Stream Analytics](stream-analytics-use-reference-data.md).  

## <a name="compression"></a>Сжатие

Azure Stream Analytics поддерживает функцию сжатия во всех источниках входных данных потока (концентраторах событий, Центрах Интернета вещей и хранилищ BLOB-объектов). Эта возможность добавляет новый параметр раскрывающегося списка в колонке **Новые входные данные** на портале Azure, позволяя при необходимости выбрать потоки данных для сжатия. В настоящее время поддерживается ссылку типы являются None, GZip и сжатие Deflate. Поддержка сжатия не доступны для ссылочных данных.

Не нужно указывать тип сжатия с сериализацией Avro. Если входные данные Avro сжаты, он обрабатывается прозрачно. 

## <a name="create-data-stream-input-from-event-hubs"></a>Создание входного потока данных из концентраторов событий

Концентраторы событий Azure предоставляют высокомасштабируемую службу приема данных о событиях публикации и подписки. Концентратор событий может обрабатывать миллионы событий в секунду, позволяя вам обрабатывать и анализировать огромное количество данных, создаваемых подключенными устройствами и приложениями. Вместе концентраторы событий и Stream Analytics предоставляют законченное решение для аналитики в реальном времени. Концентраторы событий позволяют отправлять события в Azure в реальном времени, а задания Stream Analytics — их обрабатывать. Например, в концентраторы событий можно отправлять сведения о щелчках, показания датчиков или журналы сетевых событий. Затем можно создать задания Stream Analytics, которые используют концентраторы событий в качестве входных потоков данных для фильтрации в режиме реального времени, выполнения агрегации и корреляции.

По умолчанию метка времени событий, поступающих из концентраторов событий в Stream Analytics — это метка времени поступления события в события концентратора, которая является `EventEnqueuedUtcTime`. Для обработки данных как потока с помощью метки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx).

### <a name="consumer-groups"></a>Группы получателей
Каждый концентратор событий Stream Analytics нужно настроить таким образом, чтобы у него была собственная группа получателей. Если задание включает самосоединение или несколько источников входных данных, некоторые входные данные могут последовательно считываться несколькими модулями чтения. Эта ситуация влияет на количество модулей чтения в группе получателей. Чтобы не превысить лимит на количество модулей чтения для концентратора событий (5 на каждую группу получателей в разделе), рекомендуется назначить группу получателей для каждого задания Stream Analytics. У каждого концентратора событий должно быть не более 20 групп получателей. Дополнительные сведения см. в [руководстве по программированию концентраторов событий](../event-hubs/event-hubs-programming-guide.md).

### <a name="configure-an-event-hub-as-a-data-stream-input"></a>Настройка концентратора событий в качестве входного потока данных
В следующей таблице описываются все параметры в колонке **Новые входные данные** на портале Azure при настройке концентратора событий в качестве источника входных данных.

| Свойство | ОПИСАНИЕ |
| --- | --- |
| **Псевдоним входных данных** |Понятное имя, используемое в запросах задания для ссылки на эти входные данные. |
| **Пространство имен служебной шины** |Пространство имен служебной шины Azure — это контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий создается также пространство имен служебной шины. |
| **Имя концентратора событий** |Имя концентратора событий для использования в качестве источника входных данных. |
| **Имя политики концентратора событий** |Политика общего доступа, которая предоставляет доступ к концентратору событий. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| **Группа получателей концентратора событий** (необязательное свойство) |Группа получателей, принимающих данные из концентратора событий. Если группа получателей не указана, задание Stream Analytics использует группу получателей по умолчанию. Для каждого задания Stream Analytics рекомендуется использовать отдельную группу получателей. |
| **Формат сериализации событий** |Формат сериализации (JSON, CSV или Avro) входного потока данных. |
| **Кодирование** | Сейчас UTF-8 — единственный поддерживаемый формат кодировки. |
| **Сжатие** (необязательно) | Тип сжатия (None, GZip или Deflate) входящего потока данных. |

При поступлении данных из концентратора событий запрос Stream Analytics может получить доступ к следующим полям метаданных:

| Свойство | ОПИСАНИЕ |
| --- | --- |
| **EventProcessedUtcTime** |Дата и время обработки события службой Stream Analytics. |
| **EventEnqueuedUtcTime** |Дата и время получения события концентраторами событий. |
| **PartitionId** |Идентификатор секции для входного адаптера (нумерация идет от нуля). |

Например, используя эти поля, можно писать запросы, как в следующем примере:

````
SELECT
    EventProcessedUtcTime,
    EventEnqueuedUtcTime,
    PartitionId
FROM Input
````

> [!NOTE]
> При использовании концентратора события в качестве конечной точки для маршрутов Центра Интернета вещей вы можете получить доступ к метаданным Центра Интернета вещей с помощью [функции GetMetadataPropertyValue](https://msdn.microsoft.com/en-us/library/azure/mt793845.aspx).
> 

## <a name="create-data-stream-input-from-iot-hub"></a>Создание входного потока данных из Центра Интернета вещей
Центр Azure IoT — это высокомасштабируемая служба приема данных о событиях публикации и подписки, оптимизированная под сценарии "Интернет вещей".

По умолчанию метка времени событий, поступающих из Центра Интернета вещей в Stream Analytics — это метка времени поступления события в концентратор Центра Интернета вещей, то есть `EventEnqueuedUtcTime`. Для обработки данных как потока с помощью метки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx).

> [!NOTE]
> Только сообщения, отправленные со свойством `DeviceClient`, могут быть обработаны.
> 
> 

### <a name="consumer-groups"></a>Группы получателей
Каждый Центр Интернета вещей Stream Analytics нужно настроить таким образом, чтобы у него была собственная группа получателей. Если задание включает самосоединение или несколько источников входных данных, некоторые входные данные могут последовательно считываться несколькими модулями чтения. Эта ситуация влияет на количество модулей чтения в группе получателей. Чтобы не превысить лимит на количество модулей чтения для Центра Интернета вещей (5 на каждую группу получателей в разделе), рекомендуется назначить группу получателей для каждого задания Stream Analytics.

### <a name="configure-an-iot-hub-as-a-data-stream-input"></a>Настройка Центра Интернета вещей в качестве входного потока данных
В следующей таблице описываются все параметры в колонке **Новые входные данные** на портале Azure при настройке Центра Интернета вещей в качестве источника входных данных.

| Свойство | ОПИСАНИЕ |
| --- | --- |
| **Псевдоним входных данных** |Понятное имя, используемое в запросах задания для ссылки на эти входные данные.|
| **Центр Интернета вещей** |Имя Центра Интернета вещей для использования в качестве источника входных данных. |
| **Конечная точка** |Конечная точка для Центра Интернета вещей.|
| **Имя политики общего доступа** |Политика общего доступа, которая предоставляет доступ к Центру Интернета вещей. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| **Ключ политики общего доступа** |Ключ общего доступа, используемый для авторизации доступа к Центру Интернета вещей. |
| **Группа получателей** (необязательное свойство) |Группа получателей, принимающих данные из Центра Интернета вещей. Если группа получателей не указана, задание Stream Analytics использует группу получателей по умолчанию. Для каждого задания Stream Analytics рекомендуется использовать отдельную группу получателей. |
| **Формат сериализации событий** |Формат сериализации (JSON, CSV или Avro) входного потока данных. |
| **Кодирование** |Сейчас UTF-8 — единственный поддерживаемый формат кодировки. |
| **Сжатие** (необязательно) | Тип сжатия (None, GZip или Deflate) входящего потока данных. |

При поступлении данных из Центра Интернета вещей запрос Stream Analytics может получить доступ к следующим полям метаданных:

| Свойство | ОПИСАНИЕ |
| --- | --- |
| **EventProcessedUtcTime** |Дата и время обработки события. |
| **EventEnqueuedUtcTime** |Дата и время получения события Центром Интернета вещей. |
| **PartitionId** |Идентификатор секции для входного адаптера (нумерация идет от нуля). |
| **IoTHub.MessageId** | Идентификатор, используемый для корреляции двустороннего обмена данными в Центре Интернета вещей. |
| **IoTHub.CorrelationId** |Идентификатор, используемый в ответах на сообщение и отзывах в Центре Интернета вещей. |
| **IoTHub.ConnectionDeviceId** |Идентификатор проверки подлинности, используемый для отправки этого сообщения. Это значение помещается в сообщения, связанные со службой, Центром Интернета вещей. |
| **IoTHub.ConnectionDeviceGenerationId** |Идентификатор создания устройства, прошедшего проверку подлинности, используемый для отправки этого сообщения. Это значение помещается в сообщения, связанные со службой, Центром Интернета вещей. |
| **IoTHub.EnqueuedTime** |Время, когда Центр Интернета вещей получил сообщение. |
| **IoTHub.StreamId** |Настраиваемое свойство события, добавленное устройством отправителя. |


## <a name="create-data-stream-input-from-blob-storage"></a>Создание входного потока данных из хранилища больших двоичных объектов
Хранилище больших двоичных объектов Azure служит экономичным и масштабируемым решением в сценариях, связанных с хранением больших объемов неструктурированных данных в облаке. Данные в хранилище больших двоичных объектов обычно считаются неактивными. Тем не менее их можно обрабатывать как поток данных в Stream Analytics. Обычный сценарий для обработки входных данных хранилища больших двоичных объектов с помощью Stream Analytics — это обработка журнала. В этом сценарии данные телеметрии, полученные из системы, необходимо проанализировать и обработать, чтобы извлечь значимые данные.

По умолчанию метка времени событий хранилища больших двоичных объектов в Stream Analytics — это метка времени последнего изменения большого двоичного объекта, то есть `BlobLastModifiedUtcTime`. Для обработки данных как потока с помощью метки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx).

Входным данным в формате CSV *необходимо*, чтобы строка заголовка определяла поля для набора данных. Кроме того, все поля заголовка строки должны быть уникальными.

> [!NOTE]
> Stream Analytics не поддерживает добавление содержимого в существующий файл большого двоичного объекта. Stream Analytics просматривает каждый файл только один раз, и любые изменения, которые произойдут в нем после того, как задание прочитает данные, не обрабатываются. Мы рекомендуем отправлять все данные для файла большого двоичного объекта за один раз, а затем добавлять более новые события в другой новый файл большого двоичного объекта.
> 

### <a name="configure-blob-storage-as-a-data-stream-input"></a>Настройка хранилища BLOB-объектов в качестве потока входных данных

В следующей таблице описываются все параметры в колонке **Новые входные данные** на портале Azure при настройке хранилища больших двоичных объектов в качестве входных данных.

| Свойство | ОПИСАНИЕ |
| --- | --- |
| **Псевдоним входных данных** | Понятное имя, используемое в запросах задания для ссылки на эти входные данные. |
| **Учетная запись хранения** | Имя учетной записи хранения, в которой находятся файлы больших двоичных объектов. |
| **Ключ учетной записи хранения** | Секретный ключ, связанный с учетной записью хранения. |
| **Контейнер** | Контейнер для входных данных большого двоичного объекта. Контейнеры обеспечивают логическую группировку BLOB-объектов, хранящихся в службе BLOB-объектов Microsoft Azure. При передаче большого двоичного объекта в службу хранилища BLOB-объектов для него необходимо указать контейнер. |
| **Шаблон пути** (необязательно) | Путь к файлу, используемый для поиска больших двоичных объектов в указанном контейнере. В пути можно указать один или более экземпляров следующих трех переменных: `{date}`, `{time}` или `{partition}`.<br/><br/>Пример 1: `cluster1/logs/{date}/{time}/{partition}`<br/><br/>Пример 2: `cluster1/logs/{date}`<br/><br/>Символ `*` является недопустимым значением для префикса пути. Допустимыми являются только <a HREF="https://msdn.microsoft.com/library/azure/dd135715.aspx">символы больших двоичных объектов Azure</a>. |
| **Формат даты** (необязательное свойство) | При использовании переменной даты в пути это формат даты, по которому упорядочены файлы. Пример: `YYYY/MM/DD` |
| **Формат времени** (необязательное свойство) |  При использовании переменной времени в пути это формат времени, в котором размещаются файлы. В настоящее время единственное поддерживаемое значение — это `HH`. |
| **Формат сериализации событий** | Формат сериализации (JSON, CSV или Avro) для входного потока данных. |
| **Кодирование** | В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| **Сжатие** (необязательно) | Тип сжатия (None, GZip или Deflate) входящего потока данных. |

При поступлении данных из хранилища больших двоичных объектов запрос Stream Analytics может получить доступ к следующим полям метаданных:

| Свойство | ОПИСАНИЕ |
| --- | --- |
| **BlobName** |Имя входного большого двоичного объекта, от которого поступило событие. |
| **EventProcessedUtcTime** |Дата и время обработки события службой Stream Analytics. |
| **BlobLastModifiedUtcTime** |Дата и время последнего изменения большого двоичного объекта. |
| **PartitionId** |Идентификатор секции для входного адаптера (нумерация идет от нуля). |

Например, используя эти поля, можно писать запросы, как в следующем примере:

````
SELECT
    BlobName,
    EventProcessedUtcTime,
    BlobLastModifiedUtcTime
FROM Input
````

## <a name="get-help"></a>Получение справки
За дополнительной помощью обращайтесь на наш [форум Azure Stream Analytics](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics).

## <a name="next-steps"></a>Дальнейшие действия
Вы ознакомились с параметрами подключения данных для заданий Stream Analytics в Azure. Дополнительные сведения о службе Stream Analytics см. в следующих статьях:

* [Приступая к работе с Azure Stream Analytics](stream-analytics-real-time-fraud-detection.md)
* [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
* [Справочник по языку запросов Azure Stream Analytics](https://msdn.microsoft.com/library/azure/dn834998.aspx)
* [Справочник по API-интерфейсу REST управления Stream Analytics](https://msdn.microsoft.com/library/azure/dn835031.aspx)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-real-time-fraud-detection.md
[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: http://go.microsoft.com/fwlink/?LinkId=517301
