---
title: "рекомендации по настройке производительности Storm хранилища Озера данных aaaAzure | Документы Microsoft"
description: "Рекомендации по настройке производительности для Storm в Azure Data Lake Store"
services: data-lake-store
documentationcenter: 
author: stewu
manager: amitkul
editor: stewu
ms.assetid: ebde7b9f-2e51-4d43-b7ab-566417221335
ms.service: data-lake-store
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 12/19/2016
ms.author: stewu
ms.openlocfilehash: 5412fd46cf2373f5877030913df4fe1fc6f5473a
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/06/2017
---
# <a name="performance-tuning-guidance-for-storm-on-hdinsight-and-azure-data-lake-store"></a>Рекомендации по настройке производительности для Storm в HDInsight и Azure Data Lake Store

Понимать hello факторы, которые следует учитывать при настройке производительности hello Azure Storm топологии. Например это toounderstand важные характеристики hello hello работы, выполненной hello spouts и винты hello (hello рабочих является ли ввода-вывода или большим объемом памяти). В этой статье рассматривается ряд рекомендаций по улучшению производительности, в том числе по устранению типичных неполадок.

## <a name="prerequisites"></a>Предварительные требования

* **Подписка Azure**. Ознакомьтесь с [бесплатной пробной версией Azure](https://azure.microsoft.com/pricing/free-trial/).
* **Учетная запись Azure Data Lake Store.** Инструкции о том, как один, см. в разделе toocreate [Приступая к работе с хранилища Озера данных Azure](data-lake-store-get-started-portal.md).
* **Кластер Azure HDInsight** с tooa доступа к учетной записи хранилища Озера данных. См. статью [Создание кластера HDInsight с Data Lake Store с помощью портала Azure](data-lake-store-hdinsight-hadoop-use-portal.md). Убедитесь, что включить удаленный рабочий стол для кластера hello.
* **Запущенный кластер Storm в Data Lake Store**. Дополнительные сведения см. в статье [Основные сведения об Apache Storm в службе HDInsight. Аналитика в реальном времени для Hadoop](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-storm-overview).
* **Рекомендации по настройке производительности для Data Lake Store**.  Общие вопросы производительности описаны в [рекомендациях по настройке](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance).  

## <a name="tune-hello-parallelism-of-hello-topology"></a>Настройка параллелизма hello hello топологии

Возможно, производительность может tooimprove по возрастающей параллелизм hello hello tooand ввода-вывода из хранилища Озера данных. Топология Storm имеет набор конфигураций, которые определяют hello параллелизма.
* Число рабочих процессов (hello работников равномерно распределяются между hello виртуальные машины).
* Количество экземпляров исполнителей spout.
* Количество экземпляров исполнителей bolt.
* Количество задач spout.
* Количество задач bolt.

В кластере с 4 виртуальных машин и 4 рабочих процессов, 32 spout исполнителей и 32 spout задач и 256 молнии исполнителей и задач 512 молнии, рассмотрим следующие hello:

Каждый супервизор, то есть рабочий узел, имеет один рабочий процесс виртуальной машины Java (JVM). Этот процесс JVM управляет 4-мя потоками spout и 64-ю потоками bolt. Внутри каждого потока задачи выполняются последовательно. Каждый поток spout имеет 1 задача hello предшествующий конфигурации, и каждый поток молнии имеет 2 задачи.

В Storm Вот различные компоненты, участвующие hello и их влиянии на hello уровень параллелизма, к которым у вас есть:
* используется toosubmit и управление заданиями Hello головного узла (именем Nimbus в Storm). Эти узлы не оказывают влияния на hello степень параллелизма.
* узлы начальника Hello. В HDInsight соответствует tooa рабочий узел виртуальной Машины Azure.
* Hello рабочим задачам, Storm процессы, запущенные в hello виртуальных машин. Каждая рабочая задача соответствует tooa экземпляр виртуальной машины Java. Storm распределяет hello число рабочих процессов укажите toohello рабочих узлов максимально равномерно.
* Количество экземпляров исполнителей spout и bolt. Каждый экземпляр исполнителя соответствует tooa потока, запущенного в рамках рабочих процессов hello (JVM).
* Задачи Storm. Это логические задачи, которые выполняет каждый из потоков. Это не меняет hello степень параллелизма, поэтому следует оценить, если требуется несколько задач на исполнителя или не.

### <a name="get-hello-best-performance-from-data-lake-store"></a>Обеспечения высокой производительности hello из хранилища Озера данных

При работе с хранилищем данных Озера получить hello наилучшей производительности, если hello следующие:
* Объедините маленькие добавления в пакеты большего размера (в идеале размером по 4 МБ).
* Выполняйте максимально возможное количество одновременных запросов. Так как каждый поток молнии выполняет блокирующих операций чтения, следует toohave в любом месте hello диапазона 8-12 потока на ядро. В этом случае hello сетевых КАРТ и hello ресурсов ЦП также. Крупная виртуальная машина позволяет выполнять большее число параллельных запросов.  

### <a name="example-topology"></a>Пример топологии

Предположим, что у вас есть кластер из 8 рабочих узлов на виртуальной машине Azure D13v2. Эта виртуальная машина имеет 8 ядер, поэтому среди Здравствуйте 8 рабочих узлов, у вас есть 64 общего количества ядер.

Предположим, что мы запускаем 8 потоков элементов bolt на ядро. Так как у нас есть 64 ядра, мы получаем 512 экземпляров исполнителей элемента bolt (т. е. потоков). В этом случае предположим, что мы начать с одной виртуальной машины Java на виртуальную Машину затем главным образом параллелизме потоков hello в tooachieve параллелизма hello виртуальной машины Java. Это означает, что нам нужно 8 рабочих задач (по одной на каждую виртуальную машину Azure) и 512 исполнителей элемента bolt. При такой конфигурации Storm пытается toodistribute hello рабочих процессов равномерно между узлами работника (также известный как узлы начальника), предоставляя каждый рабочий узел 1 виртуальной машины Java. Теперь в руководителями hello Storm пытается toodistribute исполнителей Привет между руководителями, предоставляя каждого руководителя (то есть JVM) 8 потоков каждого.

## <a name="tune-additional-parameters"></a>Настройка дополнительных параметров
После того как вы базовая топология hello, можно рассмотреть, следует ли tootweak hello параметры:
* **Число виртуальных машин Java на рабочий узел.** Если вы размещаете в памяти структуру данных большого объема (например, таблицу подстановки), потребуется отдельный экземпляр этой структуры для каждой виртуальной машины Java. Кроме того можно использовать структуры данных hello через много потоков при наличии меньше JVM. Для операций ввода-вывода hello молнии hello число JVM не делает столько различие hello число потоков, которые добавлены в эти JVM. Для простоты это toohave смысл, одной виртуальной машины Java для работника. В зависимости от действия вашей молнии или какое приложение обработка требуется, хотя может потребоваться toochange этот номер.
* **Количество экземпляров исполнителей spout.** Поскольку hello в предыдущем примере элементы для хранилища Озера tooData записи, количество hello spouts не связаны непосредственно toohello молнии производительности. Тем не менее в зависимости от hello объем обработки или ввода-вывода, происходит в hello spout рекомендуется tootune hello spouts для достижения оптимальной производительности. Убедитесь, что имеется достаточно spouts toobe может tookeep hello винты занят. скорость hello spouts Hello выходные данные должны соответствовать hello пропускной способности винты hello. Фактическая конфигурация Hello зависит от hello spout.
* **Число задач.** Каждый элемент bolt выполняется как один поток. Увеличение количества задач для каждого элемента bolt не повышает уровень параллелизма. Hello единственный случай, когда они имеют преимущество затрачивается большую часть времени выполнения молнии Если ваш процесс подтверждения hello кортежа. Это toogroup смысл, который множество кортежей в большее append перед отправкой подтверждения из молнии hello. Поэтому в большинстве случаев увеличение количества задач не дает дополнительных преимуществ.
* **Локальное группирование или группирование в случайном порядке.** Если этот параметр включен, кортежей отправляются toobolts внутри hello же рабочего процесса. Это уменьшает межпроцессное взаимодействие и количество сетевых вызовов. Мы рекомендуем использовать этот параметр для большинства топологий.

Такой вариант хорошо подойдет в качестве базового. Тестирование с предыдущих параметров tooachieve оптимальной производительности tootweak hello собственных данных система.

## <a name="tune-hello-spout"></a>Настройка hello spout

Можно изменить следующие параметры tootune hello spout hello.

- **Время ожидания кортежа: topology.message.timeout.secs**. Этот параметр определяет интервал времени, toocomplete принимает сообщение hello и получения подтверждения, чтобы считать не удалось.

- **Максимальный объем памяти для каждого рабочего процесса: worker.childopts**. Этот параметр позволяет указать дополнительные параметры командной строки toohello Java работников. Hello чаще всего используется параметр здесь — XmX, определяющий JVM hello Максимальная память, выделенную tooa кучи.

- **Максимальное количество ожидающих spout: topology.max.spout.pending**. Этот параметр определяет hello число кортежей, возможного в полете (еще не подтверждено на всех узлах в топологии hello) для потока spout в любое время.

 Toodo хорошо вычисления — tooestimate hello размер каждого из вашего кортежей. и разобраться, сколько памяти выделяется для потока spout. Hello общий объем памяти, выделенный поток tooa, деленное на это значение будет предоставлять hello верхнюю границу hello max spout ожидающих параметра.

## <a name="tune-hello-bolt"></a>Настройка молнии hello
При написании хранилища Озера tooData задать политику синхронизации размер (буфер на стороне клиента hello) too4 МБ. Запись на диск или hsync() затем выполняется только в том случае, если размер буфера hello — hello в это значение. Драйвер хранилища Озера данных Hello в рабочем процессе hello виртуальной Машины автоматически эта буферизация не, пока не выполняется явным образом hsync().

молнии Storm хранилища Озера данных по умолчанию Hello имеет параметр политики синхронизации размер (fileBufferSize), может быть tootune используется этот параметр.

В топологиях O-интенсивной это toohave смысл каждый поток молнии написать собственный файл tooits и tooset файловая политика поворота (fileRotationSize). Если файл hello достигает определенного размера, поток hello автоматически очищается и новый файл записывается в. Рекомендуемый размер файла для поворота Hello составляет 1 ГБ.

### <a name="handle-tuple-data"></a>Обработка данных кортежа

В Storm spout удерживает кортежа tooa пока не будет явно подтверждается hello молнии. Если кортеж считанные молнии hello, но еще не подтверждены, hello spout может не удается сохранить в хранилище Озера данных серверной части. После подтверждения кортеж hello spout может гарантироваться сохраняемости молнии hello и можно удалить из любого источника, она считывает данные из источника данных hello.  

Для обеспечения максимальной производительности в хранилище Озера данных имеют молнии hello буфера 4 МБ данных кортежа. Затем напишите toohello резервное хранилище Озера данных приводят к одной записи 4 МБ. После hello данные были успешно письменного toohello хранилища (вызывающий hflush()) молнии hello можно Подтверждаете spout задней toohello данных hello. Это пример какие hello винта здесь не. Это также допустимого toohold увеличения числа кортежей перед hello hflush() вызов и hello кортежей подтверждения. Однако при этом увеличивается hello число кортежей в полете, которая hello spout требуется toohold, и поэтому увеличивается hello объем памяти, необходимый каждой виртуальной машины Java.

> [!NOTE]
Приложения могут содержать кортежи tooacknowledge требование чаще (на размер данных меньше 4 МБ) по другим причинам не производительности. Тем не менее, которые могут повлиять на серверной части toohello пропускной способности хранилища hello ввода-вывода. Тщательно взвесить этот компромисс с точки зрения производительности операций ввода-вывода hello молнии.

Если интенсивность поступления hello кортежей не велико, поэтому hello 4 МБ буфера будет toofill много времени, рассмотрите возможность устранения это по:
* Уменьшение числа hello винты, поэтому существуют toofill меньшее число буферов.
* Наличие политику на основе времени или на основе количества, где hflush() каждые х удалений или каждые миллисекунд y, а также hello кортежей, собранные до сих подтверждения назад.

Обратите внимание, что hello пропускной способности в этом случае меньше, но с медленным частота возникновения событий, максимальная пропускная способность не крупнейших цель hello все равно. Эти исправления позволяют сократить hello общее время, необходимое для tooflow кортежа через магазин toohello. Это будет полезно, если конвейер должен работать в режиме реального времени даже при низкой частоте событий. Также Обратите внимание, в случае низкой скорости входящих кортежа следует изменить параметр topology.message.timeout_secs hello, поэтому кортежи hello времени ожидания не происходит при их получении в буфер или обработаны.

## <a name="monitor-your-topology-in-storm"></a>Мониторинг топологии в Storm  
Пока выполняется топологии, вы можете отслеживать ее в пользовательском интерфейсе Storm hello. Ниже приведены toolook основными параметрами hello в.

* **Общая задержка выполнения процесса.** Это hello среднее время, затрачиваемое toobe испускаемый hello spout, обрабатываемых молнии hello и подтверждения один кортеж.

* **Общая задержка процесса bolt.** Это hello среднее время, затраченное кортеж hello в молнии hello, пока не получит подтверждение.

* **Общая задержка выполнения bolt.** Это hello среднее время, затраченное молнии hello в hello метод execute.

* **Количество сбоев.** Это относится toohello число кортежей, которые не удалось toobe полностью обработать, прежде чем они истекло время ожидания.

* **Емкость.** Это мера занятости вашей системы. Если это значение равно 1, элементы bolt работают с максимально возможной скоростью. Если это значение меньше 1, увеличение параллелизма hello. Если это значение больше 1, уменьшите параллелизм hello.

## <a name="troubleshoot-common-problems"></a>Устранение распространенных неполадок
Здесь описано несколько типичных сценариев устранения неполадок.
* **Множество кортежей завершается по времени ожидания.** Посмотрите на каждый узел в топологии toodetermine hello, где находится узкое место hello. Hello наиболее распространенной причиной этого является то, винты hello не может tookeep вверх с hello spouts. Это порождает tootuples переполнения hello внутренние буферы при обработке toobe ожидания. Рекомендуется увеличить значение времени ожидания hello или уменьшение hello max spout ожидания.

* **Высокая общая задержка выполнения процесса, но при этом низкая задержка процесса bolt.** В этом случае возможна, hello кортежи не подтверждаемого достаточно быстро. Убедитесь, что имеется достаточное количество подтверждающих. Другая возможность заключается в том, что они ожидают в очереди hello слишком долго, до начала обработки их болтов hello. Уменьшение hello max spout ожидания.

* **Высокая задержка выполнения bolt.** Это означает, что метод execute() hello вашей молнии занимает слишком много времени. Оптимизация кода hello, или просмотрите записи размеры и сбрасывает его поведение.

### <a name="data-lake-store-throttling"></a>Регулирование в Data Lake Store
Если вы столкнулись hello ограничения пропускной способности, предоставляемые хранилища Озера данных, могут происходить сбои задач. Проверьте журналы задач на наличие ошибок регулирования. Можно уменьшить hello параллелизма, увеличив размер контейнера.    

toocheck, если вы начало регулируются, Включение журнала на стороне клиента hello hello отладки:

1. В **Ambari** > **Storm** > **Config** > **Advanced storm-worker-log4j**, изменение  **&lt;корневой уровень = «info»&gt;**  слишком**&lt;корневой уровень = «debug»&gt;**. Перезапустите все hello узлы или службы hello конфигурации tootake эффект.
2. Монитор hello Storm топологии входе рабочих узлов (в разделе /var/log/storm/worker-artifacts /&lt;TopologyName&gt;/&lt;порт&gt;/worker.log) для хранилища Озера данных форме исключений регулирования.

## <a name="next-steps"></a>Дальнейшие действия
Сведения о дополнительных настройках производительности Storm см. в этом [блоге](https://blogs.msdn.microsoft.com/shanyu/2015/05/14/performance-tuning-for-hdinsight-storm-and-microsoft-azure-eventhubs/).

Toorun дополнительный пример в разделе [такой на GitHub](https://github.com/hdinsight/storm-performance-automation).
