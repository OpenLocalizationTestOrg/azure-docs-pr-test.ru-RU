---
title: "Потоковая передача данных из Stream Analytics в Data Lake Store | Документация Майкрософт"
description: "Использование Azure Stream Analytics для потоковой передачи данных в хранилище озера данных Azure"
services: data-lake-store,stream-analytics
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
ms.assetid: edb58e0b-311f-44b0-a499-04d7e6c07a90
ms.service: data-lake-store
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 10/03/2017
ms.author: nitinme
ms.openlocfilehash: 92ddf9619a0db398f7866aab60e834f09add3e7a
ms.sourcegitcommit: 6699c77dcbd5f8a1a2f21fba3d0a0005ac9ed6b7
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/11/2017
---
# <a name="stream-data-from-azure-storage-blob-into-data-lake-store-using-azure-stream-analytics"></a>Потоковая передача данных из большого двоичного объекта службы хранилища Azure в хранилище озера данных с помощью Azure Stream Analytics
Из этой статьи вы узнаете, как использовать хранилище озера данных Azure в качестве источника выходных данных для задания Azure Stream Analytics. В этой статье показан простой сценарий, в котором данные считываются из большого двоичного объекта службы хранилища Azure (входные данные) и записываются в хранилище озера данных (выходные данные).

> [!NOTE]
> В настоящий момент создание и настройка выходных данных Data Lake Store для Stream Analytics поддерживается только на [классическом портале Azure](https://manage.windowsazure.com). Поэтому в некоторых частях этого учебника будет использоваться классический портал Azure.
>
>

## <a name="prerequisites"></a>Предварительные требования
Перед началом работы с этим учебником необходимо иметь следующее:

* **Подписка Azure**. Ознакомьтесь с [бесплатной пробной версией Azure](https://azure.microsoft.com/pricing/free-trial/).

* **Учетная запись хранения Azure.** Контейнер больших двоичных объектов из этой учетной записи будет использоваться для ввода данных для задания Stream Analytics. Для работы с этим руководством предполагается, что у вас есть учетная запись хранилища с именем **storageforasa**, а в ней — контейнер с именем **storageforasacontainer**. После создания контейнера отправьте в него образец файла данных. 
  
* **Учетная запись хранилища озера данных Azure**. Следуйте инструкциям в разделе [Приступая к работе с хранилищем озера данных Azure на портале Azure](data-lake-store-get-started-portal.md). Предположим, у вас есть учетная запись хранилища Data Lake Store **asadatalakestore**. 

## <a name="create-a-stream-analytics-job"></a>Создание задания Stream Analytics
Для начала нужно создать задание Stream Analytics с источником входных данных и целевым объектом для выходных данных. В этом руководстве источником является контейнер больших двоичных объектов Azure, а целевым объектом — хранилище озера данных.

1. Выполните вход на [портал Azure](https://portal.azure.com).

2. В области слева щелкните **Задания Stream Analytics**, а затем нажмите кнопку **Добавить**.

    ![Создание задания Stream Analytics](./media/data-lake-store-stream-analytics/create.job.png "Создание задания Stream Analytics")

    > [!NOTE]
    > Следите за тем, чтобы задание создавалось в той же области, в которой расположена учетная запись хранения. Иначе вам придется заплатить за перемещение данных между регионами.
    >

## <a name="create-a-blob-input-for-the-job"></a>Создание входных данных большого двоичного объекта для задания

1. Откройте страницу задания Stream Analytics, на панели слева перейдите на вкладку **Входные данные** и выберите команду **Добавить**.

    ![Добавление входных данных для задания](./media/data-lake-store-stream-analytics/create.input.1.png "Добавление входных данных для задания")

2. В колонке **Создание входных данных** введите следующие значения.

    ![Добавление входных данных для задания](./media/data-lake-store-stream-analytics/create.input.2.png "Добавление входных данных для задания")

    * **Входной псевдоним** — введите уникальное имя для этих входных данных задания.
    * **Тип источника** — выберите **Поток данных**.
    * **Источник** — выберите **Хранилище больших двоичных объектов**.
    * **Подписка** — выберите **Использовать хранилище BLOB-объектов из текущей подписки**.
    * **Учетная запись хранилища** — выберите учетную запись, которую вы создали при подготовке необходимых условий. 
    * **Контейнер** — выберите контейнер, который вы создали в этой учетной записи.
    * **Формат сериализации событий** — выберите **CSV**.
    * **Разделитель** — выберите **Табуляция**.
    * **Кодировка** — выберите **UTF-8**.

    Щелкните **Создать**. Портал добавит входные данные и проверит подключение к ним.


## <a name="create-a-data-lake-store-output-for-the-job"></a>Создание выходных данных хранилища озера данных для задания

1. Откройте страницу задания Stream Analytics, перейдите на вкладку **Выходные данные** и выберите команду **Добавить**.

    ![Добавление выходных данных для задания](./media/data-lake-store-stream-analytics/create.output.1.png "Добавление выходных данных для задания")

2. В колонке **Новые выходные данные** введите следующие значения.

    ![Добавление выходных данных для задания](./media/data-lake-store-stream-analytics/create.output.2.png "Добавление выходных данных для задания")

    * **Выходной псевдоним** — введите уникальное имя для этих выходных данных задания. Это понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующее хранилище озера данных.
    * **Приемник** — выберите **Data Lake Store**.
    * Появится запрос на авторизацию доступа к учетной записи Data Lake Store. Щелкните **Авторизовать**.

3. В колонке **Новые выходные данные** продолжайте ввод значений.

    ![Добавление выходных данных для задания](./media/data-lake-store-stream-analytics/create.output.3.png "Добавление выходных данных для задания")

    * **Имя учетной записи** — выберите учетную запись Data Lake Store, которую вы создали там, куда хотите отправлять результаты задания.
    * **Шаблон префикса в пути** — введите путь для сохранения файлов в указанной учетной записи Data Lake Store.
    * **Формат даты** — если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов.
    * **Формат времени** — если в префиксе пути используется маркер времени, вы можете выбрать формат времени для упорядочивания своих файлов.
    * **Формат сериализации событий** — выберите **CSV**.
    * **Разделитель** — выберите **Табуляция**.
    * **Кодировка** — выберите **UTF-8**.
    
    Щелкните **Создать**. Портал добавит выходные данные и проверит подключение к ним.
    
## <a name="run-the-stream-analytics-job"></a>Выполнение задания Stream Analytics

1. Чтобы выполнить задание Stream Analytics, необходимо выполнить запрос на вкладке **Запрос**. В этом руководстве можно выполнить образец запроса, заменив заполнители псевдонимами входных и выходных данных, как показано на снимке экрана ниже.

    ![Выполнение запроса](./media/data-lake-store-stream-analytics/run.query.png "Выполнение запроса")

2. Щелкните **Сохранить** в верхней части экрана, а затем на вкладке **Обзор** щелкните **Запустить**. В диалоговом окне выберите **Настраиваемое время** и установите текущие дату и время.

    ![Установка времени задания](./media/data-lake-store-stream-analytics/run.query.2.png "Установка времени задания")

    Щелкните **Пуск**, чтобы начать задание. Для запуска задания может потребоваться несколько минут.

3. Чтобы запустить задание выбора данных из большого двоичного объекта, скопируйте образец файла данных в контейнер больших двоичных объектов. Его можно получить из [репозитория Git Azure Data Lake](https://github.com/Azure/usql/tree/master/Examples/Samples/Data/AmbulanceData/Drivers.txt). В нашем примере мы скопируем файл **vehicle1_09142014.csv**. Чтобы передать данные в контейнер больших двоичных объектов, можно использовать различные клиенты, например [обозреватель хранилищ Azure](http://storageexplorer.com/).

4. На вкладке **Обзор** в разделе **Мониторинг** можно наблюдать за ходом обработки данных.

    ![Мониторинг задания](./media/data-lake-store-stream-analytics/run.query.3.png "Мониторинг задания")

5. А теперь можно проверить, появились ли выходные данные задания в учетной записи Data Lake Store. 

    ![Проверка выходных данных](./media/data-lake-store-stream-analytics/run.query.4.png "Проверка выходных данных")

    В области обозревателя данных можно увидеть, что выходные данные записаны в папку, указанную в параметрах выходных данных Data Lake Store (`streamanalytics/job/output/{date}/{time}`).  

## <a name="see-also"></a>Дополнительные материалы
* [Создание кластера HDInsight для работы с хранилищем озера данных](data-lake-store-hdinsight-hadoop-use-portal.md)
