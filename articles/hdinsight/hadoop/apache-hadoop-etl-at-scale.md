---
title: "Извлечение, преобразование и загрузка (ETL) в масштабе в Azure HDInsight | Документация Майкрософт"
description: "Узнайте об использовании ETL в HDInsight с использованием Hadoop."
services: hdinsight
documentationcenter: 
author: ashishthaps
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 
ms.service: hdinsight
ms.custom: hdinsightactive
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 11/14/2017
ms.author: ashishth
ms.openlocfilehash: 47c2d129cb296f6387142e03b14356bcd83ad698
ms.sourcegitcommit: 562a537ed9b96c9116c504738414e5d8c0fd53b1
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/12/2018
---
# <a name="extract-transform-and-load-etl-at-scale"></a>Извлечение, преобразование и загрузка (ETL) в масштабе

ETL представляет собой процесс, с помощью которого данные извлекаются из разных источников, собираются в стандартном расположении, очищаются, обрабатываются и в конечном итоге загружаются в хранилище данных, откуда их можно запросить. Прежние ETL-процессы дают возможность импортировать данные, очищать их на месте, а затем сохранять в реляционный обработчик данных. С HDInsight широкий набор компонентов экосистемы Hadoop поддерживает выполнение процессов ETL в масштабе. 

Использование HDInsight в процессах ETL можно кратко обозначить с помощью такого конвейера:

![Общие сведения об ETL HDInsight](./media/apache-hadoop-etl-at-scale/hdinsight-etl-at-scale-overview.png)

В разделах ниже рассматриваются все этапы ETL и их связанные компоненты.

## <a name="orchestration"></a>Оркестрация

Оркестрация охватывает все этапы конвейера ETL. Задания ETL в HDInsight часто включают в себя несколько совместно функционирующих различных продуктов.  Так, Hive можно использовать, чтобы очистить одну часть данных, а Pig — другую.  Фабрику данных Azure можно использовать для загрузки данных в базу данных SQL Azure из Azure Data Lake Store.

Оркестрация необходима для запуска соответствующего задания в соответствующее время.

### <a name="oozie"></a>Oozie,

Apache Oozie — это система координации рабочих процессов, которая управляет заданиями Hadoop. Oozie работает в кластере HDInsight и интегрирована со стеком Hadoop. Эта система поддерживает задания Hadoop для Apache MapReduce, Apache Pig, Apache Hive и Apache Sqoop. Ее также можно использовать для планирования относящихся к системе заданий, например Java-программ и сценариев оболочки.

Дополнительные сведения см. в статье [Использование Oozie с Hadoop для определения и запуска рабочих процессов в Azure HDInsight под управлением Linux](../hdinsight-use-oozie-linux-mac.md).

<!-- For a deep dive showing how to use Oozie to drive an end-to-end pipeline, see [Operationalize the Data Pipeline](hdinsight-operationalize-data-pipeline.md). -->

### <a name="azure-data-factory"></a>Фабрика данных Azure

Фабрика данных Azure предоставляет возможности оркестрации в форме "платформа как услуга". Это облачная служба интеграции данных, которая позволяет создавать управляемые данными рабочие процессы в облаке для оркестрации и автоматизации перемещения и преобразования данных. 

Используя фабрику данных Azure, можно выполнять такие задачи:

1. Создавать и включать в расписание управляемые данными рабочие процессы (конвейеры), которые принимают данные из разнородных хранилищ данных.
2. Обрабатывать и преобразовывать эти данные с помощью служб вычислений, например Azure HDInsight Hadoop, Spark, Azure Data Lake Analytics, пакетной службы и машинного обучения Azure.
3. публиковать выходные данные в хранилища данных (например, хранилище данных SQL Azure) для использования приложениями бизнес-аналитики.

Дополнительные сведения о фабрике данных Azure см. в [этой статье](../../data-factory/introduction.md).

## <a name="ingest-file-storage-and-result-storage"></a>Хранилище файлов приема и хранилище результатов

Исходные файлы данных обычно загружаются в расположение в службе хранилища Azure или Azure Data Lake Store. У них может быть любой формат, но обычно это неструктурированные файлы, такие как CSV. 

### <a name="azure-storage"></a>Хранилище Azure 

[Служба хранилища Azure](https://azure.microsoft.com/services/storage/blobs/) имеет [определенные целевые показатели масштабируемости](../../storage/common/storage-scalability-targets.md).  Для большинства аналитических узлов эта служба лучше всего масштабируется при использовании множества небольших файлов.  Эта служба обеспечивает ту же производительность, независимо от количества файлов и их размера (пока соблюдаются установленные ограничения).  Это означает, что можно хранить терабайты данных и по-прежнему получать стабильную производительность как при использовании подмножества данных, так и всех данных.

Служба хранилища Azure предоставляет несколько различных типов больших двоичных объектов.  *Добавочный большой двоичный объект* — оптимальный вариант для хранения веб-журналов или данных датчиков.  

Несколько больших двоичных объектов можно распределить по нескольким серверам для развертывания доступа к ним, но один большой двоичный объект может обслуживаться только одним сервером. Хотя большие двоичные объекты могут быть логически сгруппированы в контейнеры, это не отражается на их распределении по разделам.

Служба хранилища Azure также содержит слой API WebHDFS для хранилища BLOB-объектов.  У всех служб в HDInsight есть доступ к файлам в хранилище BLOB-объектов для очистки и обработки данных. Такой доступ напоминает использование службами распределенной файловой системы Hadoop (HDFS).

Прием данных в службу хранилища Azure выполняется с помощью PowerShell, SDK службы хранилища Azure или AZCopy.

### <a name="azure-data-lake-store"></a>Хранилище озера данных Azure

Azure Data Lake Store (ADLS) — это управляемый гипермасштабируемый репозиторий для данных аналитики, совместимый с HDFS.  ADLS использует парадигму разработки, подобную HDFS, и предлагает неограниченное масштабирование с точки зрения общей емкости и размера отдельных файлов. ADLS выступает оптимальным решением при работе с большими файлами, так как такие файлы можно сохранить на нескольких узлах.  Секционирование данных в ADLS выполняется в фоновом режиме.  Это хранилище обеспечивает колоссальную пропускную способность: более тысячи исполнителей могут одновременно запускать аналитические задания, которые эффективно считывают и записывают тысячи терабайтов данных.

Прием данных в ADLS обычно выполняется с помощью фабрики данных Azure, пакетов SDK ADLS, службы AdlCopy, Apache DistCp или Apache Sqoop.  Выбор конкретных служб значительно зависит от расположения данных.  Если данные находятся в существующем кластере Hadoop, можно использовать Apache DistCp, службу AdlCopy или фабрику данных Azure.  Если они в хранилище BLOB-объектов Azure, можно использовать пакет SDK для .NET для Azure Data Lake Store, Azure PowerShell или фабрику данных Azure.

ADLS также оптимизирована для приема событий с помощью концентратора событий Azure или Apache Storm.

#### <a name="considerations-for-both-storage-options"></a>Рекомендации для двух вариантов хранения

Для отправки наборов данных в терабайтах задержки сети могут стать серьезной проблемой, особенно если источником данных является локальное расположение.  В таких ситуациях будут уместны следующие варианты:

* Azure ExpressRoute. Позволяет создавать частные подключения между центрами обработки данных Azure и локальной инфраструктурой. Такое подключение обеспечивает надежный вариант передачи больших объемов данных. Дополнительные сведения см. в [техническом обзоре ExpressRoute](../../expressroute/expressroute-introduction.md).

* "Автономная" передача данных. Вы можете использовать [службу импорта и экспорта Azure](../../storage/common/storage-import-export-service.md) для доставки жестких дисков с данными в центр обработки данных Azure. Данные сначала будут отправлены в хранилище BLOB-объектов Azure. Затем с помощью [фабрики данных Azure](../../data-factory/v1/data-factory-azure-datalake-connector.md) или инструмента [AdlCopy](../../data-lake-store/data-lake-store-copy-data-azure-storage-blob.md) можно скопировать данные из больших двоичных объектов службы хранилища Azure в Data Lake Store.

### <a name="azure-sql-data-warehouse"></a>Хранилище данных SQL Azure

Хранилище данных SQL Azure — это оптимальный вариант хранения очищенных и предварительно подготовленных результатов для последующей аналитики.  Azure HDInsight можно использовать для выполнения этих служб для этого хранилища.

Хранилище данных SQL Azure (SQL DW) представляет собой реляционное хранилище базы данных, оптимизированное для аналитических рабочих нагрузок.  Масштабирование этого хранилища выполняется на основе секционированных таблиц.  Таблицы можно секционировать на нескольких узлах.  Узлы хранилища данных SQL Azure выбираются во время создания таблицы.  Затем их можно масштабировать, но для такого активного процесса может потребоваться перемещение данных. Дополнительные сведения см. в статье [Manage compute power in Azure SQL Data Warehouse (Overview)](../../sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md) (Управление вычислительной мощностью в хранилище данных SQL Azure (обзор)).

### <a name="hbase"></a>hbase

Apache HBase представляет собой хранилище данных типа "ключ — значение", доступное в Azure HDInsight.  Apache HBase — это база данных NoSQL с открытым кодом, созданная на основе Hadoop и смоделированная после Google BigTable. HBase обеспечивает производительный быстрый доступ и строгую согласованность для больших объемов неструктурированных и полуструктурированных данных, упорядоченных в семейства столбцов.

Данные хранятся в строках таблицы, данные в строке группируются по семейству столбцов. База данных HBase не имеет схемы в том смысле, что ни столбцы, ни типы хранимых в них данных не нужно определять до использования. Открытый код линейно масштабируется, чтобы обрабатывать петабайты данных на тысячах узлов. HBase может полагаться на избыточность данных, пакетную обработку и другие особенности, которые предусмотрены распределенными приложениями в экосистеме Hadoop.   

Это оптимальное место назначения для данных датчика и журнала для последующего анализа.

Масштабируемость HBase определяется количеством узлов в кластере HDInsight.

### <a name="azure-sql-database-and-azure-database"></a>База данных SQL Azure и Azure

Azure предлагает три различные реляционные базы данных по модели "платформа как услуга" (PAAS).

* [База данных SQL Azure](../../sql-database/sql-database-technical-overview.md) представляет собой реализацию Microsoft SQL Server. Дополнительные сведения о производительности см. в статье [Настройка производительности в Базе данных SQL Azure](../../sql-database/sql-database-performance-guidance.md).
* [База данных Azure для MySQL](../../mysql/overview.md) представляет собой реализацию Oracle MySQL.
* [База данных Azure для PostgreSQL](../../postgresql/quickstart-create-server-database-portal.md) представляет собой реализацию PostgreSQL.

В этих продуктах можно увеличивать масштаб путем добавления большего объема памяти и ЦП.  Для них также можно использовать диски категории "Премиум", чтобы повысить производительность ввода-вывода.

## <a name="azure-analysis-services"></a>Службы Azure Analysis Services 

Azure Analysis Services (AAS) представляет собой механизм аналитических данных, используемый в поддержке принятия решений и бизнес-аналитике, обеспечивающий аналитические данные для бизнес-отчетов и клиентских приложений, таких как Power BI, Excel, отчетов Reporting Services и других средств визуализации данных.

Кубы анализа можно масштабировать, изменяя уровни для каждого отдельного куба.  Дополнительные сведения см. на странице [цен на службы Azure Analysis Services](https://azure.microsoft.com/pricing/details/analysis-services/).

## <a name="extract-and-load"></a>Извлечение и загрузка

Если данные находятся в Azure, можно использовать несколько служб для их извлечения и загрузки в другие продукты.  HDInsight поддерживает средства Sqoop и Flume. 

### <a name="sqoop"></a>Sqoop

Apache Sqoop — это средство для эффективной передачи данных между структурированными, полуструктурированными и неструктурированными источниками данных. 

Sqoop использует MapReduce для импорта и экспорта данных, чтобы обеспечить параллельное выполнение операций и отказоустойчивость.

### <a name="flume"></a>Flume

Apache Flume — это распределенная, надежная и доступная служба для эффективного сбора, статистической обработки и перемещения больших объемов данных журнала. У этой службы простая и гибкая архитектура, основанная на передаче потоков данных. Это надежная отказоустойчивая служба с настраиваемыми механизмами для обеспечения надежности, а также механизмами отработки отказа и восстановления. Flume использует простую модель данных с возможностью расширения, позволяющую использовать интерактивное приложение аналитики.

Однако Apache Flume нельзя использовать с Azure HDInsight.  Локально установленная версия Hadoop позволяет использовать Flume для отправки данных в большие двоичные объекты службы хранилища Azure или Azure Data Lake Store.  Дополнительные сведения см. в записи блога об [использовании Apache Flume с HDInsight](https://blogs.msdn.microsoft.com/bigdatasupport/2014/03/18/using-apache-flume-with-hdinsight/).

## <a name="transform"></a>Преобразование

Когда данные находятся в выбранном расположении, их необходимо очистить, объединить или подготовить для определенного шаблона использования.  Hive, Pig и Spark SQL — оптимальные варианты для такой работы.  В HDInsight они все поддерживаются. 

## <a name="next-steps"></a>Дополнительная информация

* [Использование Pig с Hadoop в HDInsight](hdinsight-use-pig.md)
<!-- * [Using Apache Hive as an ETL Tool](hdinsight-using-apache-hive-as-an-etl-tool.md) -->
