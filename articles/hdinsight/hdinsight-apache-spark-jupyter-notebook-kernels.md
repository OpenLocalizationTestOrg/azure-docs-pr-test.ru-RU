---
title: "Ядра для записной книжки Jupyter в кластерах Spark в Azure HDInsight | Документация Майкрософт"
description: "Сведения о ядрах PySpark, PySpark3 и Spark для записной книжки Jupyter, доступной с кластерами Spark в Azure HDInsight."
keywords: "записная книжка jupyter в spark, jupyter в spark"
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 6cfd1c1e7b22f5460b78687c815d149e6c6deac9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/11/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="b66a6-104">Ядра для записной книжки Jupyter в кластерах Spark в Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="b66a6-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="b66a6-105">Кластеры HDInsight Spark предоставляют ядра, которые можно использовать с записной книжкой Jupyter в Spark для тестирования приложений.</span><span class="sxs-lookup"><span data-stu-id="b66a6-105">HDInsight Spark clusters provide kernels that you can use with the Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="b66a6-106">Ядра — это программа, которая выполняет и интерпретирует ваш код.</span><span class="sxs-lookup"><span data-stu-id="b66a6-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="b66a6-107">Вот эти ядра:</span><span class="sxs-lookup"><span data-stu-id="b66a6-107">The three kernels are:</span></span>

- <span data-ttu-id="b66a6-108">**PySpark** (для приложений, написанных на языке Python2).</span><span class="sxs-lookup"><span data-stu-id="b66a6-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="b66a6-109">**PySpark3** (для приложений, написанных на языке Python3).</span><span class="sxs-lookup"><span data-stu-id="b66a6-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="b66a6-110">**Spark** (для приложений, написанных на языке Scala).</span><span class="sxs-lookup"><span data-stu-id="b66a6-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="b66a6-111">В этой статье вы узнаете, как использовать эти ядра, а также преимущества их использования.</span><span class="sxs-lookup"><span data-stu-id="b66a6-111">In this article, you learn how to use these kernels and the benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="b66a6-112">Предварительные требования</span><span class="sxs-lookup"><span data-stu-id="b66a6-112">Prerequisites</span></span>

* <span data-ttu-id="b66a6-113">Кластер Apache Spark в HDInsight.</span><span class="sxs-lookup"><span data-stu-id="b66a6-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="b66a6-114">Инструкции см. в статье [Начало работы. Создание кластера Apache Spark в HDInsight на платформе Linux и выполнение интерактивных запросов с помощью SQL Spark](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="b66a6-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="b66a6-115">Создание записной книжки Jupyter в Spark HDInsight</span><span class="sxs-lookup"><span data-stu-id="b66a6-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="b66a6-116">Откройте кластер на [портале Azure](https://portal.azure.com/).</span><span class="sxs-lookup"><span data-stu-id="b66a6-116">From the [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="b66a6-117">Инструкции см. в разделе [Отображение кластеров](hdinsight-administer-use-portal-linux.md#list-and-show-clusters).</span><span class="sxs-lookup"><span data-stu-id="b66a6-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for the instructions.</span></span> <span data-ttu-id="b66a6-118">Кластер откроется в новой колонке портала.</span><span class="sxs-lookup"><span data-stu-id="b66a6-118">The cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="b66a6-119">В разделе **Быстрые ссылки** щелкните **Панели мониторинга кластера**, чтобы открыть колонку **Панели мониторинга кластера**.</span><span class="sxs-lookup"><span data-stu-id="b66a6-119">From the **Quick links** section, click **Cluster dashboards** to open the **Cluster dashboards** blade.</span></span>  <span data-ttu-id="b66a6-120">Если раздел **Быстрые ссылки** не отображается, в колонке в меню слева щелкните **Обзор**.</span><span class="sxs-lookup"><span data-stu-id="b66a6-120">If you don't see **Quick Links**, click **Overview** from the left menu on the blade.</span></span>

    <span data-ttu-id="b66a6-121">![Записная книжка Jupyter в Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Записная книжка Jupyter в Spark")</span><span class="sxs-lookup"><span data-stu-id="b66a6-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="b66a6-122">Щелкните **Записная книжка Jupyter**.</span><span class="sxs-lookup"><span data-stu-id="b66a6-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="b66a6-123">При появлении запроса введите учетные данные администратора для кластера.</span><span class="sxs-lookup"><span data-stu-id="b66a6-123">If prompted, enter the admin credentials for the cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="b66a6-124">Вы также можете получить доступ к записной книжке Jupyter в кластере Spark, открыв следующий URL-адрес в браузере.</span><span class="sxs-lookup"><span data-stu-id="b66a6-124">You may also reach the Jupyter notebook on Spark cluster by opening the following URL in your browser.</span></span> <span data-ttu-id="b66a6-125">Замените **CLUSTERNAME** именем кластера:</span><span class="sxs-lookup"><span data-stu-id="b66a6-125">Replace **CLUSTERNAME** with the name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="b66a6-126">Щелкните **Создать**, а затем — **Pyspark**, **PySpark3** или **Spark**, чтобы создать объект Notebook.</span><span class="sxs-lookup"><span data-stu-id="b66a6-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** to create a notebook.</span></span> <span data-ttu-id="b66a6-127">Для приложений Scala используйте ядро Spark, для приложений Python2 — ядро PySpark, а для приложений Python3 — ядро PySpark3.</span><span class="sxs-lookup"><span data-stu-id="b66a6-127">Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="b66a6-128">![Ядра для записной книжки Jupyter в Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Ядра для записной книжки Jupyter в Spark")</span><span class="sxs-lookup"><span data-stu-id="b66a6-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="b66a6-129">Объект Notebook должен открыться с помощью выбранного ядра.</span><span class="sxs-lookup"><span data-stu-id="b66a6-129">A notebook opens with the kernel you selected.</span></span>

## <a name="benefits-of-using-the-kernels"></a><span data-ttu-id="b66a6-130">Преимущества использования ядер</span><span class="sxs-lookup"><span data-stu-id="b66a6-130">Benefits of using the kernels</span></span>

<span data-ttu-id="b66a6-131">Ниже приведены некоторые преимущества использования новых ядер для записной книжки Jupyter в кластерах Spark HDInsight.</span><span class="sxs-lookup"><span data-stu-id="b66a6-131">Here are a few benefits of using the new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="b66a6-132">**Предустановленные контексты**.</span><span class="sxs-lookup"><span data-stu-id="b66a6-132">**Preset contexts**.</span></span> <span data-ttu-id="b66a6-133">Благодаря ядрам **PySpark**, **PySpark3** и **Spark** вам не требуется явно настраивать контексты Spark или Hive перед началом работы с приложением.</span><span class="sxs-lookup"><span data-stu-id="b66a6-133">With  **PySpark**, **PySpark3**, or the **Spark** kernels, you do not need to set the Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="b66a6-134">Они доступны по умолчанию.</span><span class="sxs-lookup"><span data-stu-id="b66a6-134">These are available by default.</span></span> <span data-ttu-id="b66a6-135">а именно:</span><span class="sxs-lookup"><span data-stu-id="b66a6-135">These contexts are:</span></span>
   
   * <span data-ttu-id="b66a6-136">**sc** для контекста Spark;</span><span class="sxs-lookup"><span data-stu-id="b66a6-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="b66a6-137">**sqlContext** для контекста Hive.</span><span class="sxs-lookup"><span data-stu-id="b66a6-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="b66a6-138">Это значит, что для настройки этих контекстов вам не придется выполнять операторы следующего вида:</span><span class="sxs-lookup"><span data-stu-id="b66a6-138">So, you don't have to run statements like the following to set the contexts:</span></span>

        <span data-ttu-id="b66a6-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="b66a6-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="b66a6-140">Вместо этого вы сможете сразу использовать в своем приложении предустановленные контексты.</span><span class="sxs-lookup"><span data-stu-id="b66a6-140">Instead, you can directly use the preset contexts in your application.</span></span>

- <span data-ttu-id="b66a6-141">**Волшебные команды.**</span><span class="sxs-lookup"><span data-stu-id="b66a6-141">**Cell magics**.</span></span> <span data-ttu-id="b66a6-142">Ядро PySpark предоставляет несколько "магических команд". Это специальные команды, которые можно вызывать с помощью `%%` (например, `%%MAGIC` <args>).</span><span class="sxs-lookup"><span data-stu-id="b66a6-142">The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="b66a6-143">Волшебная команда должна быть первым словом в ячейке кода и может состоять из нескольких строк содержимого.</span><span class="sxs-lookup"><span data-stu-id="b66a6-143">The magic command must be the first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="b66a6-144">Волшебное слово должно быть первым словом в ячейке.</span><span class="sxs-lookup"><span data-stu-id="b66a6-144">The magic word should be the first word in the cell.</span></span> <span data-ttu-id="b66a6-145">Любые другие слова перед магической командой, даже комментарии, приведут к ошибке.</span><span class="sxs-lookup"><span data-stu-id="b66a6-145">Adding anything before the magic, even comments, causes an error.</span></span>     <span data-ttu-id="b66a6-146">Дополнительные сведения о волшебных командах см. [здесь](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span><span class="sxs-lookup"><span data-stu-id="b66a6-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="b66a6-147">В следующей таблице перечислены различные магические команды, доступные для ядер.</span><span class="sxs-lookup"><span data-stu-id="b66a6-147">The following table lists the different magics available through the kernels.</span></span>

   | <span data-ttu-id="b66a6-148">Волшебная команда</span><span class="sxs-lookup"><span data-stu-id="b66a6-148">Magic</span></span> | <span data-ttu-id="b66a6-149">Пример</span><span class="sxs-lookup"><span data-stu-id="b66a6-149">Example</span></span> | <span data-ttu-id="b66a6-150">Описание</span><span class="sxs-lookup"><span data-stu-id="b66a6-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="b66a6-151">help</span><span class="sxs-lookup"><span data-stu-id="b66a6-151">help</span></span> |`%%help` |<span data-ttu-id="b66a6-152">Формирует таблицу из всех доступных волшебных слов с примерами и описанием.</span><span class="sxs-lookup"><span data-stu-id="b66a6-152">Generates a table of all the available magics with example and description</span></span> |
   | <span data-ttu-id="b66a6-153">info</span><span class="sxs-lookup"><span data-stu-id="b66a6-153">info</span></span> |`%%info` |<span data-ttu-id="b66a6-154">Выводит сведения о сеансе для текущей конечной точки Livy.</span><span class="sxs-lookup"><span data-stu-id="b66a6-154">Outputs session information for the current Livy endpoint</span></span> |
   | <span data-ttu-id="b66a6-155">Настройка</span><span class="sxs-lookup"><span data-stu-id="b66a6-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="b66a6-156">`{"executorMemory": "1000M"`,</span><span class="sxs-lookup"><span data-stu-id="b66a6-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="b66a6-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="b66a6-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="b66a6-158">Настраивает параметры для создания сеанса.</span><span class="sxs-lookup"><span data-stu-id="b66a6-158">Configures the parameters for creating a session.</span></span> <span data-ttu-id="b66a6-159">Флаг force (-f) является обязательным, если сеанс уже был создан, иначе сеанс будет удален и создан заново.</span><span class="sxs-lookup"><span data-stu-id="b66a6-159">The force flag (-f) is mandatory if a session has already been created, which ensures that the session is dropped and recreated.</span></span> <span data-ttu-id="b66a6-160">Список допустимых параметров приведен в разделе, посвященном [тексту запроса сеансов POST Livy](https://github.com/cloudera/livy#request-body) .</span><span class="sxs-lookup"><span data-stu-id="b66a6-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="b66a6-161">Параметры должны передаваться в виде строки JSON, следующей после волшебной команды, как показано в столбце примера.</span><span class="sxs-lookup"><span data-stu-id="b66a6-161">Parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.</span></span> |
   | <span data-ttu-id="b66a6-162">sql</span><span class="sxs-lookup"><span data-stu-id="b66a6-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="b66a6-163">Выполняет запрос Hive к sqlContext.</span><span class="sxs-lookup"><span data-stu-id="b66a6-163">Executes a Hive query against the sqlContext.</span></span> <span data-ttu-id="b66a6-164">Если передан параметр `-o` , результат запроса сохраняется в контексте Python %%local в качестве таблицы данных [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="b66a6-164">If the `-o` parameter is passed, the result of the query is persisted in the %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="b66a6-165">local</span><span class="sxs-lookup"><span data-stu-id="b66a6-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="b66a6-166">Весь код в последующих строках выполняется локально.</span><span class="sxs-lookup"><span data-stu-id="b66a6-166">All the code in subsequent lines is executed locally.</span></span> <span data-ttu-id="b66a6-167">В качестве кода должен быть указан допустимый код на языке Python2 (вне зависимости от используемого ядра).</span><span class="sxs-lookup"><span data-stu-id="b66a6-167">Code must be valid Python2 code even irrespective of the kernel you are using.</span></span> <span data-ttu-id="b66a6-168">Таким образом, даже если при создании объекта Notebook было выбрано ядро **PySpark3** или **Spark**, то при использовании в ячейке магической команды `%%local` в этой ячейке должен содержаться только допустимый код Python2.</span><span class="sxs-lookup"><span data-stu-id="b66a6-168">So, even if you selected **PySpark3** or **Spark** kernels while creating the notebook, if you use the `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="b66a6-169">журналы</span><span class="sxs-lookup"><span data-stu-id="b66a6-169">logs</span></span> |`%%logs` |<span data-ttu-id="b66a6-170">Выводит журналы для текущего сеанса Livy.</span><span class="sxs-lookup"><span data-stu-id="b66a6-170">Outputs the logs for the current Livy session.</span></span> |
   | <span data-ttu-id="b66a6-171">удалить</span><span class="sxs-lookup"><span data-stu-id="b66a6-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="b66a6-172">Удаляет указанный сеанс для текущей конечной точки Livy.</span><span class="sxs-lookup"><span data-stu-id="b66a6-172">Deletes a specific session of the current Livy endpoint.</span></span> <span data-ttu-id="b66a6-173">Обратите внимание, что нельзя удалить сеанс, который был инициирован самим ядром.</span><span class="sxs-lookup"><span data-stu-id="b66a6-173">Note that you cannot delete the session that is initiated for the kernel itself.</span></span> |
   | <span data-ttu-id="b66a6-174">cleanup</span><span class="sxs-lookup"><span data-stu-id="b66a6-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="b66a6-175">Удаляет все сеансы для текущей конечной точки Livy, включая сеанс этой записной книжки.</span><span class="sxs-lookup"><span data-stu-id="b66a6-175">Deletes all the sessions for the current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="b66a6-176">Флаг -f является обязательным.</span><span class="sxs-lookup"><span data-stu-id="b66a6-176">The force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="b66a6-177">Помимо магических команд, добавленных ядром PySpark, можно также использовать [встроенные магические команды](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics) IPython, в том числе `%%sh`.</span><span class="sxs-lookup"><span data-stu-id="b66a6-177">In addition to the magics added by the PySpark kernel, you can also use the [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="b66a6-178">Можно использовать магическую команду `%%sh` для выполнения сценариев и блоков кода на головном узле кластера.</span><span class="sxs-lookup"><span data-stu-id="b66a6-178">You can use the `%%sh` magic to run scripts and block of code on the cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="b66a6-179">**Автоматическая визуализация**.</span><span class="sxs-lookup"><span data-stu-id="b66a6-179">**Auto visualization**.</span></span> <span data-ttu-id="b66a6-180">Ядро **Pyspark** автоматически визуализирует выходные данные запросов Hive и SQL.</span><span class="sxs-lookup"><span data-stu-id="b66a6-180">The **Pyspark** kernel automatically visualizes the output of Hive and SQL queries.</span></span> <span data-ttu-id="b66a6-181">Вы можете выбрать различные типы средства визуализации, включая таблицы, круговые диаграммы, графики, диаграммы с областями и линейчатые диаграммы.</span><span class="sxs-lookup"><span data-stu-id="b66a6-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-the-sql-magic"></a><span data-ttu-id="b66a6-182">Параметры, поддерживаемые волшебной командой %%sql</span><span class="sxs-lookup"><span data-stu-id="b66a6-182">Parameters supported with the %%sql magic</span></span>
<span data-ttu-id="b66a6-183">Магическая команда `%%sql` поддерживает различные параметры, позволяющие управлять результатом выполнения запросов.</span><span class="sxs-lookup"><span data-stu-id="b66a6-183">The `%%sql` magic supports different parameters that you can use to control the kind of output that you receive when you run queries.</span></span> <span data-ttu-id="b66a6-184">Возможные результаты показаны в следующей таблице.</span><span class="sxs-lookup"><span data-stu-id="b66a6-184">The following table lists the output.</span></span>

| <span data-ttu-id="b66a6-185">Параметр</span><span class="sxs-lookup"><span data-stu-id="b66a6-185">Parameter</span></span> | <span data-ttu-id="b66a6-186">Пример</span><span class="sxs-lookup"><span data-stu-id="b66a6-186">Example</span></span> | <span data-ttu-id="b66a6-187">Описание</span><span class="sxs-lookup"><span data-stu-id="b66a6-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="b66a6-188">-o</span><span class="sxs-lookup"><span data-stu-id="b66a6-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="b66a6-189">При использовании этого параметра результат запроса сохраняется в контексте Python %%local в качестве таблицы данных [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="b66a6-189">Use this parameter to persist the result of the query, in the %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="b66a6-190">Именем переменной таблицы данных служит указанное вами имя переменной.</span><span class="sxs-lookup"><span data-stu-id="b66a6-190">The name of the dataframe variable is the variable name you specify.</span></span> |
| <span data-ttu-id="b66a6-191">-q</span><span class="sxs-lookup"><span data-stu-id="b66a6-191">-q</span></span> |`-q` |<span data-ttu-id="b66a6-192">Позволяет отключить визуализации для ячейки.</span><span class="sxs-lookup"><span data-stu-id="b66a6-192">Use this to turn off visualizations for the cell.</span></span> <span data-ttu-id="b66a6-193">Если вам не нужна автоматическая визуализация содержимого ячейки и вы хотите только записать ее как таблицу данных, используйте параметр `-q -o <VARIABLE>`.</span><span class="sxs-lookup"><span data-stu-id="b66a6-193">If you don't want to auto-visualize the content of a cell and just want to capture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="b66a6-194">Если вы хотите отключить визуализацию, не записывая результаты (например, для выполнения запроса SQL, такого как инструкция `CREATE TABLE`), то используйте параметр `-q` без аргумента `-o`.</span><span class="sxs-lookup"><span data-stu-id="b66a6-194">If you want to turn off visualizations without capturing the results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="b66a6-195">-m</span><span class="sxs-lookup"><span data-stu-id="b66a6-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="b66a6-196">Параметр **METHOD** имеет значение **take** или **sample** (по умолчанию используется значение **take**).</span><span class="sxs-lookup"><span data-stu-id="b66a6-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="b66a6-197">Если используется метод **take**, то ядро выбирает элементы из верхней части результирующего набора данных, который определяется параметром MAXROWS (описывается далее в этой таблице).</span><span class="sxs-lookup"><span data-stu-id="b66a6-197">If the method is **take**, the kernel picks elements from the top of the result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="b66a6-198">Если используется метод **sample**, то ядро выбирает элементы из набора данных случайным образом в соответствии с параметром `-r`, описанным далее в этой таблице.</span><span class="sxs-lookup"><span data-stu-id="b66a6-198">If the method is **sample**, the kernel randomly samples elements of the data set according to `-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="b66a6-199">-r</span><span class="sxs-lookup"><span data-stu-id="b66a6-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="b66a6-200">Здесь **FRACTION** — это число с плавающей запятой от 0,0 до 1,0.</span><span class="sxs-lookup"><span data-stu-id="b66a6-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="b66a6-201">Если для SQL-запроса используется метод выборки `sample`, то ядро выбирает заданную долю элементов из результирующего набора случайным образом.</span><span class="sxs-lookup"><span data-stu-id="b66a6-201">If the sample method for the SQL query is `sample`, then the kernel randomly samples the specified fraction of the elements of the result set for you.</span></span> <span data-ttu-id="b66a6-202">Например, при выполнении SQL-запроса с аргументами `-m sample -r 0.01` из результирующего набора данных случайным образом отбирается 1 % строк.</span><span class="sxs-lookup"><span data-stu-id="b66a6-202">For example, if you run a SQL query with the arguments `-m sample -r 0.01`, then 1% of the result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="b66a6-203">**MAXROWS** должно быть выражено целым числом.</span><span class="sxs-lookup"><span data-stu-id="b66a6-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="b66a6-204">Число выходных строк для параметра **MAXROWS** ограничивается ядром.</span><span class="sxs-lookup"><span data-stu-id="b66a6-204">The kernel limits the number of output rows to **MAXROWS**.</span></span> <span data-ttu-id="b66a6-205">Если значение параметра **MAXROWS** выражено отрицательным числом, например **-1**, то число строк в результирующем наборе не ограничивается.</span><span class="sxs-lookup"><span data-stu-id="b66a6-205">If **MAXROWS** is a negative number such as **-1**, then the number of rows in the result set is not limited.</span></span> |

<span data-ttu-id="b66a6-206">**Пример**</span><span class="sxs-lookup"><span data-stu-id="b66a6-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="b66a6-207">Приведенная выше инструкция делает следующее:</span><span class="sxs-lookup"><span data-stu-id="b66a6-207">The statement above does the following:</span></span>

* <span data-ttu-id="b66a6-208">Выбирает все записи из таблицы **hivesampletable**.</span><span class="sxs-lookup"><span data-stu-id="b66a6-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="b66a6-209">Отключает автоматическую визуализацию, так как включает параметр -q.</span><span class="sxs-lookup"><span data-stu-id="b66a6-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="b66a6-210">Случайным образом выбирает 10 % строк из таблицы hivesampletable и ограничивает размер результирующего набора 500 строками, так как включает параметр `-m sample -r 0.1 -n 500` .</span><span class="sxs-lookup"><span data-stu-id="b66a6-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of the rows in the hivesampletable and limits the size of the result set to 500 rows.</span></span>
* <span data-ttu-id="b66a6-211">И наконец, сохраняет выходные данные в таблицу данных **query2**, так как включает параметр `-o query2`.</span><span class="sxs-lookup"><span data-stu-id="b66a6-211">Finally, because we used `-o query2` it also saves the output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-the-new-kernels"></a><span data-ttu-id="b66a6-212">Рекомендации по использованию новых ядер</span><span class="sxs-lookup"><span data-stu-id="b66a6-212">Considerations while using the new kernels</span></span>

<span data-ttu-id="b66a6-213">Какое бы ядро вы ни использовали, работающие объекты Notebook потребляют ресурсы кластера.</span><span class="sxs-lookup"><span data-stu-id="b66a6-213">Whichever kernel you use, leaving the notebooks running consumes the cluster resources.</span></span>  <span data-ttu-id="b66a6-214">Используя эти ядра (так как контексты заданы предварительно), при простом выходе из объектов Notebook контекст не завершается, а значит ресурсы кластера продолжают использоваться.</span><span class="sxs-lookup"><span data-stu-id="b66a6-214">With these kernels, because the contexts are preset, simply exiting the notebooks does not kill the context and hence the cluster resources continue to be in use.</span></span> <span data-ttu-id="b66a6-215">После завершения работы с объектом Notebook рекомендуется выбрать **Close and Halt** (Закрыть и остановить) в меню **File** (Файл) объекта Notebook. Это действие завершит контекст и закроет объект Notebook.</span><span class="sxs-lookup"><span data-stu-id="b66a6-215">A good practice is to use the **Close and Halt** option from the notebook's **File** menu when you are finished using the notebook, which kills the context and then exits the notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="b66a6-216">Примеры</span><span class="sxs-lookup"><span data-stu-id="b66a6-216">Show me some examples</span></span>

<span data-ttu-id="b66a6-217">Открыв объект Jupyter Notebook, вы увидите в корневом каталоге две папки.</span><span class="sxs-lookup"><span data-stu-id="b66a6-217">When you open a Jupyter notebook, you see two folders available at the root level.</span></span>

* <span data-ttu-id="b66a6-218">Папка **PySpark** содержит примеры записных книжек, в которых используется новое ядро **Python**.</span><span class="sxs-lookup"><span data-stu-id="b66a6-218">The **PySpark** folder has sample notebooks that use the new **Python** kernel.</span></span>
* <span data-ttu-id="b66a6-219">Папка **Scala** содержит примеры записных книжек, в которых используется новое ядро **Spark**.</span><span class="sxs-lookup"><span data-stu-id="b66a6-219">The **Scala** folder has sample notebooks that use the new **Spark** kernel.</span></span>

<span data-ttu-id="b66a6-220">Чтобы получить представление о различных волшебных командах, вы можете открыть записную книжку **00 - [READ ME FIRST] Spark Magic Kernel Features** из каталога **PySpark** или **Spark**.</span><span class="sxs-lookup"><span data-stu-id="b66a6-220">You can open the **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from the **PySpark** or **Spark** folder to learn about the different magics available.</span></span> <span data-ttu-id="b66a6-221">Также можно использовать другие примеры записных книжек в этих каталогах, чтобы узнать, как реализовать различные сценарии с помощью записных книжек Jupyter с кластерами HDInsight Spark.</span><span class="sxs-lookup"><span data-stu-id="b66a6-221">You can also use the other sample notebooks available under the two folders to learn how to achieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-the-notebooks-stored"></a><span data-ttu-id="b66a6-222">Где хранятся записные книжки?</span><span class="sxs-lookup"><span data-stu-id="b66a6-222">Where are the notebooks stored?</span></span>

<span data-ttu-id="b66a6-223">Записные книжки Jupyter хранятся в учетной записи хранения, связанной с кластером в папке **/HdiNotebooks** .</span><span class="sxs-lookup"><span data-stu-id="b66a6-223">Jupyter notebooks are saved to the storage account associated with the cluster under the **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="b66a6-224">Доступ к объектам Notebook, текстовым файлам и папкам, создаваемым в Jupyter, можно получить через учетную запись хранения.</span><span class="sxs-lookup"><span data-stu-id="b66a6-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from the storage account.</span></span>  <span data-ttu-id="b66a6-225">Например, если Jupyter используется для создания папки **myfolder** и объекта Notebook **myfolder/mynotebook.ipynb**, то доступ к этому объекту можно получить в расположении `/HdiNotebooks/myfolder/mynotebook.ipynb` в учетной записи хранения.</span><span class="sxs-lookup"><span data-stu-id="b66a6-225">For example, if you use Jupyter to create a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within the storage account.</span></span>  <span data-ttu-id="b66a6-226">Верно и обратное: если вы передаете объект Notebook непосредственно в свою учетную запись хранения в `/HdiNotebooks/mynotebook1.ipynb`, то этот объект также отображается в Jupyter.</span><span class="sxs-lookup"><span data-stu-id="b66a6-226">The reverse is also true, that is, if you upload a notebook directly to your storage account at `/HdiNotebooks/mynotebook1.ipynb`, the notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="b66a6-227">Объекты Notebook хранятся в учетной записи хранения даже после удаления кластера.</span><span class="sxs-lookup"><span data-stu-id="b66a6-227">Notebooks remain in the storage account even after the cluster is deleted.</span></span>

<span data-ttu-id="b66a6-228">Записные книжки сохраняются в учетной записи хранения как в HDFS.</span><span class="sxs-lookup"><span data-stu-id="b66a6-228">The way notebooks are saved to the storage account is compatible with HDFS.</span></span> <span data-ttu-id="b66a6-229">Таким образом, подключаясь к кластеру по протоколу SSH, вы можете использовать команды управления файлами, как показано в следующем фрагменте:</span><span class="sxs-lookup"><span data-stu-id="b66a6-229">So, if you SSH into the cluster you can use file management commands as shown in the following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


<span data-ttu-id="b66a6-230">Если с доступом к учетной записи хранения для кластера возникнут проблемы, то сохраненные записные книжки можно будет также найти на головном узле `/var/lib/jupyter`.</span><span class="sxs-lookup"><span data-stu-id="b66a6-230">In case there are issues accessing the storage account for the cluster, the notebooks are also saved on the headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="b66a6-231">Поддерживаемый браузер</span><span class="sxs-lookup"><span data-stu-id="b66a6-231">Supported browser</span></span>

<span data-ttu-id="b66a6-232">Записные книжки Jupyter, выполняемые в кластерах HDInsight Spark, поддерживаются только браузером Google Chrome.</span><span class="sxs-lookup"><span data-stu-id="b66a6-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="b66a6-233">Отзыв</span><span class="sxs-lookup"><span data-stu-id="b66a6-233">Feedback</span></span>
<span data-ttu-id="b66a6-234">Новые ядра находятся в стадии развития и будут улучшаться со временем.</span><span class="sxs-lookup"><span data-stu-id="b66a6-234">The new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="b66a6-235">Кроме того, это может означать, что по мере развития этих ядер API могут измениться.</span><span class="sxs-lookup"><span data-stu-id="b66a6-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="b66a6-236">Мы будем признательны вам за любые отзывы о работе с новыми ядрами.</span><span class="sxs-lookup"><span data-stu-id="b66a6-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="b66a6-237">Ваши комментарии помогут нам оформить финальную версию этих ядер.</span><span class="sxs-lookup"><span data-stu-id="b66a6-237">This is useful in shaping the final release of these kernels.</span></span> <span data-ttu-id="b66a6-238">Отзывы и замечания оставляйте в разделе **Комментарии** под данной статьей.</span><span class="sxs-lookup"><span data-stu-id="b66a6-238">You can leave your comments/feedback under the **Comments** section at the bottom of this article.</span></span>

## <span data-ttu-id="b66a6-239"><a name="seealso"></a>Дополнительные материалы</span><span class="sxs-lookup"><span data-stu-id="b66a6-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="b66a6-240">Обзор: Apache Spark в Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="b66a6-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="b66a6-241">Сценарии</span><span class="sxs-lookup"><span data-stu-id="b66a6-241">Scenarios</span></span>
* [<span data-ttu-id="b66a6-242">Использование Spark со средствами бизнес-аналитики. Выполнение интерактивного анализа данных с использованием Spark в HDInsight с помощью средств бизнес-аналитики</span><span class="sxs-lookup"><span data-stu-id="b66a6-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="b66a6-243">Использование Spark с машинным обучением. Использование Spark в HDInsight для анализа температуры в здании на основе данных системы кондиционирования</span><span class="sxs-lookup"><span data-stu-id="b66a6-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="b66a6-244">Использование Spark с машинным обучением. Использование Spark в HDInsight для прогнозирования результатов контроля качества пищевых продуктов</span><span class="sxs-lookup"><span data-stu-id="b66a6-244">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="b66a6-245">Потоковая передача Spark. Использование Spark в HDInsight для сборки приложений потоковой передачи данных в режиме реального времени</span><span class="sxs-lookup"><span data-stu-id="b66a6-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="b66a6-246">Анализ журнала веб-сайта с использованием Spark в HDInsight</span><span class="sxs-lookup"><span data-stu-id="b66a6-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="b66a6-247">Создание и запуск приложений</span><span class="sxs-lookup"><span data-stu-id="b66a6-247">Create and run applications</span></span>
* [<span data-ttu-id="b66a6-248">Создание автономного приложения с использованием Scala</span><span class="sxs-lookup"><span data-stu-id="b66a6-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="b66a6-249">Удаленный запуск заданий с помощью Livy в кластере Spark</span><span class="sxs-lookup"><span data-stu-id="b66a6-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="b66a6-250">Средства и расширения</span><span class="sxs-lookup"><span data-stu-id="b66a6-250">Tools and extensions</span></span>
* [<span data-ttu-id="b66a6-251">Использование подключаемого модуля средств HDInsight для IntelliJ IDEA для создания и отправки приложений Spark Scala</span><span class="sxs-lookup"><span data-stu-id="b66a6-251">Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="b66a6-252">Удаленная отладка приложений Spark в кластере HDInsight Spark Linux с помощью подключаемого модуля средств HDInsight для IntelliJ IDEA</span><span class="sxs-lookup"><span data-stu-id="b66a6-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="b66a6-253">Использование записных книжек Zeppelin с кластером Spark в HDInsight</span><span class="sxs-lookup"><span data-stu-id="b66a6-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="b66a6-254">Использование внешних пакетов с записными книжками Jupyter</span><span class="sxs-lookup"><span data-stu-id="b66a6-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="b66a6-255">Установка записной книжки Jupyter на компьютере и ее подключение к кластеру Apache Spark в Azure HDInsight (предварительная версия)</span><span class="sxs-lookup"><span data-stu-id="b66a6-255">Install Jupyter on your computer and connect to an HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="b66a6-256">Управление ресурсами</span><span class="sxs-lookup"><span data-stu-id="b66a6-256">Manage resources</span></span>
* [<span data-ttu-id="b66a6-257">Управление ресурсами кластера Apache Spark в Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="b66a6-257">Manage resources for the Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="b66a6-258">Отслеживание и отладка заданий в кластере Apache Spark в HDInsight на платформе Linux</span><span class="sxs-lookup"><span data-stu-id="b66a6-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
